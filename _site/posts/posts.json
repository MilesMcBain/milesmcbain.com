[
  {
    "path": "posts/data-analysis-reuse/",
    "title": "Patterns and anti-patterns of data analysis reuse",
    "description": "A speed-run through four stages of data analysis reuse, to the end game you probably guessed was coming.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.com"
      }
    ],
    "date": "2024-03-11",
    "categories": [
      "data science",
      "workflows"
    ],
    "contents": "\n\n\n\nFigure 1: Three projects share a common code library dependency. Re-use solved?\n\n\n\nEvery data analysis / data scientist role I’ve worked in has had a strong theme\nof redoing variations of the same analysis. I expect this is something of an\nindustry-wide trend. If you’re in marketing you’re ranking prospects, and A/B\ntesting their responses. If you’re in emergency services planning you’re\nre-running the same kinds of simulations targeting the same kinds of KPIs\n(response times). If you’re in banking you’re predicting churn, arrears etc.\nThe details change, but you see the same problem structure repeated over and\nover.\nEven when I was supporting researchers I saw often saw them cut and paste\nmethods from older works and ‘adapt them’ to whatever the current question was,\nusing their favoured hammer to hit every nail.\nIt’s The Data Analyst’s Curse: You’re keen to prove you can use\nthese much-hyped skills to add value. But as soon as you do, you’re stuck. Your\nvalue-add becomes business as usual (BAU). And good luck to you if you think\nyou’re going to shirk BAU to investigate more interesting data - that would\nbe a value-subtract. And those make the well dressed people with\nsoft-keyboard-covered-iPads under their arms very unhappy.\nGiven your curse, your sanity now depends on re-using as much work as possible\nacross these achingly-samey-but-subtlety-different projects. You and your team\nneed to set yourselves up so you can re-cover old ground quickly, hopefully\nensuring that you have plenty of time to build on capabilities you’ve already\nproven, rather than just executing and re-executing the same capability until\nyou all get bored and quit.1\nSo now lets look at the ways you can reuse your data analysis work. And I’m\nmaking two pretty big assumptions in this discussion:\nYour work is written in code e.g. R, Python, Julia, Rust etc.\n- If you’re using PowerBI, or god forbid Excel, you’re largely limited to\nthe first two less than ideal approaches I am about to discuss.\nYou use a technology like Quarto, or Rmarkdown, or Shiny, such that\nany document-like products are generated from code.\n- If you don’t do this there’s a host of added pitfalls on top of what\nwe’ll discuss here, more than can sensibly be covered in one blog\npost.\nCopying and pasting from stuff you did earlier\nIt’s in the name. Instead of redoing an analysis you know how to do from\nscratch you open up the project from last time and go ctrl-c ctrl-v wild. At\nfirst this may feel pretty good. Almost like cheating. If you’re clever you\nwon’t tell your boss or clients how much time you just saved. And not because\nyou’re dishonest. No it’s because what the ol’ copypasta is actually doing is\nslowly accumulating a mountain of ‘technical-debt’ or\n‘old-weird-stuff-we-don’t-really-understand-anymore-but-keeps-coming-along-for-the-ride-and-now-we-can’t-touch-it-without-ruining-everything’.\nEventually the interest on that technical debt, paid in terms of ‘time spent\nstuffing around trying to add an incremental improvement’, eats all the\nproductivity gains you got from the copy-paste in the first place. 2\nThere are several ways this can happen. Here’s a scenario.\nThe client really liked the dynamics evident in the event stream plot on\nproject BLUE. So for project ORANGE it’s only natural to pull that plot and\nit’s data processing dependencies in via copy-paste.\n\n\n\nFigure 2: A copy of a data analysis directory is made.\n\n\n\nBut hang on your colleague Russell is a fan of the work too. He wants to show\nit to a new client on project GREEN. No worries mate, it’s in\n/projects/GREEN/GREEN_final_complete2/code/script04.R. What? No it worked on\nmy machine, I swear. Oh wait. Try the dev version of ggplot2.\n\n\n\nFigure 3: A copy of a copy is made.\n\n\n\nOh no. Murphy’s law. You found a bug in that beautiful plot you were so proud\nof. There were some pesky duplicates in the data. So you fixed it on project\nORANGE. Better fix it on project BLUE too. And lookout you forgot to tell\nRussell on project GREEN!\n\n\n\nFigure 4: A bug is found and two of the three copies are updated.\n\n\n\nWhat kind of damage can we expect from this?\n\n\n\nFigure 5: One of the copies still carries the bug.\n\n\n\nCopy-pasted code is like a virus. With each reproduction it can acquire\nmutations that make it more or less fit to be copied in the future. Maybe\nRussell added a sweet forecast with uncertainty gradients to the plot. He\ndidn’t bother to fix the duplicate issue because it was minor in his case. Now\nwhen someone wants a forecast, it is Russell’s variant of your buggy code that\nwill reproduce. After it acquires enough mutations you could very well\naccidentally copy a version containing your original bug into your own project,\nand you’ll be confused as to how this bug you fixed 18 months ago has risen\nfrom the grave.\nThe erosion of trust, the stress, the time wasted, the fumbling\nfor the right version in this every growing pile is the interest bill for your\ntechnical debt.3\nThe one template to rule them all\nThe cure for the copy-paste virus and all it’s ills is centralisation. For each\nproject you build on a centralised base that contains the definitive versions\nof all the commonly used functionality that makes up your data analysis\ncapability. Creating a shared project template is a realisation of this idea.\nThe crudest example of a viable template might be a giant monolith of a Quarto\nfile that contains all your project code and text, with placeholder text or\n#TODO comments that indicate where the data analyst needs to insert stuff\nthat is specific to their current project.\nThe return of copy-paste\nIn theory, as the team builds on their data analysis capability, the shared\ntemplate can be updated so all future projects will benefit from enhancements\nor fixes that are made. I say ‘in theory’ because the first pitfall of this\napproach is that in order to build on the template you have to take a copy of\nit. So changes you make to your copy don’t automatically get reflected in the\ncentralised version. Your colleagues aren’t even aware your changes exist.\nThere may be some kind of agreement in place that ‘we’ll all push our changes\nback to the central template as we go’, but since this task is not necessary\nfor the delivery of the work, it relies on human discipline, or rigid adherence\nto process, which inevitably fails under pressure.\nSo despite the best of intentions your team can end up with diverging templates\nin play, and the temptation is to reuse a version with the features you need\nrather then the shared version, and THUNK you’ve backslid to ‘copying and\npasting from stuff you did earlier’.\nBut that’ll never happen to your team right? You’re not that lazy. Well let me\ntell you about a different kind of trap templates hold for clever industrious\ntypes.\nPower overwhelming\nWhen you’re maintaining a template it seems like the productive thing to do\nshould be to push as much as possible into it, to minimise the work needed for\neach data analysis job. Perhaps you have an ideal where you press a button, the\nmachinery cranks away, and spits out all the metrics, tables, and plots you\nneed. Leaving you just bit of interpretation and a few points of commentary to\ndo.\nThe challenge you’ll face in getting there is that there are always niggling\nlittle details of each new data set, and new context that require some special\nhandling. To try to solve for this you can make the template configurable. You\nmight even employ meta-programming techniques to conditionally change the way\nyour outputs look.\nSo because the word ‘configure’ was spoken, someone will suggest using YAML or\nJSON to store it. Guaranteed. Because that’s just what you do config right?\nResist this. I urge you. Does YAML look that much better to your eyes than a\nliteral R list or Python dictionary? Code can be refactored, code can be\nauto-completed, code can be analysed, checked, styled etc. But text? May you be\nso lucky.\nOnce you’re talking template config you’re on a narrow and dangerous path. You\nfeed the beast and it grows bigger and hungrier. Despite your best efforts the\ntemplate will never be flexible enough to satisfy your ideal. You’ll find\nyourself saying ‘if we just allow X to be configurable…’ and field upon field\nwill be added. Perhaps you’ll find away to allow configured objects to be\nreferenced so they can be reused. Or maybe you’ll sprinkle a bit of flow\ncontrol on it - ‘if this not that’ etc.\nOr maybe to speed up the now arduous config process you’ll template the config\nfor the template, yeah that should probably do it. No? Okay fuck it. Maybe\nyou’ll allow the user to supply custom code expressions enclosed in \" (of\ncourse) to be inserted at critical junctures in the project. You may think I am\nexaggerating but these are horrors I have seen with my own eyes. Some created\nby my own hand.\nAnd maybe if you’re fortunate, when you’re bloodied and panting, having just\nscreamed “ARE YOU NOT ENTERTAINED?” into your screen like a lunatic, you will\nhave a moment of clarity, and you will realise that you were never configuring\nanything. You were slowly imbuing your configuration with the powers of code to\ncreate yourself a new high level, fully functional, but horribly specified\nlanguage with which to describe your solution to the data analysis problem. It\nwas programming all along.\nIf you are at this point now do not despair. Many have been here before you. I\nknow this phenomenon as The Inner Platform\nEffect and I learned\nabout it the hard way in my first job. I also like to remix Greenspun’s tenth rule of programming\nfor this occasion:\nAny sufficiently complicated plain text configuration scheme contains an ad\nhoc, informally-specified, bug-ridden, slow implementation of half of\nCommon Lisp.\nTemplates are, at best, a partial solution to data analysis reuse. In a similar\nway to the copy-paste strategy, they can feel very productive initially, but if\nyou let them become too complex, increasing amounts of resources will go to\nkeeping the template machinery working, sucking time away from developing your\nteam’s data analysis capabilities.\nThe one package to rule them all\nInstead of expressing a high level project solution in template configuration,\nthere’s a much better time to be had creating a library of powerful domain\nspecific functions (or objects if that’s your thing) that can be composed and\ninvoked to perform major parts of the data analysis. The flexibility to handle\nall the little individual circumstantial curios comes from parameters which can\nchange the behaviour of functions, or from composing the functions in different\nways, perhaps adding, removing, or swapping them to account for different\nsituations.\nWith powerful functions you still maximise the ratio of work done to keystrokes\npressed. You do so by doing more work with each line of code you type, as\nopposed to building a machine powered by config that outputs prefabricated\nanalysis. And code, unlike configuration, has all the degrees of freedom you\ncould possibly need from the outset.\nSo assuming you can create this suite of functions designed to work together\nthat the whole team will share and contribute to, it’s fairly natural to put\nthem into a software package or library. One reason for this is that the\ncombination of package name and version give you what you need create\nreproducible work. The package can be updated, but so long as you know the\nversion number your project depends on, you can install that version from the\nsource code repository, and your code won’t break due to changes made by\nothers.\nWhy is this important? Well I think one of the main reasons templates backslide\ninto copy-pasting your own bespoke versions of said templates is because it can\nfeel like ‘there’s not enough time’ to understand what (if any) changes have\nbeen made to the centralised version by others, and how to incorporate them\nwith your own. A package acts as a kind of ‘forcing function’ for better\npractices that make this easier.\nAn example practice is automated testing. This is a practice typically\nassociated with packages, but not configurable templates. A test suite can give\nyour team the confidence to merge multiple streams of changes together, since a\npassing test suite becomes strong evidence that the changes mesh together\nnicely!\nYou may be wondering about the overhead of building and maintaining software\npackages. People who haven’t done much of it usually do. The trick is that\nthere are software packages that contain sets of powerful functions dedicated\nto creating and maintaining software packages. In R these are {devtools},\n{testthat}, {usethis} et. al. With these I can create a fully compliant and\ninstallable R package, backed by a git repository, in less than 10 minutes. In\nmost decent programming languages I have used this is also the case. Nothing\nexceptional going on here.\nThat fact that the pattern of breaking a problem domain into powerful functions\nthat are packaged into modular units can be recursively applied to solve the\nproblem of creating packages of functions should be some evidence that this is\na scalable technique for managing complexity. And it has been complexity, in\nvarious guises, that has wrought havoc on the reuse techniques mentioned up\nuntil this point.\nCaution: Oversized\nI’m going to warn you about a pitfall here, but unfortunately it probably won’t\nsave you. Every workplace I have seen turn toward software packaging as a tool\nfor data analysis reuse has fallen into this pit. Perhaps we are all destined\nto.\nI surmise it’s to do the fact that initially creating a software package can be\ndaunting. There’s a bit to get your head around. Conventions to learn. So\nsince it seems like it’s going to be a slog, maybe teams imagine that this is a\ntask they’re going to undertake once. If they work for ACME company then\nthey’re making a single package: {aceme}, {acmefun}, {acemetools}, {acmeutils}\nor some such generic concept like that.\nThe problem with this idea is that eventually complexity overcomes here as\nwell. One reason is that it’s generally considered bad juju to remove functions\nfrom a package. You typically do a thing called ‘deprecation’ where you make\ncalling the function throw threatening warnings to stop using it. And you leave\nthose warnings in place for a long while before you hit delete, so that all the\npackage’s users have a chance to update their project sources.\nSo if it’s easy to add, but hard to remove, package bloat is a risk. In a\nbloated package it gets hard to find things. Reading the documentation becomes\na chore. In their bloat-induced-ignorance people may add functionality to it\nthat is a rehash of stuff that already exists in a different form, further\nexacerbating this problem.\nI also believe monolithic packages encourage bad software design. Assumptions\nabout the internal implementations of functions or the internal data structure\nof their outputs may bleed out into other areas of the package, causing them to\nbecome coupled by those assumptions. In a sprawling code base these kinds of\ncouplings are not always clear, so in practice this means you run into\nunexpected challenges when rewriting things to work better, since you\nunwittingly violate assumptions made elsewhere, making more work for yourself.\nBy contrast, in a modular system of tightly focused software packages, it is\nmore obvious that knowledge relating to implementation details should not\ninfluence code across package boundaries. To do so would imply the two packages\nhave to be updated in a synchronised fashion, and that sounds like an awkward\nbad time, because it is.\nRelated: One function to rule them all\nI’ve seen a similar (anti)pattern to ‘one-package’ play out with functions.\nThis seems to catch out people who are transitioning out of a script-based\nworkflow with lots of duplicated code, into a style based on functional\ncomposition.\nMassive functions get written with perhaps a dozen arguments, containing\nhundreds of lines of code, that are really not much more than wrangle_data.script rebadged as:\n\n\nwrangle_data(\n  x, \n  y, \n  z, \n  cubits, \n  flotsits, \n  n_widgets, \n  calibration_factor,\n  tolerance, \n  initial_state, \n  start_date, \n  end_date, \n  output_path\n  )\n\n\nSure we’re using functions, but we’re actually attempting to template the\nentire solution using the function’s arguments. Many of the pitfalls of\ntemplates apply 4.\nSuch a function is pretty much un-testable due to the combinatoric explosion of\npossible sets of inputs. And you can bet that the internals are not written in\nhigh level domain-specific code. When I look at that function signature I hear\nthe screams of matrices being ground to a fine powder of bits through the conical\nburrs of nested for loops, and laboured index arithmetic.\nFunctions with these properties are hard to update with confidence, so they\ntend to set like concrete. Discrete sections of code that might be more widely\napplicable are locked up inside, unless… you could copy-paste them out? ;)\nAs per concrete pretty much the only way to fix these functions is to break\nthem up do over (with modular re-usable pieces!).\nYour personal ’verse of packages\nWe’re getting to it now! When we were discussing the ‘one-package’ pattern we\ntalked about how the pattern of breaking down data analysis code into reusable\nfunctions stored in packages is a recursively applicable method for managing\ncomplexity in reusable code - that is we can use the same idea to break down\nboth analysis projects and monolithic packages into a modular ‘universe’ of\npackages that are interoperable.\nA healthy universe will likely contain packages spread out along the ‘ladder of\nabstraction’ - that is we’ll have a mix of user-facing tools (high level), and\nlower level packages that contain common code that powers those tools (low\nlevel).\nExamples are probably useful here. In my current job we have a small package\ncalled {checkurself} that facilitates creating and running list of quality\nchecks on a dataset. This package can be used directly, but it shows up as a\ndependency in a higher level package called {tdctargets} which contains\nmacros5\nfor pipeline construction. There are macros that\ndefine certain kinds of pipeline targets (inputs / outputs) that are quality\nchecked, with the quality check results automatically becoming a separate\npipeline target. You can roll with a default quality check, or you can use\n{checkurself} directly to plug in your own.\nOther examples of lower level packages in your universe might be things that\nwrap up API endpoints or SQL queries to provide easy access to commonly used source data.\nThese might show up as dependencies for higher level packages that make\nfrequently used visualisations of those data.\nIn my last job we were endlessly plotting fire stations on maps. So it made\nsense build out tooling to the point where you could just do something like:\n\n\n\n{qfesdata}, {snapbox}, and {qfesvis} were the in-house built packages\nthat wrapped up a surprisingly large amount stuff: database queries, API calls,\ngeospatial data transformation, plotting aesthetic mappings etc. Making\nbeautiful maps went from hours down to minutes.\nWe also retained the flexibility to build off in new directions on top of what\nwe had, without the weight of copying hundreds of lines of code around.\nOn the subject of identifying niches for internal packages, I really enjoyed\nEmily Reiderer’s essay\non the topic, which introduces the idea that internally developed packages are\nkind of like extra expert team members.\nYou can give your software a graceful death\nWith the package universe, the problem of ‘easy to add difficult to remove’\nthat drives individual package bloat is a bit less severe. If a package becomes\nsuperseded or irrelevant, you can just stop taking a dependency on it, and\nforget about it. Unlike deprecated functions that have to live on guarded by\nwarnings, you’re not going to be constantly reminded of a superseded package.\nYou won’t have to navigate around its source and documentation when they show\nup in your searches.6\nSnap back to reality\nI’ve presented a shared universe of domain specific software packages as an\nidealised solution to data analysis code reuse. There are several problems with\nthis idea in practice.\nThe first is how and when do you actually build out the universe? Typically you\ncan’t design this type of thing up front. You might know from experience that\nyou’re certain to need a few pieces, but there will always be niche domain\nspecific tasks that you do not recognise as common components of the workflow\nuntil you’ve actually performed the same flavour of data analysis a few times.\nThe implication here is that you’re going to need to adopt a ‘crafts-person’\nstyle approach, where your tools are continually honed, and their effectiveness\nat producing superior results continuously examined. This should lead to\nfeedback that improves the tooling, creating the headroom on top of ‘business\nas usual’ that you can use to improve the tooling in the future. It’s a\nvirtuous cycle.\nIt’s probable you’re going to have to start this virtuous cycle from less than\nvirtuous, or ‘humble’, beginnings. And you should be fine with that. The\nimportant thing is to keep applying the pattern of breaking down the work into\npowerful domain specific modular tools. I always idealise this process like a\nratchet. Ideally the when the team ‘clicks forward’ we lock in capability in a\nway that can’t be wound back.\nTo lock in capability the tooling has to be well designed and built. So\npractices like ‘dogfooding’7, seeking peer feedback\nearly, formal code review, and software testing become highly relevant to\nmaking this happen.\nThe second problem is that it’s all well and good to have this universe of\npowerful tools at your disposal, but starting a known type of data analysis\nfrom scratch every time, typing out the same boilerplate code to load your tools,\nand ingest the data can feel like a bit of a chore.\nThis is where a little templating can go a long way. My advice is to avoid\nbuilding the complete machine (no config!), and rather conceive a lightweight\nproject skeleton, that includes boiler plate code, and implies the overall\nproject structure8,\nbut leaves the user to flesh out the (hopefully) interesting parts.\nConclusion\nI presented four patterns for reusing data analysis code:\nCopy-paste from stuff you did earlier\nA Centralised template\nA shared software package\nA shared universe of software packages\nAnother way of viewing this sequence is as a kind of march of progress like this:\n\n\n\nFigure 6: ‘The March of Progress’ of humanity’s ancient ape-like ancestors into modern humans. From ‘Early Man’ (1965).\n\n\n\nEvery data analysis team I have worked on has gone through this march, although\nsome were already more advanced when I joined. I am not sure if it’s possible\nto skip evolutions. It it may only be possible to ‘speed run’ our teams through\nthem.\nThings being increasingly ‘centralised’ or ‘shared’ was a common thread, but I\nhave completely neglected cultural challenges that can arise with doing this.\nSuffice to say for now: establishing and maintaining trust and shared values\namong the team is a necessary condition for effective reuse of shared\nresources, and progressing through the stages.\nPerhaps you have seen teams even further along this march than I have. I’d be\neager to hear what lies beyond!\nPresented with thanks to Nick Tierney for reviewing this post. It runs with some ideas from a talk I gave in 2022.\n\nThe sort of mind that will tolerate the guts of computers, and the various ways they can torture data, does so out of a kind of insatiable curiosity. That curiosity can be harnessed to great productive effect. But remove the opportunity to be curious, and curiosity will vest elsewhere, and morale will suffer.↩︎\nThe interest on technical debt is debited at the least opportune time.↩︎\nPerhaps you can see why if you don’t use something like Quarto\nto generate your documents, you increase the surface area for ‘copy-paste gone\nwrong’ type mistakes to occur on greatly, since you need to execute\nsynchronised copy-pastes in source code and document files.↩︎\nIf a function starts taking YAML configuration files as arguments you are on borrowed time.↩︎\n Okay in R, they’re ‘target factories’ that depend on R’s special flavour of meta-programming.↩︎\nIn reality you probably are because you’ll have that one colleague that will cling to it.↩︎\nthat is ‘eating your own dogfood’ or using your own tools to understand their strengths and weaknesses↩︎\nIt’s a little appreciated fact that deciding how to structure things is actually the ‘deciding how to name things’ problem raised to a power↩︎\n",
    "preview": "posts/data-analysis-reuse/copy5.png",
    "last_modified": "2024-03-13T00:43:28+10:00",
    "input_file": {},
    "preview_width": 1040,
    "preview_height": 583
  },
  {
    "path": "posts/tribe-evamos-longtail-cargo-review/",
    "title": "12 months and 2700km with the Tribe Evamos Longtail cargo ebike: Review",
    "description": "I review our car-replacement cargo ebike one year on.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.com"
      }
    ],
    "date": "2023-10-23",
    "categories": [
      "review",
      "cargobikes"
    ],
    "contents": "\n\n\n\nLast week marked the one year anniversary of the purchase of our Tribe Evamos\nlongtail cargo ebike. During this time we’ve racked up a whopping 2700\nkilometres of trips to kindy, school, supermarkets, pool, gymnastics, library,\nparks, doctors, cafes, cinemas, and this list goes on. Should you buy this bike?\nYes (probably). Is it perfect? No. It has some flaws. Read on for my analysis.\nBackground\nMy family was not new to getting around by bike. For a couple of years, we had\nbeen getting around locally using an ebike to tow 2 kids in a Thule bike\ntrailer. We started looking into cargo bikes around September 2022, since our\nboys were getting too large for the trailer, and the\nnew school run seemed to demand something a bit more nimble.\nI’d been a commuter cyclist for 8 years, and my partner had been an on / off ebike\ncommuter for the last couple of years. It was important that the cargo bike\ncould suit us both.\nComparing the Market\nThe frustrating thing about searching for an electric cargo bike capable of replacing a car in Australia is the market is essentially a binary choice between:\nCheap sketchy drop-shipped-from-China brands with flimsy looking attachments in the $2K-$3K bracket.\nHideously expensive overseas imports with sticker prices upward from $8K before you even start to accessorise.\nAn Australian company called Tribe have made a sharp play for the middle, pitching a complete Evamos rig for around $5K on top of which you’ll probably pay another $400 for decent kids seats.\nIn my opinion we can judge this play as a success if:\nThe Evamos is good enough quality to function as a daily family “driver”. I.e. it maintains decent battery range, and doesn’t fall to bits in the face of abuse from two rambunctious kids, or occasionally the adult pilot.\nThe Evamos is just comfortable and ergonomic enough, that the design and concessions made to drive down cost don’t grate on you over time, and push you to the point where you wish you’d ‘just paid the extra’ to get the swish Euro ride with the telescopic dual-latching seat post.\nAs a Daily Driver\nMost of the time our Evamos is configured as a people mover. We have it kitted out with the front basket, the running boards, the Crew Cab safety enclosure, a Polisport Guppy seat for the older child, and a Thule Yepp Maxi for the younger.\n\n\n\nOver the course of a typical week, it’s in service every single day. The staple\nrides are to the school1, to the shops, or\ninto the city for some entertainment. I’ll usually charge the bike’s battery\ntwice over the week, for which I’ll pay the princely sum of about 18 cents. I am\ncompletely satisfied with the battery life we get.\nMy partner and I have been caught out one time each needing to haul kids with no\nbattery. There is a bit of a trick with the battery indicator, in that it is\nnon-linear2. What this means is that the first 3 bars of\nbattery will last you 2x or 3x as long as the last 3 bars. And when you’re on\nyour last bars, the power from the pedal assist drops off - It’s still helping\nbut not as much.\n\n\n\nLuckily the ‘granny’ gears (1 and 2) on the bike mean you can get going, and even tackle moderate hills hauling kids or cargo with no electronic assist at all. It’s just going to take you a while, and you’re going to sweat.\nOver the year we’ve had it I’ve seen a bike mechanic twice. The first time was to replace the brake pads, and check if the gears needed indexing - they didn’t, the chain just needed a clean and lube. The second time was to bleed the brake lines which had air built up in them. This is minor stuff, and I think I paid around $120 each time.\nBoth times the mechanic has made a comment about a lot of bolts on the bike being looser than they should be. For stuff like the running boards, and the crew cab it’s not a big deal, but he reckons the handle bar bolts really shouldn’t get loose, although it is a thing he sees with manufacturers that use a certain kind of cheap bolt. I’m checking all these more regularly now.\nSo my conclusion, based on 1 year of daily use, is that Tribe have built a machine that functionally stands up to the everyday tasks you’ll likely want to throw at it, and they have succeed in achieving criteria 1. from above.\n\n\n\nComfort, ergonomics, and power\nOne of the things that is limited on the Evamos compared to the higher end bikes\nis degree of adjustment. You have a small amount of play in the handlebar height\nand angle. You have the seat height. That’s it. And to me it feels like maybe\nthe configurable range of the seat and handlebars caters better to taller riders\nthan shorter.\nWe have our handlebars as high, and as swept back as they can go. But at my\npreferred seat height, I’m hunched just a little more over than I would like. My\npartner is short, and she uses the seat as far down as it can go, bottoming out\nthe post, so possibly she could go a little lower. So neither of us can quite\ndial in our ‘ideal’ position.\nI’ll also point out that although the stem is billed as adjustable, you need an\nallen key to do it, and you want to make sure it’s done up really tight for\nsafety, so it just doesn’t feel practical to tune that every time you swap\nriders, which for us is basically every day.\nAt this point though it hasn’t been a big issue. The ‘good enough’ position we\ncan both get by setting the seat height is just that. We’re not touring on this\nthing, it’s a daily run about. The longest trip we do regularly is to the city\nlibrary which is probably about 30 minutes in the saddle each way. Neither of us\nare getting any aches or pains from that.\nSpeaking of saddle, the seat was one thing I expected to be rubbish, and that\nwe’d need to swap, but somehow the stock seat that comes with the Evamos is\nactually a pretty nice one. Neither of us has had any trouble with it, unlike\nprevious bikes we’ve purchased.\nThe kids are happy with their setups too. They prefer getting around by bike to\nthe car. They often insist on riding, and oddly, even more firmly when it’s\nraining! There’s not a lot of space once we’re all lined up, but somehow it’s\nenough. No one whinges. My eldest son will occasionally jab me in the back with\nthe peak of his hat, but he’s mostly too busy checking things out to be looking\nat my boring back. As the kids get larger we’ll be able to change the seating to\nopen up more space.3\nBut how does it feel? When you’re riding unencumbered it feels surprisingly\nlike a normal bike. It’s a little on the heavy side, but not unbearably so. And\nit’s a very smooth ride. More so than something like a Tern with small wheels\nfront and back. To me, these feel more twitchy and ‘scooterish’.\nWhen you add kids or cargo, it takes on a more ‘planted’ feel. Kind of like\nyou’re on some kind of watercraft. At very low speeds, or when stationary,\nwriggling kids can make the balance feel precarious. As soon as you’re moving\nwith any speed the Evamos is a stable, smooth, and confident ride.\nAs for power, I do live in a pretty hilly area so I feel qualified to comment.\nFor reference, I’d estimate our load with 2 kids and some gear to be ~145kg. So,\nfully loaded, the Evamos has enough power on the max assist level to get us\ncomfortably up the kinds of hills that would leave me a little bit hot and\nbothered on a regular bike by myself. What I have found by experimentation is\nthat if I take on a hill that looks like it would be seriously hard work on a\nregular bike, it’s still seriously hard work with 2 kids on the Evamos.\nSo for my partner who is smaller, and less cycling fit than I am, this means\nthere is one hill in particular on our markets route that I feel is within range\non the Evamos, but she feels is best avoided if she’s riding. Something to\nconsider if you’ve hills between you and key destinations.\nIt’s worth noting that the 250W Bafang M400 motor on our Evamos has the maximum\npower rating allowed for street-legal ebikes in Australia and Europe. So it’s\nnot really fair to say the bike could have been made more powerful. But I do\nhope that one day there is a legal class of higher-powered motors permitted for\nebikes designed to carry multiple passengers (especially in hot places like\nBrisbane!). The speed limiter can stay at 25km/h.\n\n\n\nFlaws\nSome of the things I will discuss here only apply to the first-generation model\nwe bought, and have since been addressed. But I figure these are handy to know\nabout in case you’re looking at buying an Evamos secondhand.\nDouble top tube clearance\nIt’s easy to miss in the photos of the bike online, but the Evamos’ frame actually has two top tubes that branch off the head tube forming a kind of triangle that joins the rear ‘rack’.\n\n\n\nThis creates an issue for me, but not my partner, whereby if I jump on and adopt\nmy natural riding posture, my calf muscles will rub on the\ntwo-top-tube-triangular arrangement. I have to widen my riding stance a little,\nplacing the pedal more toward the front of my foot, and shaping my leg outward a\nlittle to avoid this. These days I hardly ever notice it happening, but it was\ndefinitely something that had me taken aback when we first got the bike.\nWhether this affects you is probably very dependent on your body geometry, and\nyour sensitivity to the rubbing sensation. But it’s definitely something to\ncheck out in a test ride.\nHigh step-thu\nThe Evamos’ frame has a ‘step-thu’ look about it, but the sweep of the top tubes comes quite high. So much so that my shorter partner still has a bit of difficulty getting her leg over when the kids are on the bike.\nFork mounted front rack (now fixed)\nOur Evamos shipped with a front rack that was mounted on the front fork close to\nthe wheel axel. This meant that adding anything more than a small amount of\nweight to the front basket made the steering feel awful, heavy, and prone to\n‘flopping’. To the point where I would say it was genuinely dangerous to fill\nthe front basket. We’ve since replaced the original front rack with the\nframe-mounted version that now ships as stock configuration. It completely\nmitigates this issue. So I’d advise you to make sure the Evamos you are thinking\nof buying has the frame-mounted rack or factor getting it into what you pay.\nChain bounce damping wheel (now fixed)\nSince the Evamos has such a long chain it is prone to bouncing around, perhaps\neven out of position, when going over big bumps. The original solution to this\nwas a small rubber wheel mounted on an arm. But the wheel was set too high, which\ncaused the chain to rapidly eat into it, and eventually saw it in half.\nThis happened to us after only 20 or so kilometres.\nWe ended up having a fair bit of correspondence with Steve at Tribe over this issue. Multiple replacement\nwheels and arms were sent, until one day a new design arrived out of the blue,\nand the issue was solved. The chain now sits over a lower wheel, within an\nenclosure, that keeps the chain from bouncing around too much. Make sure the\nEvamos you are buying has a black metal enclosure that holds the wheel, and the\nchain, not just a wheel on an arm. Get the enclosure from Tribe if not.\n\n\n\nChain clearance in first gear\nIn first gear, the chain makes light contact with the rear tyre. It could be\nthat this is a setup issue, but it’s been like this since the bike was shipped\nto use with the rear wheel on. This is not really a practical problem since you\nwill almost never have cause to use the first gear, and if you do, you’re\nprobably in a state where you couldn’t care less about the rubbing noise. But it\nis a rough edge that probably shouldn’t exist on a $5K bike.\n\n\n\nMIK stability (?)\nThe Evamos is compatible with an accessory attachment system called MIK. I have\nnoticed that there is perhaps more wobble than you might expect in our front\nbasket when attached using this system, on both racks we have used. This\nwouldn’t have rated a mention, except for the fact that a lady in my\nneighbourhood also bought an Evamos, but she went the route of using the MIK\nadaptor on the tail to attach a child seat. She wasn’t really comfortable with\nhow much the seat wobbled around, and was impressed with how firmly seated our Yep\nMaxi is, which does not use the MIK system. I never saw her setup so this is\nsecond-hand information, but it was corroborated by the wobble in our basket, so\nI thought it worth a mention. See if you can check out child seat mounted using\nMIK before you buy.\nGood stuff\nAlright, onward now to some things I really appreciate about the bike.\nRear cassette and derailleur\nMany of the expensive cargo ebikes are using internally geared hubs. I actually\nlove internally geared hubs, and have used a three-speed on my commuter bike for\nmany years now. However you need to treat these hubs well or they can fail, and\nthen things get very expensive.\nI had a Shimano Nexus fail due to moisture ingress, and replacing it nearly cost\nme the price of the whole bike again. I also have a friend in the area with an\nUrban Arrow who has gone through two Enviolo hubs, and is now contemplating\npurchasing a Rholoff ($$$). He reckons hauling his kids up hills is just too much\nfor the stock internal gears.\nThe use of a simple 1 x 8 speed single cassette and derailleur on the Evamos is\na choice I respect. If any of the non-electric bits fail the replacement will be\na readily available commodity part, not something that needs to be specially\nordered in from far away.\nSmall rear wheel\nTribe didn’t invent this idea, but the slightly odd-ball looking combination of\na 20” rear wheel and a 24” in the front is a very practical compromise,\nespecially if you plan to cart children.\nUnlike static loads, children wriggle, rubberneck, fight, fall asleep and flop\netc. You’ll feel those movements actually change the bike’s direction a little\nbit, sometimes requiring rider compensation. The more leverage you give the\nkids, by placing them higher, the more they’ll be able to shift the bike off\ncourse by throwing their weight around. A lower wheel reduces the kids sitting\nheight, and also their leverage.\nThe front of the bike has some different trade-offs though. If you make the\nfront wheel smaller it will be knocked around by bumps (or sticks!) more, and\nthe steering will feel more dynamic (‘scooterish’). On the Tern bikes with 20”\nfront wheels, shock absorbers are added to try to mitigate the bump factor, but\nthe handling still feels twitchy. I have ridden a Tern with a child on the rear,\nand the kid wriggles combine with the twitchy handling to create a completely\ndifferent ride. It doesn’t exactly feel unsafe because the Tern has such a low\ncentre of gravity - But it’s a much more dynamic ride than the Evamos which, as\nI have said, is smooth, and somewhat boring by comparison.\nAesthetics\nFor certain this is down to personal taste, but I appreciate curves. There’s a\nretro vibe about the curves within the Evamos’ frame, that makes it somehow a\nbit of a throwback to bikes of times past. Bikes that were more celebratory of\nthe basic human fun, and empowerment to be found in cruising around by bike.\nBikes from before cycling became fully sportified, and the design language\nshifted to communicating performance.\nIt’s not a bike that says “I’m for going fast”, “I’m for going off-road”, or\neven “I’m for carrying lots of heavy shit”. It’s more like “I’m for having fun”,\nand yeah if you get one of the bright colours there’s a bit of “Check out much\nI’m having fun” in there too.\nIt turns out the design is a bit of a beacon. Definitely do not buy this bike if\nyou don’t enjoy random strangers striking up conversations about ebikes, cargo\nbikes, riding with kids, replacing cars, urbanism etc. Because this quite\nliterally happens to me on a weekly basis. It’s getting funny now because often\nthe kids will take the lead evangelising #bikelife, and I’ll be left trying to\nget a word in edgewise.\nCustomer Support\nI don’t think this is featured enough in reviews of products. When\nyou buy a product you’re buying into someone’s idea of customer support, and\nthat can either be an absolute drain that sucks your time, or a delight that\ncompletely reverses any bad feelings about the product you were having due to\nsome kind of problem.\nAs it happens we had two significant issues that started on day 1 when the bike\nwas delivered in a torn box, with a small scratch on the frame, but more\nimportantly it was missing the battery. I believe that the package was\ntampered with by someone who had access to it during shipping, and the battery\nwas stolen to make a few hundred bucks.\nSteve from Tribe was great to deal with, and in short order, we had a new battery\non the way, and some credit toward accessories for our trouble.\nThe second issue we had I’ve discussed already: the rubber chain bounce damper\ngizmo wheel. This was a bit interesting because I would say at first Steve was a\nlittle sceptical there was a design flaw, and he asked me to check a few things,\nbut later he called me back and admitted that there was indeed evidence of the\nproblem on other test bikes, it just hadn’t been noticed before. Steve argued\nthis kind of thing was to be expected on a first-generation product, and seemed\nrelieved, when I, a software developer, agreed.\nHe sent me some newly designed parts, and when they led to the rubber wheels\nlasting longer but still failing in an unreasonable amount of time, he sent us a\nwhole bag of rubber wheels to use as a stop-gap until the ultimate design they had\nin the works could be shipped. As I said, that came unprompted, as promised,\ncomplete with detailed installation instructions. And there’s been not a hiccup\nsince.\nI’m giving Tribe top marks for customer support. Well done Steve.\nVersus Alternatives\nI’ve already strewn a bit of ‘versus Tern’ throughout this review. These are some more general thoughts about styles.\nIn Australia you have three main choices of style for a car-replacement cargo ebike:\nLongtail, like our Evamos\n‘Bakfiets’ or a two-wheeled bike with a bucket in the front like the Urban Arrow\nA three-wheeled bike with a large box in the front. Tribe’s frist bike was one of these.\nI haven’t ridden a trike, so I can’t comment much on that. It seems like it\nwould be a bit of a hassle weaving through the crowds at school pickup / dropoff\netc. And you’d be a bit more limited in places you can park it.\nFor me the bakfiets is the main contender against the longtail. I’ve ridden\nbakfiets’, I think they’re very cool, but as far as I am concerned this is a\ndesign conceived by a person from a cool climate.\nI live in a hot climate. When kids heat up they get shitty, and what you don’t\ndo is cram multiple hot shitty kids together in a small box, where they will\nbegin to sweat, fidget, and whinge about their brother touching them. All of the\nbakfiets I’ve seen have this problem. Unless the kids are both under about 5,\nthe fit in the bucket is too tight for comfort in the heat.\nIn my humble opinion, the Longtail is the hot weather solution. It maximises\nairflow over everyone’s bodies. The kids can stretch out and wave their limbs\naround. I have seen sunshade-type accessories for Longtails as well as Bakfiets.\nAlthough none are offered by Tribe. Honestly, we haven’t needed it. We use\nsunscreen, long-sleeve shirts, and helmet covers with peaks and neck flaps, and\nwe’re doing okay.\n\n\n\nConclusion\nI’ve already said that I think Tribe has nailed a level of quality that permits\nthis bike to become a family’s reliable daily ride. I’ve discussed what I\nbelieve are some of its limitations and flaws, and so the questions that remains\nis:\nNow that I know for sure how well my family takes to the cargo #bikelife, do I regret not finding the extra cash for a more expensive bike?\nAnd the answer is: I do not!\nFor bikes that are very similar to the Evamos, I don’t see how the extra expense\ncould be justified. The Evamos is good enough but far cheaper. For bikes that\nare a little different, but with tons of nice features etc like the Terns, it’s\nnot a clear cut. I’d still be losing features I think are valuable, and paying\ncrazy high prices for the privilege.\nSo yes, I can recommend the Tribe Evamos to anyone looking for a car-replacing\ncargo ebike for local family transport. But do try to ride before you buy to\nmake sure its limitations work for you.\nThe way I pitch this bike to car people is like this: It’s probably destined to\nbe an Australian classic in the same way a car like the Hyundai i30 is. It’s\nreliable, and economical transport that has just enough features, and wins\nbecause of what you get for the price.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuck yeah no school car park hell↩︎\nThis is pretty common↩︎\nWord of warning: If you give the kids ice-creams or cream\nbun-type things you need to go really easy on the brakes or you’re going to wear\nit on your back.↩︎\n",
    "preview": "posts/tribe-evamos-longtail-cargo-review/evamos_city.jpg",
    "last_modified": "2024-03-11T22:26:12+10:00",
    "input_file": {}
  },
  {
    "path": "posts/assertive-programming-for-pipelines/",
    "title": "How to be assertive about not testing your data science pipeline",
    "description": "An introduction to assertive programming as an alternative to testing",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2023-04-23",
    "categories": [
      "rstats",
      "workflow"
    ],
    "contents": "\n\n\n\nFigure 1: A solidly built pipeline, or is it?\n\n\n\nThis is a short post introducing assertive programming as an alternative to traditional software testing that probably makes more sense for data science pipeline code.\nGetting assertive\nAssertive programming is a useful defensive programming technique where we try to identify problems with our data early, and throw an error rather than letting that problem propagate deeper into the program where it might cause all sorts of weird and wonderful bugs.\nIf you’ve ever written an #rstats function there’s a reasonable chance you made some attempt to validate your inputs. Congratulations you’re assertive programming. For example:\n\nmy_function <- function(name) {\n  stopifnot(length(name) == 1 && is.character(name))\n\n  ...do stuff\n}\n\nThis type of example is common in R because all our basic data types are vectors, but a lot of the time we want to treat them conceptually as scalar values, and not have to worry about handling vectors of longer lengths. There are some nice helpers in {rlang} for performing this kind of check.\nLately, assertive programming has become of interest to me because I’ve realised that for data science pipeline code, it makes more sense to invest effort in assertive programming rather than traditional software testing. This has resolved a long-held awkward feeling for me about the thousands of lines of code per project that are written and tested interactively against datasets, but rarely have any more rigorous testing applied.\nPrefer being lazy to absurd\nThe reason no tests ever got written on my pipeline code is that it felt too awkward. Pipeline code is not generic like the code that gets written into functions and packaged. It’s specific to the datasets at hand, and typically makes many implicit assumptions about their structure and contents. Coming up with variations of our data that we might need to handle in our specific sitiutation can border on the absurd.\nFor example, say we had a function in our pipeline that does some kind of classification or binning of data:\n\n\nlibrary(dplyr)\nadd_record_classification <- function(dataset) {\n  dataset |>\n    mutate(size = case_when(\n      body_mass_g >= 4000 ~ \"big\",\n      body_mass_g >= 3000 ~ \"medium\",\n      body_mass_g < 3000 ~ \"small\"\n    ))\n}\n\n\nThis example is so simple we wouldn’t bother testing it, but imagine the classification was a bit more involved and the code makes us a bit uneasy.\nIf we tried the traditional approach and fired up {testthat} to write a test for add_record_classification() the very first hurdle we’d face is to create some test data. And this is where things get start to get awkward. We already have a dataset… so should we carve off some of that and use it as test data? That would be an oddly specific test. But then should we create a test dataset that is different from the dataset we know we plan to use for the sake of testing the function?\nIf you’ve done a lot of testing you might say, “Ahah but what if body_mass_g is NA?”, and that would be a useful thing to think about. But it turns out our current dataset doesn’t have any NAs for now. Should we invent a test dataset that contains NAs we know we don’t have for the sake of it? That would force us to write new code to address a problem we don’t have.\nI mean sure the data might change, that’s a valid perspective. But writing robust NA handling for the sake of passing a test, for a case we know we won’t encounter for real now is very speculative. In my opinion, it’s better to take a lazy approach, and put in the NA handling code if and when that situation arises. Assertive programming is how we detect when that situation arises:\n\n\nadd_record_classification <- function(dataset) {\n\n  if (anyNA(dataset$body_mass_g)) {\n    rlang::abort(\"NAs are present in 'body_mass_g' column\")\n  }\n\n  dataset |>\n    mutate(size = case_when(\n      body_mass_g >= 4000 ~ \"big\",\n      body_mass_g >= 3000 ~ \"medium\",\n      body_mass_g < 3000 ~ \"mmall\"\n    ))\n}\n\n\nIf you’re scoffing, remember this is pipeline code. It’s designed to take a specific known dataset and generate some specific desired output. There is not a universe of possibilities to handle here. If the code genuinely is generic, then it likely belongs in a package, where traditional testing makes sense.\nYou may agree with my approach but be uncomfortable with being so lazy because your pipelines are a long-running process. It would suck to get that error 30 minutes into a run - that’s for sure. But if that’s the case I’d argue you can do a lazier and better job of things by converting your pipeline to use {targets} and then that problem goes away. You can resume processing from close to where the error occurred.\n{testthat} as an assertive programming tool\nThere are lots of great tools in R for assertive programming. An interesting find for me is that {testthat} is incredibly useful in this regard, due to the detailed errors it gives when it finds conditions that aren’t met. For example, quite often after combining a bunch of datasets or sending them through a pipeline of manipulations, I want to assert that I have not inadvertently changed the length of the output dataset either by accidentally dropping rows or accidentally introducing duplicates:\n\nlibrary(testthat)\nmake_my_rectangle <- function(dataset_a, dataset_b, dataset_c) {\n\n... Do stuff\n\n  expect_equal(nrow(output_dataset), nrow(dataset_a))\n  expect_false(any(duplicated(output_dataset$id)))\n\n  output_dataset\n}\n\nIf the number of rows was changed we see:\nError: nrow(output_dataset) not equal to nrow(dataset_a).\n1/1 mismatches\n[1] 200 - 344 == -144\n{testthat} adds a nice amount of numeric detail to the error report.\nThese assertions become useful yardsticks when working backward from a bug in the pipeline to find the source. They save you from loading up the intermediate datasets from your {targets} cache to check this stuff manually. The code they relate to can be excluded as a possible source by eye.\nConclusion\nI’m really enjoying {testthat} for my in-pipeline assertive programming at the moment. I can use the same kinds of patterns I use in my package code tests, which reduces my cognitive load as I context switch. And the fact that one package can span both the testing and assertive programming use cases possibly hints at a deeper relationship between the two. In my opinion, assertive programming can be employed as a kind of dual of traditional software testing, conveying equivalent benefits in the right context.\nThere’s one significant advantage assertive programming has over traditional testing for pipelines that is worth mentioning and remembering if you’re ever challenged about it: by design, software tests run outside of the ‘production’ pipeline code. The test suite is separate from the main codebase that gets the work done. A consequence of that is even though the tests exist, there is nothing that guarantees they all passed, or indeed even ran, before the pipeline code was executed. By moving the test conditions into the pipeline and expressing them as assertions, we guarantee all the conditions were met, otherwise, the pipeline would have failed to complete. This is a very useful guarantee from a reproducibility perspective.\nSome other packages I’ve used, and recommend for assertive programming:\n{assertthat} It’s a lightweight assertion package by Hadley, the reporting is not as nice as {testthat} but it has some handy one-liners for common assertions.\n{pointblank} This is a heavier-duty package that helps you validate all aspects of datatsets with extensive reporting capabilities.\nTools suggested by #rstats community members on Mastodon:\n{assertr} Includes many flavours of data frame checks.\n{checkmate} Rich facilities for function argument type validation.\n{contentid} A complementary tool for ensuring input data has not changed.\nIn conclusion, I hope I’ve helped you feel better about not testing your pipeline code. You know what to do. Be assertive about it!\n\n\n\n",
    "preview": "posts/assertive-programming-for-pipelines/pipelines.png",
    "last_modified": "2024-03-11T22:19:01+10:00",
    "input_file": {},
    "preview_width": 1300,
    "preview_height": 867
  },
  {
    "path": "posts/vector-tiles/",
    "title": "Push the limits of interactive mapping in R with vector tiles",
    "description": "A tutorial on mapping with vector tiles in R",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2022-04-10",
    "categories": [
      "rstats",
      "vector-tiles",
      "spatial"
    ],
    "contents": "\n\nContents\nWhat are vector tiles?\nMapbox vector tiles\nMBTiles databases\n\nHow my team use vector tiles\nHow to make vector tiles\nTutorial\nGetting the data\nGenerating the tiles\nMapping the data\n\n\nServing vector tiles in production\nMapbox\nAWS lambda\nOther options\n\nConclusion\n\n\n\n\nI can recall being giddy with power when I discovered how easy it was to make those fancy interactive web widgety map whatsits with leaflet and R.\nThey neatly solve the ‘spatial boundary problem’: exactly how much map to show to give enough context for the data, without the data appearing so small as to be unintelligible. With interactive there is no boundary! Users set their own context. Zoom in, zoom out, discover, and have a ball! Static maps feel like ancient cave paintings in comparison.\nBut there are boundaries still. If you use interactive maps enough - and we’re making them daily to hourly at my work - you’ll find yourself up against all sorts of limits that stop you from presenting your data as you wish.\nThere’s a limit to how many features leaflet maps can handle, because at some point the DOM gets too full and your browser stops being able to parse it.\nOnce you start rendering spatial data on WebGL canvasses instead of the DOM you’ll find there is a low number of WebGL contexts that can co-exist on any one web page, typically limiting you to only around 8 maps.\nThen you’ll try to reuse WebGL maps by toggling on and off different layers of data for the user at opportune times. This is an improvement, but data for all those layers piles up, and your toolchain wants to embed this in your page as reams of base64 encoded text. Page file sizes are completely blowing out.\nPeople can no longer email your fancy pants map reports around. They are not below the limit set by the email administrators. Your users are upset. Barbarians are at the gates. What to do?\nMake vector tiles.\nThe rest of this blog post is all about them, specifically:\nwhat they are\nwhat my team use them for\nhow to make them\nhow to map them\nWhat are vector tiles?\nLet’s start with tiles: If you’ve used interactive maps or ‘slippy maps’ as they are sometimes known, you’re probably aware of tiles. A slippy map (E.g. Google Maps) is comprised of thousands to millions of little images, a selection of which are served to you depending on the spatial viewport you have established on the map as you slip-slide around.\nThese solve the problem of how to serve a high resolution map image to the user. High resolution images of the Earth use a ton of data and bandwidth to transfer, but the typical user is only chasing an image of a very small section that contains whatever they are interested in.\nSo some clever person came up with the idea of cutting up the images of the earth into three dimensional dataset of x, y, and zoom. A tileset, as they are called, contains the whole Earth projected onto an x/y plane, rendered at progressively higher resolution or higher detail, with increasing zoom.\nAs we ‘zoom-in’, or increase our position on the zoom dimension, we are sent only the tile images for our new viewport of the Earth, which contains roughly the same amount of tiles, but those tiles represent increasingly less area of the Earth’s surface. It follows that the number of tile images associated with each zoom level increases geometrically with increasing zoom (by a factor of 4).\nIf you remember the dial-up internet era, you will recall waiting for these little rectangular sections of your maps to ‘pop-in’ over a grey placeholder as they were painstakingly downloaded. Back in the early days of slippy maps all the tileset images were raster images. That is they were actually just small rectangular pngs or jpegs.\nNow we get to the ‘vector’ part: As personal computing devices became more powerful, it was realised that it could be more efficient to transmit tiles in ‘vector’ form, and have the user’s browser process them into the final tile image. Vector tiles contain arrays of annotated spatial coordinate data: The edges of the roads, the boundaries of buildings etc. Not an image, but the building blocks for one.\nThis information is combined with a separately transmitted stylesheet to produce the tile image. Different stylesheets can use the same vector data to produce radically different looking maps that either highlight or omit data with certain attributes.\nAs mappers of data we can use the same trick of streaming tiled sections of our data in vector form to drastically boost the amount of information we can make available on interactive maps.\nMapbox vector tiles\nA vector tile specfication created by the Mapbox company is currently the de-facto standard for vector tile files. Mabbox Vector Tiles, or ‘MVT’ as they are often abbreviated, are stored as a Google protocol buffer - a tightly packed binary format.\nMBTiles databases\nMBTiles is a second specification created by Mapbox that describes a method of storing an entire MVT tileset inside a single file. Internally .mtbtiles files are SQLlite databases containg two tables: metadata and tiles.\nThe tiles table is indexed by z,x,y and contains a tile_data column for the vector tile protocol buffers, which are stored compressed using gzip1.\nThis single-file tileset is much easier to ship around than millions of tiny files, and the SQLite format and gzip compression help with efficient retrieval and transmission.\nThe guide later in this blog post will focus on generating, serving, and viewing MVT tilesets in .mbtiles format.\nHow my team use vector tiles\nAn important challenge that was solved with vector tiles was plotting attributes associated with the entire road network of our Australian State of Queensland.\nIn this case we could not apply the default strategy of simplifying the geometry, since when zoomed in the road network we were plotting had to align with the road network on the basemap, so that viewers could see features that lie along sections of road they were interested in.\n\n\n\nFigure 1: A travel time isochrone spanning the entire Queensland road network\n\n\n\nAnother problem was how to introduce a selection of toggleable ‘reference layers’ our stakeholders wanted without completely blowing out report file size. My team delivers a lot of our work in html format (built in RMarkdown). The default for slippy map widgets is to embed all the data for all the layers into the html document itself.\nEven simplified, together these added up to hundreds of MB of geojson that tanked the page performance.\nUsing vector tiles we can have unlimited reference layers. Each one contributes nothing to the report file size since it is only streamed on demand when required.\n\n\n\nFigure 2: A state-wide bushfire risk reference layer\n\n\n\nOf course that data needs to be streamed from somewhere. A server needs to be involved. We’ll discuss strategies for serving your own MVT tilesets later.\nIn summary you might look at using vector tiles for mapping your data when:\nSimplification is not desirable, e.g. because of alignment issues\nSimplification doesn’t really help, you still have too many features\nYou’re suffering death by a thousand cuts, and cumulatively your datasets are too much to handle.\nHow to make vector tiles\nThere are a variety of tools that you use to can do this. Possibly some exorbitantly priced graphical GIS tools have simple MVT export features, and it would be worth investigating that if you have these in your stack.\nIn my team we use Tippecanoe - a command line tool purpose built by Mapbox. I need to give you two warnings about Tippecanoe:\nOne: Tippecanoe is officially unsupported at present, although it does have commits as recently as 2021. There is a community fork that is being actively maintained at the time of writing. I cannot vouch for the community fork, we’ve never had cause to try it. We have been using the main repository version without any issues for the last couple of years.\nTwo: To use Tippecanoe you’re likely going to have to build it from source yourself. The instructions for this are short, and the process is completely automated, but I appreciate this may create headaches if you don’t have access to a Linux or MacOS system where acquiring the dependencies is made simple.\nNow onward to usage: The Mapbox version of Tippecanoe takes geojson as input and can output a .mbtiles database or a folder structure full of protocol buffer files. We’re shooting for the .mbtiles file.\nSo a rough R workflow looks like:\nIn R, read source data as an sf, and wrangle\nWrite data out to geojson\nOn the command line, convert geojson to .mbtiles using the tippecanoe command line utility.\nTutorial\nWe’re going to use R and Tippecanoe to turn the Australian Bureau of Statistics 2021 Mesh Block dataset into vector tiles.\nFor the outlanders, we’re looking at the smallest published statistical areas that cover Australia. In the big cities they get quite tight, covering maybe a single city block or building.\nThis dataset is roughly 370 000 polygons of varying detail.\nNot the heftiest, but too complex for leaflet, and good enough for illustrative purposes.\nGetting the data\nlibrary(sf)\nlibrary(curl)\nlibrary(dplyr)\nlibrary(magrittr)\n\nmesh_block_url <-\n  \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/MB_2021_AUST_SHP_GDA2020.zip\"\ndownload_folder <- file.path(tempdir(), \"mesh_blocks\")\ndir.create(download_folder)\ndownload_path <- file.path(download_folder, \"mb.zip\")\n\ncurl_download(\n  mesh_block_url,\n  download_path\n)\n\nunzip(\n  download_path,\n  exdir = download_folder\n)\n\nmb_shapes <- read_sf(download_folder)\n\nmb_shapes %>%\n  st_transform(4326) %>% # standard lon lat \n  mutate(\n    random_attribute = sample(\n      1:7,\n      nrow(cur_data()),\n      replace = TRUE\n    )\n  ) %>%\n  select(\n    geometry,\n    random_attribute,\n    MB_CODE21,\n    MB_CAT21,\n    SA2_NAME21\n  ) %>%\n  st_write(\n    \"mb_shapes.geojson\"\n  )\n\nunlink(download_folder, recursive = TRUE)\nI am going to assume the workflow here is mostly rice and beans for anyone interested enough in R and spatial data to read this post. We have:\ncurl a url\nunzip it\nread as sf and mutate the data\nwrite it to geojson\nTwo noteworthy moves:\ntransform to ‘4326’ aka ‘WGS84’ aka “standard lon lat”. This is the input projection that Tippecanoe expects by default.2\nThin out the data columns using select() to choose the minimal set of metadata that is going to be written to JSON and then into tiles. When you’re dealing with big datasets trimming the fat helps speed the pipeline up a lot.\nGenerating the tiles\nSo now the fun part! Here’s the command I ran in the terminal to generate the tiles from the geojson we wrote:\ntippecanoe -zg \\\n           -o abs_mesh_blocks.mbtiles \\\n           --coalesce-densest-as-needed \\\n           --extend-zooms-if-still-dropping \\\n           mb_shapes.geojson\nAnd here is a sample of the output:\nFor layer 0, using name \"mb_shapes\"\n368255 features, 110834523 bytes of geometry, 6144695 bytes of separate metadata, 4830542 bytes of string pool\nChoosing a maxzoom of -z8 for features about 1094 feet (334 meters) apart\nChoosing a maxzoom of -z11 for resolution of about 179 feet (54 meters) within features\ntile 3/7/4 size is 649415 with detail 12, >500000    \nGoing to try keeping the sparsest 69.29% of the features to make it fit\ntile 3/7/4 size is 649787 with detail 12, >500000    \nGoing to try keeping the sparsest 47.99% of the features to make it fit\ntile 3/7/4 size is 649835 with detail 12, >500000    \nGoing to try keeping the sparsest 33.23% of the features to make it fit  ... \nAfter completion we should have an .mbtiles available, let’s check how big it is in the terminal:\n>$du -h abs_mesh_blocks.mbtiles\n83M     abs_mesh_blocks.mbtiles\nNot too shabby compared with:\n>$du -h mb_shapes.geojson\n932M    mb_shapes.geojson\nYou’re probably wondering where I pulled the command from. The tippecanoe README has a helpful ‘cookbook’ section which contains example invocations for common tasks. I copied that command from a section titled Continuous polygon features (states and provinces), visible at all zoom levels. All I changed were the input and output file names.\nTo understand the options you need to understand what tippecanoe is trying achieve. It’s all about trying to factor the geometry into tiles such that the tiles remain under size limits that are designed to maintain map performance. For example:\nThe default tile size limit is 500Kb\nThe default tile feature limit is 200 000 features\nOne thing tippecanoe does toward this end that you cant turn off is simplification. However it’s much smarter than simplifying the whole dataset by some arbitrary fraction. tippecanoe simplifies each zoom level according to the scale of the tiles. For example, at zoom 10, each pixel is roughly 10 meters apart, so tippecanoe uses a 10 metre ‘tolerance’ type parameter to simplify your geometry at that zoom.\nThis kind of simplification is often not sufficient alone to respect the tile size limits, so there are a plethora of options concerning strategies to use to reduce either the number of features, or amount of data associated with those features. Different strategies are better suited to different types of geometry.\nSomething that’s worth noting here is that the 500kb size limit includes any attributes associated with the geometry data. So the select() call in our R code to ‘trim the fat’ has an impact here, allowing us to fit more geometry on each tile. It’s also possible to increase the size limits, though my team has never done so.\nAnother notion that’s useful to understand is ‘overzooming’. Overzooming is browser-side scaling applied to your rendered geometry, such that as you ‘zoom in’ or increase zoom, it is automatically scaled to match the zoom of more detailed tilesets, e.g. the base map.\nPractically speaking this means that if your visualisation tool supports overzooming3, you don’t need to create a tileset for your data for every possible zoom, or every possible zoom of your base map. Ideally you create tiles up to the zoom level where there is no need to reduce features - everything is rendered in sufficient detail.\nAs an example, the difference between a tileset that goes up to zoom 12 and one that goes up to zoom 13 could be up to 50 million vector tiles so it pays to choose the minimum acceptable zoom. In the command we ran we offloaded that choice to tippecanoe with the -zg or “zoom guess” option.\nReturning to the idea of reducing features and data: coalesce-densest-as-needed is an option that tells tippecanoe to combine the densest, i.e. smallest and most tightly packed, features into larger features (polygons for us).\nCorrupting the data can sound scary for us data analyst types, but the result looks quite natural. Tightly packed small polygons are impossible to derive meaning from at low zooms anyway, so they appear as part of other larger polygons, resolving as separate polygons at increased zoom.\nIf you increase the zoom very rapidly you may be able to catch the tileset with its pants down, and see the crudely formed combined polygons. But most of the time the increasingly detailed tiles are downloaded so fast you don’t really notice the corrupted versions of your geometry.\nThe only option used we haven’t discussed is extend-zooms-if-still-dropping, which means: If you are having to drop features to respect size limits at the nominated maximum zoom level for the tileset, extend the tileset to increased zoom levels until that no longer happens. In our case we let tippecanoe choose the maximum zoom level anyway, so this is really just a fallback in case the heuristics applied to do that failed to work well.\nThe last thing we need to cover is this type of output\ntile 3/7/4 size is 649415 with detail 12, >500000    \nGoing to try keeping the sparsest 69.29% of the features to make it fit\nHere tippecanoe is letting us know that even with simplification and the reduction strategies we asked it to apply, a tile turned out bigger than 500Kb. As a last ditch effort its going to keep a selection of features and drop others. Some of this is to be expected, particularly at high zooms like 3. Consider the tiles that contain capital cities at zoom 3, they’re going to have a ton of data, most of which won’t be visible, so the dropping has little observable effect. However, these reattempts slow up the processing, and if you’re getting too many, it might be worth researching some of the other options tippecanoe makes available4.\nSo that’s tippecanoe. Don’t worry if some (or a lot) of that went over your head. It’s taken me a few passes at the README and bunch of playing around to get what feels like a tenuous handle on how this tool works.\nMapping the data\nTo stream vector tiles on demand to maps two thing are needed:\nA map widget that supports making http requests for vector tiles (front end)\nA vector tile server that can processes the http requests and serve the requested tiles (back end)\nAt present, as far as I am aware, there is only one package in the R ecosystem which has a map widget that can render Mapbox vector tiles: {rdeck}. {rdeck} is a deck.gl wrapper created by my colleague Anthony North, and has been my team’s main interactive mapping workhorse for the last couple of years.\nFor this example we’ll serve the tiles with {mvtview}, a package I put together that provides a basic MVT server and viewer powered by {plumber} and {rdeck} respectively. It’s meant as a development tool, to help you iterate on tippecanoe parameters, not as a production solution.\nWe’ll discuss production options for serving vector tiles in the next section.\nTo map the tiles we created earlier:\nlibrary(mvtview)\nlibrary(rdeck)\n\n# Fire up the server\nserve_mvt(\"abs_mesh_blocks.mbtiles\", port = 8765)\n# Serving your tile data from http://0.0.0.0:8765/abs_mesh_blocks.json.\n# Run clean_mvt() to remove all server sessions.\n\n# Map the data\nrdeck() |>\n  add_mvt_layer(\n    data = tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\n    get_fill_color = scale_color_linear(\n      random_attribute\n    ),\n    opacity = 0.6\n  )\nAnd you should get:\n\n\n\nFigure 3: ABS mesh blocks, continental view\n\n\n\n\n\n\nFigure 4: ABS mesh blocks, local view\n\n\n\nYou might be wondering where this file came from: abs_mesh_blocks.json. Recall that a component of a .mbtiles database is a metadata table. {mvtview} provides a way to fetch this metadata as json by querying a json file with the same name as the .mbitles file. The structure of ‘tilejson’ is yet another specification created by Mapbox, and is supported in deck.gl (and therefore {rdeck}) to describe tile endpoints.\nThe tilejson can also be useful for an end user, for example to set the map limits:\n# Use the tile json\nlibrary(jsonlite)\ntile_json <- fromJSON(\"http://0.0.0.0:8765/abs_mesh_blocks.json\")\n\nrdeck(\n  initial_bounds = structure(tile_json$bounds, crs = 4326, class = \"bbox\") # using the tilejson\n) |>\n  add_mvt_layer(\n    data = tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\n    get_fill_color = scale_color_linear(\n      random_attribute\n    ),\n    opacity = 0.6\n  )\nServing vector tiles in production\nMapbox\nMapbox offers a vector tile hosting as a service, and we trialled this and it seemed fine, however I cannot recommend it because the pricing model was unworkable for us. Mapbox bill per square kilometre of tiles ‘processed’, and then per square kilometre hosted per day. The processing fee for the tileset we just created would probably be around $40 AUD - insanely steep considering all the work already done to convert the data to .mbtiles, and the amount of cloud compute that could be purchased for that price.\nThe per square kilometre idea is completely broken for a jurisdiction like Queensland, which is quite large but also very sparsely populated. For example a tileset that has the entire Queensland road network could cost hundreds of dollars a month to host, ridiculous considering many many of those square kilometres of tiles have hardly any data. So if you do want try Mapbox, be sure to read the pricing pages closely!\nAWS lambda\nWe rolled our own Amazon Web Services solution that serves tilesets upload to an S3 bucket via an API gateway that handles requests with AWS lambda functions that run a Node.js environment. So this is a serverless solution where we only pay for the storage and compute we actually use, and the solution will scale to huge numbers of requests that we’ll never see. Our maps only have users in order of dozens, so this works out to be so cheap as to make the Mapbox option look absurd.\nThe AWS solution is built with AWS SAM, meaning it is all infrastructure as code / config, and so should be deployable with minor config changes by other teams who would like to try it out. Check it out here.\nOther options\nThe ‘Awesome Vector Tiles’ repository has a number of open source tile servers linked.\nYou could also roll your own. Tile servers are fairly simple programs. They only have to handle two types of requests. If you take a look around the {plumber} implementation in {mvtview} for inspiration, you’ll see we’re talking less than 300 lines of code. Our AWS solution is a similarly small amount of Typescript.\nFalling back to more traditional methods for serving tiles is possible if you are willing to deal in thousands to millions of tiny vector tile files in a directory structure, rather than an .mbtiles database. A directory structure can be served with an ordinary vanilla webserver program. tippicanoe can output such a directory structure with the --output-to-directory option. Instead of a tile json URL you can give {rdeck} template URL like:\nrdeck() |>\n  add_mvt_layer(\n    data = \"http://0.0.0.0:8765/abs_mesh_blocks/{z}/{x}/{y}.vector.pbf\",\n    get_fill_color = scale_color_linear(\n      random_attribute,\n      limits = c(1,7) # without the tilejson metadata we have no defaults available for the attribute range\n    ),\n    opacity = 0.6\n  )\nConclusion\nThat’s it for this crash course on vector tiles. Hopefully it’s enough to get you exploring the space and reaping some benefits. Converting data to vector tiles feels a bit like an art at times, so don’t worry if you struggle to get a feel for the right options at first. That’s something we had to go through as well.\nI would advise packaging up tippecanoe recipes that work well for the kinds of datasets you have as you discover them. This will save you effort remembering them in the future, and make life easier for your teammates.\nWe barely scratched the surface of what {rdeck} can do, but rest assured its capabilities extend well beyond vector tiles. It is an excellent choice for a general purpose web mapping widget for products created with {rmarkdown}5 and {shiny}.\nPlease feel free to leave a comment! Especially if something is still mysterious, or you have your own completely different vector tiles workflow.\nAll the code used in this blog post is in this repository.\n\nCompression is normally assumed to be used. It is possible to avoid it though.↩︎\nThe mesh blocks are projected in GDA2020, how modern!↩︎\nwhich {rdeck} does↩︎\nIn particular --hilbert seems to make coalescing much better↩︎\nand yes, Quarto↩︎\n",
    "preview": "posts/vector-tiles/fire_and_ice.png",
    "last_modified": "2024-03-11T22:26:25+10:00",
    "input_file": {},
    "preview_width": 2560,
    "preview_height": 1301
  },
  {
    "path": "posts/zsa-moonlander-review/",
    "title": "60 Days with the ZSA Moonlander: Review",
    "description": "I spent 60 days with the ZSA Moonlander Keyboard and ultimately returned it. I wanted to like it, but couldn't make the ergonomics work for me.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2021-06-20",
    "categories": [
      "keyboards",
      "ergonomics",
      "review"
    ],
    "contents": "\n\n\n\nEven if you’re not a massive input nerd, there’s a pretty good chance you’ve heard about the ZSA Moonlander Ergonomic Keyboard. It’s quite an aesthetically pleasing device, and It’s billed as the next generation of the much vaunted ErgodoxEZ lineage. Like its predecessor it carries an eye-watering price tag: USD $365.\nThis post is my review of this apex ergonomic device. I spent 60 days with it before giving up on it due to steadily worsening strain injuries in my hands. Despite the ergonomics being something of a critical failure for me, I did find a lot to like about it, especially on the configuration and customisation side of things. It came frustratingly close to a device I could really get excited about.\nThe rest of this review is going to examine the these aspects of the board:\nQuality\nPortability\nTweakability\nErgonomics\nSupport Experience\nFrom my perspective these represent the main selling points.\nSome context about me: I spend most of my job writing code to do data science\nwork in R, during which I use a keyboard-driven workflow. I work standing 100%\nof the time, and I avoid using a mouse wherever possible. For text editing I\nfavour either Spacemacs or\nVSpaceCode. My daily-driver keyboard for the\nlast 4 or so years has been the Microsoft Sculpt.\nQuality\nFor the most part the Moonlander felt like a premium device. Its surfaces felt\ngood under my hands. The key caps are subtly textured and ever so slightly\ngrippy. They felt weighty and durable. The typing sounds were thick and\nsubstantial, without being excessively loud 1. It gave a very tidy first impression.\nI do have one nitpick: The mechanism in the tenting legs felt flimsy. When rotating them I could feel a\ngear clicking though a discrete set of positions, but the sound and feel of this\ngave the impression that the gear was plastic. Unsettling metallic groans were\nemitted from the springs on the allen screws when tightening them that made me\nfeel caution was required.\nPortability\nThis is an important one for me. I can’t afford two Moonlanders, so I needed it\nto be able accompany me in a bicycle pannier bag on my daily commute. Also\nsetting up and tearing down had to be reasonably low effort.\nA big strike against the Moonlander in this regard is the lack of rigid case.\nFor a device in this price range it was a let down. What was provided wasn’t\nreally a case so much as a thin neoprene slip that keeps the two halves, allen\nkey, and cables bundled together. It didn’t seem like it would offer any\nprotection whatsoever from bumps during transit. By contrast, I have a set of\nheadphones which cost in the same ballpark as the Moonlander that shipped with a\nrigid zip-up case.\n\n\n\nFigure 1: The Moonlander’s case is very lightweight.\n\n\n\nAfter some careful measuring I was able to find exactly one reasonably priced rigid case for DJ headphones that would accept it, for which I paid AUD $40.\nIn its stock neoprene travel bundle, the board, cables, and allen key weighed in\nat around a kilo. I didn’t notice the weight on the bike, but you would in a\nbackpack, particularly since it is dense weight. Heavy enough to notice, not\nheavy enough to be an issue I think.\nThe setup and tear down process did feel a bit cumbersome at first. The fit of\nthe allen key in the tenting screws was quite tight, so I found myself always\nhaving to concentrate to slot it in. Perhaps the most difficult thing to get\ndown was that the left thumb cluster screw is reverse threaded. “Lift-to-loosen”\nI found myself muttering under my breath. It’s the kind of thing you forget in\nhurry, which I frequently am when trying to depart.\nWhen packing up, the cables don’t have a dedicated place to live in the neoprene\nslip which meant pack up always felt a bit haphazard. A couple of times I\naccidentally popped off a key cap trying to squeeze the cables into the slip\nalong side one of the halves.\nOn the plus side, the wrist rest folding mechanism is quite slick with a\nmagnetic clasp that keeps them in travel mode. The wrist rest hinge also gave a\nrobust impression.\nAfter I got the rigid case packing was greatly simplified. I started using the\nslip as padding only, and the case was sufficiently large that I could keep the\ntenting legs in my favoured position while stowing it.\nSo while I agree the Moonlander is portable, I don’t think it’s appreciably more\nportable than a small keyboard (e.g. 60% form factor). Compared with other\nergonomic boards, it probably would be at the portable end, definitely more\nportable than traditional offerings from Microsoft, Kinesis, et. al.\nTweakability\nSoftware\nThis is where the Moonlander shone for me. The software side of things is\nincredibly well executed. There are two main ways you can customise the\nMoonlander’s functionality:\nYou can use online GUI editor called ‘Oryx’ which seems to be backed by git.\nChanges are incrementally tracked and you can fork a configuration at any\npoint. Oryx is focused on the key map, but it does expose a subset of deeper\nstuff that is possible in the firmware.\nYou can edit the Moonlander firmware directly in C code. It uses a framework\ncalled ‘QMK’. ZSA provide instructions on how to get going with this, and QMK\nitself has some of the nicest documentation of any open source project that I\nhave ever seen.\nI didn’t do anything too radical but the possibilities are expansive. To give a taste for what’s possible:\nI swapped out my shift keys for layer toggles. This meant I could directly\ncontrol the ‘shifted’ version of each key. With this concept I:\nMade underscore, _, the shift of space.\nSwapped the locations of angle brackets <> and parentheses ()\nMade Lshift + Rshift { with } on hold\nMade Rshift + Lshift [ with ] on hold\nThe idea I was working on was to move symbols that I use regularly while working\nto more convenient locations - and the results felt pretty damn good!2\nThis experience has cemented for me that deep programmability is an important facet\nof truly ergonomic hardware. All of us have different preferences and physical\nlimitations, too many and varied to design for in hardware!\nA bonus in the Moonlander’s favour is that QMK is an open source firmware that\nis used by a variety of exotic keyboards including kit-builds. So experience\nwith QMK is likely to be portable to other boards, rather than being one\nmanufacturer’s way of doing things.\nI didn’t get deep into QMK but I did work with it to set faster macros for\ncommonly used symbol combinations like %>% and <-, as well as playing with\nthe acceleration parameters of mouse control mode. This was all much easier than\nI expected, thanks to the excellent docs. I found a Docker config for the QMK build environment that made building my changes a breeze.\nOne nitpick I’d make about tweaking is that the iteration loop is quite slow for\ncontinuous parameters. These underpin some important features, for example: How\nlong does a key need to be held down to register as a hold? How quick does a\ndouble tap need to be? How fast should the mouse accelerate in mouse control\nmode?\nThe only way to set these is according to personal taste, but to change a\nvalue means recompiling the firmware and re-flashing the board. It would be cool\nif the board could provide some kind of shell that would accept commands\nto alter these parameters live and feel the effects 3.\nNitpicks aside, the Moonlander was a software tweaker’s dream. I definitely want my next board to be running QMK.\nHardware\nOn the hardware side there are a few avenues for tweaking. The thumb clusters\nseem to have been designed to be swappable, although no other alternatives exist\nat this time. I could easily imagine ZSA is planning a cluster with a pointing\ndevice. There’s also some mysterious sets of screw holes in the base of the\nboard, maybe for extreme tenting mode? Again there’s nothing out at the moment.\nSo the hardware tweaking angle I had open to me was the key switches. They’re\n‘hot-swappable’, which to my surprise meant I could literally swap them out\nwhile the board was hot (powered). The swapping process was repetitive but not\notherwise challenging. There was a switch puller tool included for this\npurpose. It felt kind of like what imagine medieval dentistry was.\nI tried 3 sets of switches all up, including the Kailh Box Red switches I\nordered the board with. The Box Reds were definitely a mistake for me. I found\nthe travel too deep, like typing on a pillow. This lead to a lot of\nfumbling with missed presses or double-presses as I struggled to get a feel for\nthe actuation point. I tried pushing the switches all the way down to the bottom-out point, but that felt fatiguing.\nI had a much better time with some Kailh Speed Copper switches which have lower\ntravel and a higher actuation point. But even these felt like work compared to\nwhat I was used to on the Microsoft Sculpt.\nThe hot-swap design worked really well for me since I didn’t really know what I\nwanted in a switch. I ordered a sampler batch and had a fun time trying them all\nin different positions to get a feel for what I liked. The Moonlander taught me a lot about the influence of switches, and the hot-swap ability is a convenient feature that I would regard highly when choosing my next keyboard, although it may not be essential, having some concept of what I like now.\n\n\n\nFigure 2: A switch swap in progress on the Moonlander\n\n\n\nErgonomics\nZSA’s marketing bills the Moonlanader as a “Next-generation ergonomic keyboard”.\nApart from the customisable key layouts, the main ergonomic features of the\nMoonlander that it makes this claim based on seem to be:\nSplit halves for typing at more comfortable width and angle\nStaggered ‘Columnar’ or ‘Ortholinear’ key arrangement which changes home position reach angles reach distances\nWrist rests (removable)\nTiltable thumb cluster\nAbility to tent the board using a tenting leg and thumb cluster.\nSome of these things worked for me, others did not. I expect this is a highly personal thing.\nSplit Halves\nA thing that worked was typing with a much wider posture encouraged by having\nthe halves set around shoulder width. I have a sense that standing to type with\nmy wrists close together exacerbates a tendency for me to round my shoulders\nforward - since it concentrates weight in front of me. With the Moonlander I\nfelt like I could more easily maintain a healthier posture.\nStaggered Columns\nThe staggered columnar arrangement presented a bit of a stumbling block\ninitially. It took me 7 days to get back to my previous typing speeds 4. I have mixed\nfeelings about the arrangement. I can get on board with the idea it makes hand\nmovements symmetrical, and I did find it easy to visualise the ‘grid’ in my\nhead, but I didn’t find it universally better as the hype would have had me\nbelieve.\nThe columnar arrangement makes different tradeoffs. Some keys are easier to\nreach while some get harder. For example: The middle columns are offset slightly\ndownward from the index finger columns 5. The effect of this is that (in QWERTY) Y and T become\neasier to reach while N, a frequent key in programming, becomes a much further reach than on a staggered board.\n\n\n\nFigure 3: The difference in spacing between J and N on the Moonlander (left) and my laptop (right).\n\n\n\nI did notice some fatigue and pain in my right index finger and I believe the\nN position contributed. This isn’t an insurmountable problem, it could be\nfixed by swapping N for a less common key - but as was noted by the creator of the Workman keyboard layout, swapping keys without consideration for the interaction effects can produce poor results. Probably a better option is swapping to a new layout that has been holistically designed.\nFollowing that line of thinking I experimented with learning DVORAK, where I\nnoticed another issue introduced by the staggered columns: In DVORAK the outside\nkeys above home are ' and L. It seems that the L position upsets a lot of\nprogrammers and terminal users, in part due to proximity to S which is\nimmediately below it. I.e. ls -l uses the pinky for all the characters other\nthan space. So what is recommended is that the ring finger is used\nwhen reaching upward for L and -. This is unconventional but perfectly\nviable on a staggered board, but felt super awkward to me with the Moonlander’s\ncolumns.\nWhat these examples made clear to me is that layouts designed for staggered key\narrangements do not necessarily perform as well with the offset columnar\narrangement. Which seems kind of obvious now, but was not something I\nconsidered when purchasing the Moonlander. N was a significant niggle for me\nin QWERTY as I suspect it may be for other programmers.\nWrist rests\nMoving along to other ergonomic features brings us to the wrist rests. They’re\npremium feeling plastic, but I found them too hard. I couldn’t use them for much\nmore than about 45 minutes at a stretch before my wrists and palms started going\nnumb and tingly. There’s a chance I might have been using them wrong, but I am\nable to use the slightly softer wrist rest on my Microsoft Sculpt all day\nwithout consciously thinking about it. I removed the wrist rests and went with\nhover typing after the first few days. Thankfully this was easy to do.\nTenting and Thumb Cluster\nSo now finally we arrive at the cluster that was tenting and the thumb cluster. This is a challenging part to write since I still have a lot of really raw frustration and anger attached to my experience with this part of the keyboard.\nI’ll start by pointing you to how the thumb cluster is marketed by ZSA:\n\nThe thumb module tilts up and down. Use it to find the perfect typing angle, and the wrist support module will automatically follow.\n\n\nFor small hands, tilt the thumb module up. For bigger hands, tilt it down.\n\n\nA custom fit for your hand size.\n\nNext I should tell you my hand size. A common way to measure this seems to be hand length and breadth. So here are these dimensions for me against the 2012 US Army Anthropometric Survey for males and females6:\n\n\nlibrary(tidyverse)\nansur_2 <- bind_rows(\n  read_csv(\"./ANSUR II MALE Public.csv\"),\n  read_csv(\"./ANSUR II FEMALE Public.csv\")\n)\nmy_hand <- tibble(handlength = 195, handbreadth = 81, label = \"Me\")\n\nggplot(data = ansur_2, aes(x = handbreadth, y = handlength)) +\n  geom_point(colour = \"grey\") +\n  geom_density_2d() +\n  geom_point(data = my_hand, colour = \"red\") +\n  theme_minimal() +\n  geom_text(\n    data = my_hand,\n    aes(label = \"Me\"),\n    colour = \"red\",\n    nudge_x = 0,\n    nudge_y = 4,\n    size = 4\n  ) +\n  labs(\n    title = \"My hand size vs service people in the ANSUR II 2012 sample\",\n    subtitle = \"Contours depict observation density\",\n    x = \"Hand breadth (mm)\",\n    y = \"Hand length (mm)\"\n  )\n\n\n\nFigure 4: Compared with the 4082 servicemen and 1986 servicewomen in the sample I am at the 71st percentile for length but only the 30th for breadth.\n\n\n\nSo I guess there’s some evidence to suggest my hand shape is a little uncommon. I\ncertainly don’t have ‘large’ hands, but to call them ‘small’ could be misleading\ndepending on what dimensions are important. I have ‘skinny’ hands may be better\n7.\nWhat I do know for sure is that with these hands I couldn’t find a way to use\nthe thumb cluster without pain. Inflammation in my thumbs and wrists slowly\naccumulated during my time with the Moonlander, and really started to worry me\naround week 3 - 4 when I noticed pain away from the keyboard when I was doing\nthings like gripping a steering wheel or holding a cup of coffee. The locus of\npain was centred at the base of both thumbs running up to my second knuckle and\nback toward my wrists.\nI tried all sorts of things to get relief. Using the thumb cluster tilted up\nfelt horrible, inducing sharp pain quite quickly, so I started looking at\nmodifications including:\npurchasing aftermarket 3d printed legs to\nallow the Moonlander to tent with the thumb cluster up\nover-size key caps to move the pressable area of the key closer to my natural position\nwooden blocks to allow the board to have a degree of negative tilt.\nHere’s a picture with those mods:\n\n\n\nFigure 5: My modified Moonalander hardware configuration\n\n\n\nThe only thing I found that reliably gave me a break from the pain was switching\nback to my Microsoft Sculpt. This made it abundantly clear that the Moonlander was\nthe root cause of my problems.\nWhat’s got me a bit annoyed about this experience is that ZSA’s marketing made\nme feel like the board has been designed to cater to all hands. They tout a\nnext-generation ergonomic keyboard that can provide “A custom fit for your hand\nsize”. I think I probably bought into this a bit too much, and was convinced that\nthe flexibility of the design would allow it to work for me if I could just find\nthe right combination of parameters.\nIn retrospect it’s a fairly significant oversight by ZSA to talk about the\nMoonlander working in different ways for people with ‘large’ and ‘small’ hands\nwhilst providing no guidance as to the definitions of large and small. Ideally I\nwould have got clarification from them before purchase, but my degree of trust\nin them was high due to the experiences of other people I know who have purchased\ntheir hardware.\nZSA can and absolutely should provide potential customers with sizing aids. I\nhave suggested to them that they should make printable paper templates available\nlike Apple do for their watches, so people can get a feel for how their hands\nwill fall on the board8.\nBeyond my personal woes I see a larger issue with the Moonlander’s ergonomics:\nOut of the box tenting is only supported for people with large hands. This is because\ntilting the thumb cluster down to use as a stababliser moves the thumb’s keys quite\nsome distance from the bottom row of letters. Also the angle of tenting is quite limited. Notably there is no ability to tilt forward which is recommended by most ergonomics guides.\nRather than being next-generation, this strikes me as something of a regression.\nThe ErgodoxEZ could tent in more directions, for all users, while the Moonlander can only tent one way for a\nsmall proportion. People who are interested in tenting should be aware that the\nMoonlander may not work for them. Ideally ZSA would provide guidance in the form\nof hand measurements that tenting will work for, or even better: include some kind\nof kit in the box that makes tenting accessible to all.\nSupport experience\nAround day 60 I got fed up. I’d tried so much and was still experiencing pain. I\nwas considering even wilder solutions, like swapping the left and right halves,\nwhen I had a moment of clarity: My experience was far from what would\nreasonably expected with a top tier ergonomic keyboard. So I wrote an email to\nZSA support. It was a simple email that outlined the trouble I was having, the\nthings I had tried, and asked them if they had any advice they could offer\nbefore I gave up on it completely.\nThe response was very impressive. In short order I received an email from ZSA\nCEO Erez Zuckerman who offered to work with me personally to try to find a setup\nthat would work. One of the things he requested from me were some photos of my\nhands on the board lying flat in its stock configuration. These are some of the\nphotos I responded with:\n\n\n\nFigure 6: My hand in my natural typing position on the Microsoft Sculpt\n\n\n\n\n\n\nFigure 7: My hand trying to reproduce my natural typing position on the Moonlander\n\n\n\nErez’s response again impressed me, because on seeing the photos he quickly\nresponded with a note to say that in his opinion the thumb cluster shape was not\ncompatible with my hand shape. He also offered to accept a return and make a\nrefund even though I was well outside the window in ZSA’s official returns\npolicy.\nThe direct approach was a real relief. I was gearing myself up for the kind of\npainful support experience you get from many companies where they walk you\nthrough a script and make you try all the things you’ve already said you’ve\ntried. This was very different. My time and efforts prior to contacting support\nwere being respected. This is quite a rare thing!\nIt was also nice to have confirmation that I wasn’t missing something. I had\nfailed to find a good solution most likely because one did not exist. I accepted\nthe refund offer and mailed my Moonlander for return a few days later 9.\nThroughout the support process we discussed some potential improvements to the\ncustomer experience, and Erez gave the impression he was giving consideration to\nmy suggestions. He sent me example paper templates for feedback. He also\nrecommended a couple of other boards I should check out made by competitors! As\nfar as I am concerned this was a support experience befitting a top tier\nproduct.\nConclusion\nMy experience with the Moonlander was frustrating and painful. Yet at times it was also\nexhilarating, and in the end it was satisfactory. ZSA lost my money but kept\nmy trust, and I give them full credit for their degree of customer care.\nIn writing this review I have aimed to give an account of my experiences with\nthe Moonlander that might help others avoid wasting time and money as I did. But I\nhave also tried to be fair to the parts of the product that are genuinely well\ndone, and could be appreciated by those with the right size hands.\nThe Moonlander is not a keyboard for everyone, although I think many of its\nideas could have wide appeal. I am now definitely in the market for a\nsplit ortholinear/columnar keyboard running QMK. I’ll be pleased to take recommendations!10.\n\nalthough a lot of this is due to my initial choice of a linear switch.↩︎\nThe last layout I made in Oryx is here if you’re curious: https://configure.zsa.io/moonlander/layouts/L4P6y/latest/0↩︎\nThis nitpick is minor since, I think within QMK it would definitely be possible to assign keys to increment and decrement these parameters live - it just would be nice to have a workflow for this out of the box.↩︎\nPerhaps\nI had some advantage coming from a board that was already split.↩︎\nWhy is this is case when these columns\nuse the same finger?↩︎\nInitially when I made this plot I made it for males only. The data sets are separated by gender. Against males I am in the 60th percentile for length and the 6th for breadth. So I definitely have a more unusual shape compared with men only. Later I realised this was a mistake since ergonomic keyboards shouldn’t be designed for men only.↩︎\nFor someone in the US Army. Compared with warriors of the keyboard kind I expect I’d be a bit more normal.↩︎\nTo ZSA’s credit they actually sent me some of these for feedback, so this may be a work in progress. In the meantime if anyone would like a paper template for the Moonlander or Ergodox, let me know.↩︎\nReturns are made to Taiwan. From Australia this is quite cheap. I think I paid AUD $24. Elsewhere in the world it is much more cost prohibitive.↩︎\nFor now I have an eye on the Corne and the Gergo keyboards. The low profile idea appeals to me as a smaller step from what I am comfortable with on the Sculpt.↩︎\n",
    "preview": "posts/zsa-moonlander-review/moonlander_new.jpg",
    "last_modified": "2024-03-11T22:33:34+10:00",
    "input_file": {}
  },
  {
    "path": "posts/the-stack/",
    "title": "A Public Service Data Science Stack: Collaboration",
    "description": "git, GitHub, {targets}, VSCode, and other tools that our data science team uses to collaborate.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "rstats",
      "collaboration",
      "workflow",
      "reproducibility",
      "runapp"
    ],
    "contents": "\nI recently gave a talk for the R User’s Network: Australian Public Policy (Runapp)\nabout the kinds of data science products we make at my work, and\nthe stack of tools behind them. I was left with a bit to say on the tools\nside of things, so I decided to follow up here, also for the benefit of\npeople outside the group.\nIntroducing The Stack\nAfter probably too much context setting, this was the map I presented of our\ntool stack:\n\n\n\nFigure 1: A map of the tools we use for collaboration, data analysis, open source development, and mapping / geocomputation\n\n\n\nI’ve grouped things into 4 blobs. Imagine these blobs pushing up against\neachother and kind of undulating organically. Things overlap and shift from\nproject to project.\nMy estimation is that probably the most novel / useful things I can share are in the\nCollaboration and Mapping and geocomputation blobs, so I am committing to\nwrite a blog post examining each of these. Maybe the others too if I really\nget fired up.\nFor rest of this post I’ll be talking about our Collaboration stack, which\nhas strong links to things I’ve written about before.\nCollaboration\nThe bedrock of our collaboration is git and GitHub. We have a GitHub\norganisation our team works within - they’re free! Every data science project\nis a git project. We use the GitHub project management tools to organise. The\nmain reason this works for us is because our projects are quite code heavy.\nGit\nWe use R to build pipelines that suck in data from a variety of sources and\ntransform it into analytical reports designed to address specific questions.\nThese reports and all our internal communications that involve data analysis\nare written with {rmarkdown} so they too can easily be source controlled\nwith git.\nThere are many benefits to this kind of ‘from code’ approach. I suspect\nanyone who reads this blog will be on board with many of them, so I am just\ngoing to mention two:\nreproducibility: not just of results, but understanding. If we are having difficulty\nunderstanding the detail of a teammate’s approach - as invariably happens -\nwe can go read their code.\nreuse: as we find ourselves wanting to reuse code from previous projects, we\ncan easily lay hands on it, and refactor it out into functions in team\npackages. This ensures people are computing the same things the same way,\nand everyone’s work benefits from enhancements discovered over time.\nWhen taking the from code approach for a team, I can’t imagine not using\ngit. How can you expect to reproduce something if there is any uncertainty as\nto the final source version that produced the output? The same uncertainty\nundermines reuse. If I can’t be sure how something was done, the safest option\nis to redo it from scratch.\nI’d be interested to hear from teams that don’t use git (or subversion\netc.): How you do you manage multiple analysts contributing to the same projects?\nGit facilitates continuous integration of project work so that you can always\nbe working on a project that contains up to date input from colleagues.\nWithout git in the mix this becomes a delicate dance. I bet collaborating\nin a more manual way tends toward certain pathologies like:\nTeam members working mainly in isolation for most of the project, followed\nby a painful integration phase where they try to make eachother’s work fit\ntogether in a rush at the end. Mistakes are made.\nDivergent coding styles and dependency usage due to above. The effect of which is people have\ntrouble following eachother’s work. Reproducibility is challenged, and\npeople tend want to rewrite the work of others in their style rather than bothering to understand it.\nThese are real things I have seen in teams that are not collaborating well.\nTo inoculate against these pathologies, I think it’s obvious you need to have\npeople seeing and using eachother’s work as it is developed, so there’s ample\nopportunity for questions to be asked and approaches to be harmonised.\nThis can invite some micro-conflicts, but I think these are a healthy part of\nestablishing the team’s norms - and preferable to the larger scale\nconflicts that ensue when things don’t fit together well\nin a final crunch.\nGit does not magically guarantee the this kind of collaboration for data\nscience projects, but it makes\nit easier and thus more likely.\nGitHub\nInitially my team was using hosted git repos on Azure Dev Ops,\nbecause at that time GitHub organisations were not free, and we had access\nto Dev Ops via existing organisational licensing arrangements.\nWe found Dev Ops generally buried information we wanted to see under\ntoo many clicks. Its project management tools are geared towards\nenterprise workflows. Complex workflows are\nmade possible at the cost of making simple workflows complex 1.\nSo instead of the built-in stuff we were using Microsoft Planner for\nproject management and hating it. Things had to be manually kept in sync\nsince there was no integration with our repositories.\nGitHub by contrast doesn’t give you highly configurable project management\ntools, however it feels like they hit a sweet spot for us where there is\njust enough flexibility to accommodate our needs. We switched to GitHub pretty\nmuch as soon as organisations became free.\nWe’ve used GitHub issues\nas general vehicles for project related stuff like:\nDescribing requirements for tasks that need to be done\nDeciding on methodology\nSharing observations arising from data analysis (complete with plots etc)\nReview comments\nBug reports and feature requests for internal packages\nOf course a lot of this happens informally via face to\nface conversations or instant messenger, but we very much prefer issues to email.\nThey keep project information centralised with the project code where it can\nadd context, rather dispersed into inboxes where it can be lost.\n\n\n\nFigure 2: Some of our GitHub issues\n\n\n\n\n\n\nFigure 3: Code and analysis in an issue\n\n\n\nFor larger projects we use the GitHub project boards\nfeature to organise issues. So far we’ve kept this to a simple ‘To do’,\n‘Doing’, ‘Done’ type setup. With sparing use of issue tags for certain\nimportant tasks. There’s some basic automation options available - for\nexample closing issues that are moved to ‘Done’.\nProject boards can operate at the organisational level, potentially pulling\nin issues from multiple repositories, or at the repository level. We use both\ntypes, but not as a rule. We scale the level of project management to match\nthe project.\n\n\n\nFigure 4: A repo project board\n\n\n\n{targets} / {drake}\nI’ve written at some length about our {drake}/{targets} workflow,\nso I won’t rehash that here. I’ll make three extra observations:\nThere are some nice interactions between git and pipeline tools that offer\ncache management like {drake}/{targets}. These tools take away the question\nof what code needs to be re-run when pulling or merging the work of others,\nand guarantees it will be as little as possible.\nI’ve seen this work to reduce ‘pull-hesitancy’ - where collaborators avoid\npulling updated work from the main repository in order to avoid re-running\nlong-running computations they feel they don’t have time for.\nObservation two is the the utility of the pipeline plan a high level map of\nthe project. It allows team members to easily see where their work fits,\nand how it interacts with the work of others. I think this can\nhelp build shared understanding, and thus shared ownership of the project.\nAlso helps with getting up to speed after a context switch.\nThe final observation is how the dependency graph creates confidence when\nit comes to changing or fixing code written by collaborators. I can remember\nsome bad old projects, where the global environment was used a little too\nrecklessly, and the only way to truly know what changes were safe to make was\nto do a close reading of the entire source of the project. Either that or\njust make the change and hope you could find all the broken things.\nThose days feel forever behind us now. Both {drake} and {targets} can\nprovide you with beautiful graphical representations of exactly what targets\nwill be affected by a change, and that lets you know the precise list of\nplaces you need to check for unintended consequences, after which you’re D-O-N-E.\n\n\n\nFigure 5: A {drake} dependency graph with targets outdated by an earlier code change.\n\n\n\n{rmarkdown}\nAs I touched on before, {rmarkdown}’s value as a collaborative tool comes\nfrom the way it keeps analysis output in sync with the source code that\ngenerates it. This helps avoid inaccuracies creeping in due to rework, and\nmeans when ambiguity is discovered it can be resolved by consulting the\nsource code that generated the document.\nTo make the source-output link easy to follow we’ve adopted the practice of\nincluding a ‘reproducibility receipt’ at the foot of our report source.\nIt looks like this in a report:\n\n\n\nFigure 6: A knitted reproducibility receipt, showing date, repo, and R environment.\n\n\n\nWith the receipt in place, you can always have a reference to the location\nand version of the report source.\nThe classic scenario where this has proven very satisfying to have is when a\ndocument boomerangs on the team, usually with changes required on short\nnotice for an important engagement. This has happened multiple times when the\nprimary author of the report has been unavailable 2.\nThe fact that, under duress, you can quickly pinpoint the exact source code\nthat produced the document, which will be organised in clearly laid out\npipeline plan, really takes the heat out of a potentially a stressful\nsituation.\nVisual Studio Code\n\n\n\nFigure 7: VSCode being used for R development\n\n\n\nVSCode has pretty great support for R, it’s the most popular choice of daily driver in my team 3.\nIt has one incredible draw-card feature for collaboration, and that is: Live Share.\nThis is collaborative Google-Docs-style editing. Everyone can see what\neveryone else is typing or highlighting in real time. Participants can chat\nvia text or voice. The R session is run by the host who can allow guests to\nsend commands to the host’s R terminal and view results.\nThere are some rough edges, since the VSCode-R extension wasn’t built with\nthis mode in mind, however at time of writing there is a very promising body of work underway to improve this that is nearing completion.\nThis will make shared views of things like plots and help ‘just work’.\nEven in its’ current state it’s very usable for pair programming. I spent 90\nminutes or so just last week pairing with a colleague via Live Share to solve\nsome data processing scaling issues. It was actually just heaps of fun.\nPossibly even superior to us both standing in front of a computer together,\nsince there’s no awkwardness due to switching who’s driving or not being able\nto type for shit on someone else’s fancy keyboard. The session felt very\nfluid and productive.\nMicrosoft Teams\nTeams cops a lot of flack for being either a shoddy Slack or a poor Zoom. I\nthink to appreciate it at all, it has to be thought of as kind of a mash up of\nboth of these tools that is better than neither in their niche, but excels at\nproviding a blended medium that simplifies switching between text and video.\nI’ve come to appreciate this low effort video option to give head room to an\ninstant message conversation that is being limited by text. The team really\ngot into the groove of using it during 100% work from home periods of 2020.\nWe started doing frequent unscheduled small group video chats for quick\ndiscussions, rather than scheduled fixtures. I do think this helped us stay\nproductive.\nThe screen sharing is solid so it’s good for demonstrating. You can grant\nteammates remote control of your screen too, but it’s a bit laggy and janky\nfor pair programming - it’s completely inferior to VSCode Live Share for this\npurpose.\nWrapping up\nSo that was the main pieces in the collaboration stack. Underpinning the\nchoice of tools is a team focus on reproducibility and visibility in the\ncontext of code-centric workflows. There’s also a distinct bias toward open\nsource and free tools - we’re quite proud to be able to achieve what we do\nwithout charging exorbitant licenses to the public purse.\nIn a this new work-from-home friendly world we’ve also found ourselves\nattracted to tools that allow voice, video, screen-share, pair programming\netc to happen in adhoc ways, simulating interactions that would take place\nin the office.\nAs always I am not trying to put us on a pedestal or suggesting that we have\nmade the best choices in every regard. What we have seems to be working well,\nbut I am very keen to hear tips from other people doing similar kinds of work\nfor similarly constrained organisations. Please share in the comments below or tag\nme in the Runapp Slack channel.\nSee you in the next post where I’ll unpack our mapping stack!\n\nThis is also a problem with Jira + Confluence.↩︎\nAnd will to you too via\nMurphy’s Law↩︎\nThat’s not due to me, I was actually late to the party!↩︎\n",
    "preview": "posts/the-stack/the_stack.png",
    "last_modified": "2024-03-11T22:25:50+10:00",
    "input_file": {},
    "preview_width": 1550,
    "preview_height": 935
  },
  {
    "path": "posts/adding-addins-to-vscode/",
    "title": "Adding RStudio Addins to VSCode",
    "description": "Technical details and select highlights of my project to bring RStudio addins to VSCode.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2020-10-22",
    "categories": [
      "rstats",
      "vscode",
      "rstudio"
    ],
    "contents": "\n\nContents\nBut why?\nGreen lights\nThe how\nThe humane punching of ducks\nVSCode-R communications\n\nChallenges\nLack of specificity\nTypescript workflow\nMerging a large PR\n\nAnd then I broke library(tidyverse)\nConclusion\n\n\n\n\nI recently completed a project to add RStudio addins to the Visual Studio Code\nR extension. In this post I explain the\ntechnical sleight of hand that makes this work, and share some of my select\nhighlights from the project.\nBut why?\nMost people who follow me online know that I use Spacemacs and ESS to write\nR. A few things changed recently that made VSCode radically more appealing to me\nat work, where I am forced to use Windows:\nI learned of a Spacemacs inspired extension for VSCode: VSpaceCode. 1.\nThe whole 2020 and working from home thing has meant my team have been using VSCode’s slick Live share feature to do real-time remote pair programming and debugging. It’s shockingly good.\nMy team has just been dipping our toes into AWS and VSCode facilitates wonderfully seamless remote development over ssh.\nSo I started experimenting to see how close I could make VSpaceCode feel to my\nSpacemacs setup. The first major sticking point was an R package of mine I use many times daily: fnmate\nTo use fnmate your editor needs to interface with your R session to pass back\nthe context of your document around your cursor. R does the magic once it has\nthat context.\nEmacs and RStudio facilitate this in different ways, but ultimately they both\nallow you execute code on data taken from the text editor and shove the result\nof that into an R function called in your R session. 2\nVSCode was without this ability. So I thought that to implement a binding for my\nbeloved fnmate my options were:\nWrite my own VSCode extension - having to redo work already in the R\nextension, relating to interfacing with the R process in terminal.\nGet a binding for fnmate merged into the R extension.\nAnd I guess I was mulling over what unsatisfying options these were when I\nrealised that what the VSCode R extension really needed was something like the\nRStudio API - HOLD ON. Can I port the RStudio API? Can I make fnmate,\ndatapasta and other popular addins work with VSCode with no changes to their\ncode?! I think I know how to do this!\nIt was like as soon as the lightning bolt struck I knew I was not going to be able to stop myself from doing this.\nGreen lights\nI was pretty jazzed about the idea and it occurred to me that it was so good\nthat I was probably not the first person to have it. A GitHub search\nrevealed that my intuition was correct: In May of 2020, VSCode-R extension\nmaintainer Andrew Craig had created an issue discussing the possibility.\nWhen I read through the discussion I saw that it had kind of stalled out because\nthe maintainers were worried about RStudio’s reaction and legal issues. They’d\nresolved to contact RStudio before making any moves.\nThe implementation I had in mind side stepped of their initial legal concerns,\nand I was pretty confident that the RStudio team I knew, the great people that I\nhad met, would not be bothered.\nI offered to contribute and was invited to start working on the idea.3 Once I had a proof of concept - 1 working API call - I decided to email the authors of {rstudioapi} to let them know what we were planning to do.\nThe RStudio response was all class. You can see for yourself in the issue\nthread. JJ Allaire wrote back\nwith encouraging words, a heads up on future API directions, and an offer to\ntalk about the API. Kevin Ushey did likewise, filling out some more detail on\nfuture plans, and invited questions and feedback on the API. When I did have a\nquestion about the API, Kevin answered it quickly.\nThis is RStudio walking the walk of being a company that works in the\ncommunity’s interests. Diversity is good. Friendly competition is good. It’s\nreally impressive stuff, and it’s a part of this story that I really wanted to\nshare, because it meant I could really dive into the work unencumbered by worry\nabout how it would be received.\nThe how\nThe humane punching of ducks\nNow we get to some fun stuff. How is it that I can call:\nrstudioapi::getActiveDocumentContext() from within an R session in VSCode and\nget back the context of my VSCode active text editor window?\nThe trick is one that is often used the testing context, there it’s known as\n‘mocking’. In other context it can be called ‘duck punching’ or ‘monkey\npatching’:\nWithin your VSCode sessions only, I’ve replaced the definition of the\ngetActiveDocumentContext function in the rstudioapi name space, with code\nthat talks to VSCode instead of RStudio. The incredible thing about this to me,\nis that even though it feels a bit dodgy, everything you could want to have to\ndo it is built into the R language.\nThe first piece is a function called assignInNamespace which allows us to do this:\n\n\nassignInNamespace(\"getActiveDocumentContext\",\n                  function(...) {\n                    print(\"Duck punch!\")\n                  },\n                  \"rstudioapi\"\n                  )\n\n\nCalling rstudioapi::getActiveDocumentContext() would now return “Duck punch!”.\nThe second piece allows us to make that assignment happen, immediately after the rstudioapi namespace gets loaded into your R session, whenever, if ever, that may be. It’s a function called setHook:\n\n\nrstudioapi_hook <- function(pkgname, pkgpath) {\n  print(\"Running your package onLoad hook...\")\n  assignInNamespace(\"getActiveDocumentContext\",\n                    function(...) {\n                      print(\"Duck punch!\")\n                    },\n                    \"rstudioapi\"\n                    )\n}\n\nsetHook(packageEvent(\"rstudioapi\", \"onLoad\"), rstudioapi_hook)\n\nrstudioapi::getActiveDocumentContext()\n\n# [1] \"Running your package onLoad hook...\"\n# [1] \"Duck punch!\"\n\n\nSo we’re asking R to only perform the duck punching if the user actually loads\nthe {rstudioapi} namespace, which will happen when any function in it is\ncalled 4. So if the user\nnever calls the API we never do anything spooky.\nIt’s also handy to be able to patch the API functions selectively, since it\nallows us to keep rstudioapi functions that don’t talk to the IDE unchanged,\ne.g. rstudioapi::is.document_range, and we can call those in our adapted\nversions.\nVSCode-R communications\nThe VSCode-R extension already implemented a one-way communication channel\nbetween the user’s R session and VSCode for signalling custom rendering\nbehaviour of plots, shiny apps, web pages etc. So my main challenge was to\nimplement the return protocol so that VSCode could send data back to the R\nsession in response to rstudioapi requests. This had to be written in\nTypescript using the VSCode API.\nThe way in which the communications worked was quite interesting to me - I guess\nI assumed it would be some kind of fancy web socket protocol thing. Actually\nit’s pretty lofi, based on files. It worked like this:\nOn startup VSCode R sessions create two files request.log and\nrequest.lock. In VSCode/Typescript land a file watcher process is attached\nto the lock file that triggers a callback function whenever the lock file is\nupdated.\nIn the R session, a request for VSCode is made by writing some JSON to the\nrequest.log and then writing the current timestamp to the request.lock.\nIn VSCode the callback for the lock file update fires and confirms that the\ntimestamp indicates a new message, and fires the JSON off to a request\nrouter.\nA request router is just a fancy name for a giant switch statement that\nexamines the JSON and decides what functions to call.\nMy implementation for the return path copied these ideas. Once you make an\nrstudioapi call from R, the R sessions blocks, checking a response.lock file\nevery 100ms until an update appears and response data is read from\nresponse.log.\nOne mistake I made initially was that I thought some API calls might be able to\nbe asynchronous, for example inserting text into a document - it seems like it\ncould done without a response back to R. But I soon learned in testing that\nthere are addins out there that blast the rstudioapi with a stream of function\ncalls 5, and I needed to make sure the all\nchanges in VSCode had been applied before I allowed a new request to be written,\notherwise request data might be overwritten without being handled.\nChallenges\nLack of specificity\nBy far the biggest challenge was trying to make VSCode behave exactly as per RStudio. Part of the issue is that the two applications have fundamentally different models.\nIn VSCode for instance, terminals and text editors are different things. You\nhave an active text editor and an active terminal, determining which one of\nthose things has the user’s focus is not a facility provided by the API.\nRStudio, on the other hand, lets you treat everything as a ‘document’, and you\ncan easily the context of the active document that has the user’s focus.\nBut the hardest bit really is that while both APIs have reasonable documentation\nfor the average end user, it suddenly seems lacking when you’re trying to make\none program behave exactly like another - only a small portion of each program’s\nbehavioural extent is documented. There was a lot of experimentation and\nprobing at the limits of each application’s conventions to establish what it could handle.\nI want to give special mention to rstudioapi::insertText() which is quite\npossibly the most loosely specified function I have ever encountered. It was my\nnemeses on this project. This is the function signature:\ninsertText <- function (location, text, id = NULL) {\n\n}\nInsert text at location in document specified by id or active document.\nBut this simplicity belies incredible complexity lurking beneath:\nlocation can be:\na position object - (row, column)\na range object - (row, column) to (row, column)\na length 2 numeric vector - (cast to position)\na length 4 numeric vector - (cast to range)\na mixed list containing any or all of above\nthe text to be inserted if text is not supplied. e.g. Allowing calls like insertText(\"some text\") to work.\n\ntext can be length 1 or length(location)\nNumeric elements allowed in location are all integers in the range [-Inf, Inf]! These need to be resolved to actual possible locations further down the line.\nPhew! There are a couple of hundred lines dedicated to emulating this function\nand its’ aliases alone.\nTypescript workflow\nTypescript is a statically typed Javascript-a-like that compiles to Javascript. I\nquite enjoy the language, but I found working without a REPL to be\nexcruciatingly slow initially.\nI was going:\nwrite code\ncompile and build extension\ntest extension\nrepeat\nAnd the compilation/build time, and repetitive nature of setting up state for a test was a drag. A couple of things I discovered:\nIf you put a breakpoint inside a function you are developing you can get to\nsomething like a REPL pretty quickly by doing your development in debug\nmode. You have the entire application’s state available in an object called\nvscode_1 that you can tinker with in the debug console.\nBy stopping fighting the type checker, and refactoring code that didn’t play\nnicely with it, I found the strong linting and hover help capability of\nVSCode gave me enough context to work without the safety net of the REPL.\nIt’s a different kind of safety net, one that works well with application code.\nMerging a large PR\nThe maintainers of the VSCode-R extension have quite high standards for the\ncode. Kun Ren gives unrelentingly thorough code\nreviews, which are actually a really valuable thing that I haven’t had a lot of\nin my software-writing career. It felt a little like a session with a code\npersonal trainer. I realised I had been lazy in parts, or there were nicer ways\nof doing things that weren’t as complex as I had estimated.\nI was quite sympathetic to this process. Asking people to take on 1000 lines of\npotential technical debt is a big deal. We’re playing catch with a loaded\nfoot-gun - whatever precautions the catcher wants to take have to be respected.\nAnd then I broke library(tidyverse)\nIt turned out that much to my embarrassment the foot-gun went off pretty much\nimmediately!\nWhen repository owner Yuki Ueda creates a\nrelease, it automatically flows through to VSCode users the next time they open\nVSCode. The PR was merged in the afternoon and in the evening I logged into my\nwork machine to catch up on a task from a few hours earlier. My code fell in a\nheap at library(tidyverse) with an error that was painfully familiar, since I\ncoded the message myself:\nerror: This `{rstudioapi}` function is not currenlty implemented for VSCode.\nWAT. I frantically tried to reproduce the bug on my personal laptop running\nUbuntu and couldn’t. Ken Ren couldn’t reproduce it on Mac OS either. It seemed\nwe had missed some Windows specific behaviour which I was the first to encounter\non my Windows work machine.\nIn the midst of this, a colleague of mine who is studying and uses R, messaged\nme to let me know the bug had sent someone in his study group into a panic and\nhe had bailed them out with a fork of the extension. I felt terrible!\nOver the next couple of hours we worked out the issues and implemented a fix. We\nalso made the {rstudioapi} emulation in VSCode opt-in for now with an option\nthat must be set: options(vsc.rstudioapi = TRUE) - in case there are any other\nbig showstoppers we haven’t seen yet.\nIn retrospect the option would have been a good idea from the start, I did think\nabout it at some point, but I guess I got overconfident.\nThe bug itself is was quite interesting to me. When a user does\nlibrary(tidyverse) for the first time in a session there is an on-attach hook\nthat runs that prints a familiar fancy looking message:\n\nlibrary(tidyverse)\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n✔ ggplot2 3.3.2     ✔ purrr   0.3.4\n✔ tibble  3.0.3     ✔ dplyr   1.0.2\n✔ tidyr   1.1.2     ✔ stringr 1.4.0\n✔ readr   1.3.1     ✔ forcats 0.5.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nThis reproduction on my blog is not as colourful as the one we see.\nThis message makes heavy use of two packages for command line output: {crayon}\nand {cli}. Now the first time these packages are called in a session they\nfire off hundreds of lines of code designed to sniff details of your terminal\nand operating system, ultimately to determine:\nWhat is the number of colours in the palette I can use in the terminal?\nWhat symbols are supported in the terminal?\nOn Windows only, {cli} ends up needing to know the version of RStudio you’re\nrunning, and it gets this via rstudioapi::getVersion. Up until this point\nI had shied away from spoofing an API version - that is my implementation\nwould return TRUE for rstudioapi::isAvailable() but error if you tried to\nget the version or assert a specific version. This worked well for the dozen or so addin packages I tested.\nThere was no way around VSCode pretending to have an RStudio version for now to get tidyverse working. We shall see if this creates any issues.\nConclusion\nSo that’s been the highs and lows of this project. Certainly I learned a lot,\nand I thank the maintainers of the VSCode-R extension (Yuki, Kun, and Andy) for\nthe opportunity, and JJ and Kevin from RStudio for their support.\nJust today I used fnmate numerous times on an instance in AWS I had connected\nto using VSCode-over-SSH. I never actually tested this specific case, but VSCode\nis geared such that the remote development case is not something an extension\nauthor has to care that much about. Extremely satisfying!\n\nThanks to the Jack of Some YouTube channel↩︎\nIn Emacs that code is\nLisp. In RStudio it’s R.↩︎\nI’d made one PR for a small feature to the VSCode-R extension in the past, so I think that helped in this regard.↩︎\nOr they do library(rstudioapi) - but this is discouraged by the\n{rstudioapi} documentation and doesn’t make heaps of sense↩︎\nThanks {remedy} and {sinew} ;)↩︎\n",
    "preview": "posts/adding-addins-to-vscode/vscode_addins.png",
    "last_modified": "2024-03-11T22:18:21+10:00",
    "input_file": {},
    "preview_width": 2274,
    "preview_height": 1396
  },
  {
    "path": "posts/an-okay-idea/",
    "title": "Project as an R package: An okay idea",
    "description": "The overarching problem I see with conforming analysis to the package domain, is that it introduces artifacts not of the project domain and that makes the project harder to comprehend",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2020-07-24",
    "categories": [
      "rstats",
      "workflow",
      "data science"
    ],
    "contents": "\n\n\n\nThis post is about an okay idea: structuring your R data analysis projects as packages.\nI say an “okay idea” because I’ve heard good points made by R community members I respect, and I agree with some of them - BUT ultimately I don’t think those points add up to a payoff that makes it a “good idea” for most teams in an industry context.\nThere are quite a number of resources that list the pros of this way of working, and I will briefly summarise those before I examine the cons. I’ll discuss:\ndouble handling of metadata\n\npointless artifacts\n\nmilesmcbain.xyz::added milesmcbain.yxz::noise\nduplication of metadata\nPros\nThe primary argument is usually that by structuring your project as an R package you gain access to a plethora of automations and facilities that are going to make your life better.\nYou get {devtools} and {usethis} for populating for speeding up menial development tasks including:\ndevtools::load_all() to populate your global environment with the most recent versions of your functions\nusethis::use_package() to declare a dependency.\nThe R CMD check routine can run on your project which will find things that do not comply with CRAN policy .e.g:\nundeclared dependencies\nincomplete function documentation\nYou get a well paved on-ramp to continuous integration services on GitHub to run automated test reports on your project code after pushing each commit.\nYou can wield the DESCRIPTION file, which:\ncan make your work more easily searchable\nclearly documents the authors/copyright holders and helps make your work citable (via citation())\ndocuments your dependencies\nIf you install your project package onto a system it automatically installs dependencies defined in the DESCRIPTION file.\nYou’re forced to put all your work in functions which hopefully triggers thought processes about composability and reuse.\nFurther reading\nThe first place I encountered the project as package idea was when I heard about Research Compendia and I believe it was through following rOpensci and Ben Marwick online.\nIt is notable that prominent proposals for the structure of a Research Compendium deviate significantly from that of a standard R package, for example: https://github.com/ropensci/rrrpkg. Ben Marwick has even created distinct devtools-like tooling for Research Compendia.\nDavid Neuzerling just wrote an excellent overview of the case for project as a package in his post Data Science Workflows.\nCons\nAnd now for the main event…\nDouble handling of metadata\nThe machine readable metadata in the DESCRIPTION file makes a lot of sense for something going to CRAN or some other deep storage archive. If you’re not actually releasing the project to public storage though some things become redundant.\nLet’s assume you use git for your project version control and host on GitHub. GitHub is your “deep storage”, and that means:\nThe Authors of the project are already tracked and displayed on GitHub\nTitle and Description are made redundant by your README\nURL and BugReports are useless because that’s the GitHub repo you work out of.\nVersion can be a git tag (GitHub release) 1\nImports, Depends, and Suggests describe your dependencies. Unfortunately they do not describe your dependencies in sufficient detail such that any guarantee can be made about your project package being able to be run after installation. Since your package isn’t on CRAN it is not being reverse dependency checked, and breaking changes in your dependencies will flow to you freely. And this can happen at any moment - Worked fine after install yesterday, broken today.2.\nTo be able to make a guarantee that your project will run with working dependencies at some point in the future you need to capture a known-good list of versions of all your dependencies3. To do this you use a lockfile or a dockerfile or both. Let’s say you use a lockfile created with {renv}. You now have a second place in your project repository that lists R package dependencies and version numbers.\nWhat’s worse is that these two statements of version dependence can easily become out of sync. This happens since lockfiles are typically generated by an automated snapshot process that examines your environment, while your DESCRIPTION is updated manually. Lockfiles obsolete dependencies in DESCRIPTION.\nPointless artifacts\nPointless artifacts are things that must exist for your project to be a valid package by CRAN’s policies but serve no purpose for a project that isn’t going to be publicly released on CRAN.\nThe most absurd pointless artifact for never-to-be released software is a license. You’ll need to declare an open source license acceptable to CRAN or R CMD check will throw an error in your continuous integration reports.\nAlso in this category are the paths you must use for your files. Documents that are to be built by your project go in ./vignettes. Report? It’s a vignette. Slide deck? That’s a vignette. Flexdashboard? That’s vignette too. What isn’t a vignette? Someone may well ask.\nAlternatively you could bury your documents in /inst/docs, since the inst folder is fair game, or you could introduce an .Rbuildignore file to indicate to R CMD check that a simple ./docs should not be cause for alarm. The .Rbuildignore file is the pointless artifact in this case.\nAdded noise\nEvery function call made in an R package that is not part of the base language needs to be tied to an appropriate namespace. The community has settled on the noisiest but most flexible approach to do this as standard, using the :: operator.\nI dug up a real snippet of code from a recent project to demonstrate the difference.\nHere’s the standard for project as a package:\n  plot_data <- daily_work %>%\n    dplyr::filter(IncidentTypeCategory %in% incident_type_categories) %>%\n    # determine ordering for category factor\n    dplyr::group_by(IncidentTypeCategory) %>%\n    dplyr::mutate(total_hours = sum(hours)) %>%\n    dplyr::ungroup() %>%\n    dplyr::mutate(\n      month = tsibble::yearmonth(floor_date(date, \"month\")),\n      category = forcats::fct_reorder(\n        IncidentTypeCategory,\n        total_hours,\n        .desc = TRUE\n      )\n    ) %>%\n    dplyr::group_by(month, category) %>%\n    dplyr::summarise(hours = sum(hours)) %>%\n    dplyr::ungroup() %>%\n    tsibble::as_tsibble(index = month, key = \"category\") %>%\n    tsibble::fill_gaps(hours = 0)\n\n  ggplot2::ggplot(plot_data, aes(x = month, y = hours, colour = category)) +\n    ggplot2::geom_path() +\n    ggplot2::scale_x_date() +\n    ggplot2::expand_limits(y = 0) +\n    ggplot2::facet_wrap(\n      ~category,\n      ncol = 1,\n      scales = \"free_y\"\n    ) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(\n      title = \"QFES monthly vehicle hours\",\n      y = \"vehicle hours\"\n    ) +\n    ggplot2::geom_smooth(se = FALSE, linetype = \"dashed\")\nand here’s what my team actually had:\n  plot_data <- daily_work %>%\n    filter(IncidentTypeCategory %in% incident_type_categories) %>%\n    # determine ordering for category factor\n    group_by(IncidentTypeCategory) %>%\n    mutate(total_hours = sum(hours)) %>%\n    ungroup() %>%\n    mutate(\n      month = yearmonth(floor_date(date, \"month\")),\n      category = fct_reorder(\n        IncidentTypeCategory,\n        total_hours,\n        .desc = TRUE\n      )\n    ) %>%\n    group_by(month, category) %>%\n    summarise(hours = sum(hours)) %>%\n    ungroup() %>%\n    as_tsibble(index = month, key = \"category\") %>%\n    tsibble::fill_gaps(hours = 0)\n\n  ggplot(plot_data, aes(x = month, y = hours, colour = category)) +\n    geom_path() +\n    scale_x_date() +\n    expand_limits(y = 0) +\n    facet_wrap(\n      ~category,\n      ncol = 1,\n      scales = \"free_y\"\n    ) +\n    theme_minimal() +\n    labs(\n      title = \"QFES monthly vehicle hours\",\n      y = \"vehicle hours\"\n    ) +\n    geom_smooth(se = FALSE, linetype = \"dashed\")\nNote how we had tsibble::fill_gaps even in the second example even though we didn’t need to. This is sometimes done as a kindness for the reader when using lesser known functions.\nI find reading from a %>% down into a namespace prefix particularly jarring. I can visually filter the prefixes if I concentrate and regain some of the nice natural language style dplyr flow, but I feel my eyes constantly being tugged to the left as I move down the lines. The procedure that reads a line of text starting from the left most character seems powerfully ingrained in my mind.4\nSurprisingly you must also use :: to prefix functions from core namespaces that are loaded automatically by R. So that means stats::, utils::, and tools:: etc with accompanying declarations in the DESCRIPTION file. A lot of commonly used functions are held in the core packages and I am regularly tripped up in package development trying to use functions without namespace prefixes that I thought were in the base language.\nAnother bit of noise gets introduced if you want to use global constants - that is data objects that are not functions. These are forbidden by default, but have valid use cases. For example I might define a global constant like EPSG_WEB_MERCATOR <- 3857 so later in my code I can do st_transform(sf_object, EPSG_WEB_MERCATOR) instead of having the magic number 3857 appear from the ether.5\nTo do this in a package project I must introduce this odd looking side-effect function into my code:\nglobalVariables(c(\"EPSG_WEB_MERCATOR\"), \"my_package\")\nWEBMERCATOR_EPSG <- 3857\nDissecting the killer features\nFrom what fans of the project as a package workflow have said to me, the two killer features are:\nHaving an easily installable artifact that will install its own dependencies\nEasy on-ramp to automated testing on GitHub continuous integration services.\nWith respect to 1. I’ve already debunked the dependencies aspect - making your project a package is not a robust form of dependency management6. On sharing, I’d argue git clone https://github.com/my_org/my_project is a pretty nice way to share a project, equivalent to install_github(\"my_org/my_project\").\nThinking about 2: Automated testing is great, and the package ecosystem has a nicely paved flow consisting of just a couple of {usethis} calls. But your automated tests are run within RMD check which adds overhead and a whole basket of compliance requirements that are pointless.\nI think people are missing the fact the that there is nothing spooky about the way {testthat} works. There is a function called testthat::test_dir that runs a folder of testthat tests in an environment that is setup by testthat.R . You could change a couple of lines in the GitHub action for CMD check, swapping rcmdcheck::rcmdcheck for testthat::test_dir and you have your automated testing without the tyranny of the CMD check and R package structure.\nWeighing the cons\nOkay so here’s where I go full subjective and get to the heart of why project as a package upsets me.\nAs data scientists we work in a field where there are very few objectively right outcomes. It doesn’t matter how we wrote our code, or how many unit tests it has, or even if our models converged7. What matters is that we have successfully constructed a convincing chain of inferential reasoning, lead by principled data analysis at every step, to a conclusion that is actionable somehow.\nAnd the key word is convincing. It’s not possible to write unit tests that validate the choice of one methodology over another. There are far too many subjective elements. So our code then has the key function of documenting our methodology, and our chain of inferential reasoning so that it can be audited, and validated by peers8.\nSo the whole shebang hangs, not on the functional correctness of our code, but on its clarity. And this is why I will fiercely advocate for writing code to be read. I try to structure my projects to be navigated, and a navigable project must be a reflection of its domain. Every choice I make prioritises surfacing the links my chain of reasoning over cpu cycles, over memory consumption, and everything else 9.\nThe overarching problem I see with conforming analysis to the package domain, is that it introduces artifacts not of the project domain and that makes the project harder to comprehend.\nPeople have said: “Yes but once you know how to make packages it actually makes things clearer”. I find this an inhumane response on a number of levels. It steepens the learning curve before collaborators can become contributors. This is cruel if there are less convoluted options that would suffice.\nMy response to advocates of project as a package is: ==You’re wasting precious time making the wrong packages.==\nInstead of shoehorning your work into the package development domain, with all the loss of fidelity that entails, why aren’t you packaging tools that create the smooth {devtools}/{usethis} style experience for your own domain?10\nNo really. Why. Aren’t. You. Doing. That?\nYou can cherry pick everything you like from package development and leave anything painful, annoying, or frustrating behind11. Your domain, done in your style, under your rules.\nConclusions, caveats, concessions\nMy aim here has been to provide some balance to a discussion that felt a bit one sided. I fully accept that I have but one vantage point and others will evaluate the tradeoffs project as package makes differently from theirs.\nFor my peers slugging away in industry data science roles, my argument essentially boils down to:\nThe two key benefits of the project as package workflow are prone to being overstated. For all the other things listed as ‘pros’, most can be realised under alternative workflows12. The loss of fidelity in terms of the mapping of the project to the domain is risky because it makes your methodology more difficult to verify.\nI am more convinced of the R package approach for reproducible research objects. Although they may not be going to CRAN, they fully expect to be archived. In this case the DESCRIPTION file has more value. It also makes sense to adhere to a really common standard, since if the domain shifts over a very long period of time, it may actually aid comprehension of the work to be in a more general standard form.\nWhere to next\nWhen I step back and look at what’s going on here I am reminded of this very lucid article shared by Hadley Wickham on Twitter recently. The article talks about the phenomenon of ‘bitrot’ and ‘bitcreep’ which are reciprocal software phenomena. In R, our collective tooling has seen significant bitcreep toward package development due to the amount and quality of developers who depend on these tools.\nConversely tools that break the package development paradigm for projects are succeptible to bitrot since they have fewer developers and everything that comes with that is its own self-reinforcing disincentive to users and future developers.\nThe way I see to combat this is with modularity. With small easy to maintain tools that we can compose to build our domain specific workflows. An example would be some kind of {testthis} package that encapsulates how to set up a project for automated testing with {testthat} sans CMD check. Another example might be a package that does something similar for linting.\nWith thanks to\nThe following #rstats community members who responded for my callout for feedback on Twitter, each opening my eyes to a different aspect of projects as packages:\nRaphaël Simon\nEdwin Thoen\nJon Harmon\nNick Tierney and Dean Machiori for being a sounding board for some of my agruments.\nMy team at QFES for being on board with putting clarity above all else, especially Anthony North.\n\nwhich you were probably doing anyway since it facilitates: devtools::install_github(\"milesmcbain/slippymath@0.3.1)↩︎\nYour own automated package checks may alert to the fact your package is already in a broken state if you are triggering them frequently enough↩︎\nAND your dependencies dependencies dependencies…↩︎\nSpeaking of %>%, you’ll have noticed we don’t have to do magrittr::%>% everywhere that is used. It is declared via importFrom(magrittr,\"%>%\") in the package’s NAMESPACE file. NAMESPACE is arguably pointless artifact if the project as a package is never installed.↩︎\nGlobal variables are dangerous. Global constants are useful communication tools.↩︎\nAnd if you don’t have proper dependency management, you are the dependency management!↩︎\nSince misspecified models can converge to an answer that is not well founded.↩︎\nWith possible peers including our future selves in 3-6 months time.↩︎\nAnd I’m lucky enough to work with some people that share this view.↩︎\nThere are many examples of R developers doing this. Some prominent examples to consider are: {workflowr}, {rrpkg}, {orderly}. I have made my own efforts public as discussed in my previous blog post. ↩︎\nTo me, the freedom to iterate toward for-purpose tooling is a core ingredient of a humane work environment. Over time the snags and rough-edges of ill-suited tools wear me down like water torture, until I get pangs of almost-physical pain each time I hit them.↩︎\nIf you like writing functions you owe it to yourself to give {drake} a look.↩︎\n",
    "preview": "posts/an-okay-idea/corrugated-cardboard-3853506_1280.jpg",
    "last_modified": "2024-03-11T22:18:44+10:00",
    "input_file": {}
  },
  {
    "path": "posts/the-drake-post/",
    "title": "Benefits of a function-based diet (The {drake} post)",
    "description": "The {drake} post I've been threatening to make has landed. All 4000 words of it.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2020-04-30",
    "categories": [
      "rstats",
      "productivity",
      "workflow",
      "reproducibility",
      "debugging"
    ],
    "contents": "\n\n\n\n\nHeads up! This post assumes elementary knowledge of what {drake} does. If you’re at all hazy watch this excellent short video (5:48).\n\nA benefit of joining a fledgling Data Science team is that you get to shape the team culture and workflow. So when I joined QFES Futures just over a year ago I was keen to take the opportunity to try to steer us away from some of practices that I have seen make data science collaboration difficult in the past.\nOne of my main early catch cries was “reproducibility!”. But in this context I wanted that to mean more than just being able to blindly run eachother’s code and archive the same output. I want us to be able to reproduce the understanding of how the work is answering relevant questions. Apart from affording a degree of rigour, this has the benefit of reproducing skills across the team. Skills siloing leading to role “lock-in” is a path to collective unhappiness in my experience. 1\nSo now we get to the R package drake. Initially I was attracted to it as an easier alternative to make. Why would I want make? There were 2 main reasons:\nPretty much all our projects involve hitting enterprise databases with SQL queries to pull large volumes of data. This places two things in tension: For reproducibility’s sake you want to be running your data science pipeline end-to-end as often as possible. But for the sake of not being a jerk on a shared platform, you don’t want to be regularly fetching the exact same data you’ve received prior. I wanted caching, but with minimal explicit cache management.\nThe second thing I was craving was some common scaffolding we could use to structure and orchestrate our data science pipelines. Over the years I’ve tried a lot of the different R project templating tools out there. I’ve realised I don’t enjoy those that get too intricate with their folder structures. Inevitably you hit something that doesn’t neatly fit the schema.\nHappily I found drake had a lot to offer in both respects. And then some.\nOne type of question that emerges repeatedly in discussion with the drake-curious is something like “When does it become worth it to use drake?” or “I only do X type of work, will drake help me?”\nMy take is this:\nApproaches to data science pipelines using drake are valuable on projects large and small because they help you move faster by:\navoiding unnecessary computation time\neliminating common classes of bugs associated with interactive development\nproviding easy-to-use debugging access-panels\nfacilitating comprehensible project and code structure\nBUT. It’s not like you get all this for free just by using drake. You get nudges in the right direction. It is still up to you to establish a workflow that will fully convey these benefits. And I think this is where people are getting stuck - taking the first steps.\nSo with the rest of this post I’m trying to offer you an on-ramp by outlining my team’s current drake workflow. It is the result of many iterations over a year of #rstats data science project work and I hope it serves you well.\nFundamental Principles\nA problem my team has is that our workload can get choppy on short notice. Working for emergency services is by definition working through extreme and hence potentially unforeseen events. The media and politicians add their own spice to this mix lest thing get boring.\nComplexity does not survive in this environment. People need to be able to context switch between different analysis projects quickly, and workflow artifacts need to be zero-overhead to create, otherwise there simply ‘won’t be time’ to do the things that save us time.\nThe underlying principles of our workflow are a response to this environment:\nWe automate workflow tasks behind one line function calls. Very much inspired by {devtools} and {usethis}.\nWe use active countermeasures to ward off common bugs that can derail something due on short notice.\nWe try to write ‘self-documenting’ code in our drake plans, with an emphasis object/function names that articulate their role in the pipeline.\nI’ve put as much of this as possible in a package called {dflow} that helps create projects that follow our workflow.\nInitial Project Structure\nWe call dflow::use_dflow() to create starter projects that have this structure:\n.\n├── .env\n├── R\n│   └── plan.R\n├── _drake.R\n└── packages.R\n\nIt’s a simple flat structure that sets up the machinery for drake::r_make(). r_make() is a variant of make() that runs your pipeline described in plan.R within it’s own self contained R session. This is important because you want to avoid running your pipeline in an environment that contains things that have accumulated there during interactive development. Some of these objects might be stale - old function definitions for example - and this can lead to bugs or failure to reproduce results on a teammate’s computer.\nr_make() looks for a file called _drake.R which has the role of sourcing all your functions, making all the library() calls, and otherwise setting up any objects the pipeline needs to run. The final thing it does is return your plan and associated config to be run by the drake machinery.\nThe other files packages.R, and .env are about declaring dependencies and setting useful environment variables. They come populated with some stuff in them that I will return to later.\nMature Project Structure\nBelow is a more mature example that shows how things can evolve. Apart from R, doc is effectively the only standard one of these, since we use a function called dflow::use_rmd() that creates RMarkdown files in it. The other folders may not appear, or may appear under different names, depending on what the project is trying to do, and lead author’s taste.\n.\n├── .env\n├── .gitignore\n├── R\n│   ├── assign_crew.R\n│   ├── assign_stations_to_hexes.R\n│   ├── fetch_geoscience_data.R\n│   ├── get_block_data.R\n│   ├── get_crewnum_19.R\n│   ├── get_hexes.R\n│   ├── get_oms_responses.R\n│   ├── get_population_data.R\n│   ├── get_qld_sa1s.R\n│   ├── get_stations.R\n│   ├── intersect_population_hexes.R\n│   ├── knn_prediction.R\n│   ├── knn_prediction_future.R\n│   ├── plan.R\n│   ├── read_geoscience_data.R\n│   ├── summarise_hex_response_times.R\n│   ├── summarise_hex_structure_fires_future.R\n│   ├── summarise_hexes.R\n│   ├── tag_hexes.R\n│   └── tag_responses.R\n├── README.md\n├── _drake.R\n├── doc\n│   └── analysis.Rmd\n├── input\n│   └── qgso_sa1_pop.rds\n├── packages.R\n└── sql\n    ├── get_h3_remoteness_areas.sql\n    ├── get_hexes.sql\n    └── get_oms_responses.sql\nNotice how all the R files seem to be about doing one specific thing, and they’re all lumped in together under the R folder. This might feel weird if you’re used to a more traditional folder-based approach e.g. ./00_load/, ./01_wrangle/, ./02_model/. The difference is that the bulk of the project structure is provided by the plan.R file. To make that structure coherent it is useful to have functions named after specific steps. We have a convention one of these functions per file. This has advantages I will discuss very soon.\nPlan conventions\nHere is a slightly stripped back2 version of the plan.R file that corresponds to that project structure:\nthe_plan <-\n  drake_plan(\n    start_date = as_datetime(\"2018-01-01\"),\n    end_date = as_datetime(\"2020-01-01\") - seconds(1),\n\n    # spatial block is H3\n    h3_resolution = 6,\n    hexes = get_hexes(h3_resolution),\n\n    stations = get_stations(),\n\n    crews = get_crewnum_19(),\n\n    h3_geoscience_res6_source = fetch_geoscience_data(),\n\n    h3_geoscience_res6 = read_geoscience_data(h3_geoscience_res6_source),\n\n    qld_sa1s = get_qld_sa1s(),\n\n    future_pop = get_population_data(),\n\n    hex_populations = intersect_population_hexes(future_pop,\n                                                 qld_sa1s,\n                                                 hexes),\n\n    oms_responses = get_oms_responses(start_date, end_date),\n\n    tagged_oms_responses = tag_responses(oms_responses, hexes),\n\n    ## Structure Fires\n    hex_summary = summarise_hexes(\n      hexes,\n      h3_geoscience_res6,\n      tagged_oms_responses,\n      hex_populations\n    ),\n\n    hex_predictions = knn_prediction(hex_summary),\n\n    report = target(\n      command = {\n        rmarkdown::render(knitr_in(\"doc/analysis.Rmd\"))\n        file_out(\"doc/analysis.html\")\n      }\n    )\n  )\nIt’s all function calls. There is no explicit code for data wrangling, modelling, or plotting present in the plan. We try to keep the plan as close as possible to a high level description of what is happening. I wouldn’t expect you to fully follow what is going on, there’s a lot of assumed team context, but hopefully you can make out:\ndata acquisition steps with function names like get_*, fetch_*, read_*\ndata wrangling steps: intersect_population_hexes, tag_responses\na modelling step: knn_prediction\na report built from analysis.Rmd\nThe long and specific function names contextualise the processes they contain.\nWould you have guessed this project has had 3 people contributing code to it? That explains the slightly different choices of verbage in function names etc. Despite this it has maintained coherency.\nThe plan also induces structure that can help navigate and understand it. Let’s say we’re interested in what this hexes target is about and we want to see how it is defined. Our convention all but guarantees there will be a file, ./R/get_hexes.R, that corresponds to the get_hexes(h3_resolution) function call.\nWe could navigate to that file in our file browser. Or we could use the ‘jump to definition’ shortcut available in most IDEs/editors3.\nFunction Conventions\nOur convention is to match function argument names in the definition to the names of dependent plan targets where possible4. For example the definition of summarise_hexes looks like this:\n#' summarise_hexes\n#'\n#' @param hexes\n#' @param h3_geoscience_res6\n#' @param tagged_oms_responses\n#' @param hex_populations\n#'\n#' @return hex_summary\n#' @export\n#'\n#' @author Miles McBain\nsummarise_hexes <- function(hexes,\n                            h3_geoscience_res6,\n                            tagged_oms_responses,\n                            hex_populations) {\n...\n# convert all to tibbles and select columns\n# cascading left_join() calls\n...\n\n}\nwith all the argument names mirroring targets in the plan.\nThis convention speeds up the process of setting up suitable input values for interactive development of the function code, since we can read them directly out of drake’s cache! If you’re using RStudio, you have an addin provided by drake called “loadd target at cursor” which will load the target under the cursor into your global environment 5. This makes starting from a fresh session, jumping to some code, and getting set up to edit quite fast. Here’s a gif6:\n\n\n\nFigure 1: Jump to source, loadd targets at cursor, run code.\n\n\n\nFunctions are usually short, not much more than a screenfull of code. This maximises the effectiveness of the cache as a development accelerator. For example: if you combine a database query followed by some hectic wrangling into one target, you’ll be re-running the query every time you fine tune your wrangling. You’re better off splitting them into two functions so that the query only gets run once and cached.\nAutomated function templating\nWriting lots of these functions in individual files can get tedious, so I’ve automated this process. The {fnmate} package allows you to call functions into existence in a conventionally named file, just by writing a call to that hypothetical function:\n\n\n\nFigure 2: {fnmate} in use in RStudio\n\n\n\nBeware. This gets addictive and there’s no way back. I use it many times daily.\nRMarkdown Conventions\nWe’ll often have multiple outputs knitted from RMarkdown files. A report and a companion slide deck has been a favourite combo, or perhaps reports for peers vs reports for stakeholders.\nIn this scenario it made sense to promote objects like maps and plots that are shared across documents to plan targets. So it’s common to see stuff like this in our plans:\nthe_plan <-\n  drake_plan(\n    ...\n\n    coverage_map = create_coverage_map(isochrones, escad_incidents),\n\n    daily_work_series_plot = create_daily_work_series_plot(escad_incidents,   incident_categories)\n\n    ...\n\n    report = target(\n      command = {\n        rmarkdown::render(knitr_in(\"doc/analysis.Rmd\"))\n        file_out(\"doc/analysis.html\")\n      }\n    )\n\n  )\nWhere these targets are {mapdeck} and {ggplot} objects respectively. The RMarkdown then becomes extremely simple:\n...\n\n## 14 minute travel isochrone\n\n\\```{r, echo = FALSE, out.width = 100%}\nreadd(coverage_map)\n\\```\n\n...\nWhere drake::readd pulls coverage_map from the cache into the rendering environment. It also signifies to drake that report has a dependency on coverage_map.7\nAnd as a general consequence our RMarkdown documents have far less R code in them than I had become used to. Rendering completes in seconds and rarely errors.\nWhen we do include R code beyond reading targets from the cache, our principle is to keep it directly related to the presentation rather than computation of results.\nIt’s been interesting to see how niggles I have with Rmd and notebooks in general have evaporated under these conventions. Do you ever feel when looking at the source of an Rmd that the narrative of the text and the narrative of the code are out of sync somehow? This approach may release that tension for you.\nDebugging the pipeline\nWhen something goes badly wrong and errors, drake will give you the stack trace and the name the target on which the error occurred. From that you know which function to jump into so debugging from there becomes:\nload and inspect targets the function takes as inputs from the cache e.g. using loadd(target_name) or the ‘loadd target at cursor’ addin.\nPlay debug detective, e.g. :\nCheck the input conforms to your assumptions\njump to THAT target’s function if no\n\nInspect intermediate results\nExplain the problem to a duck\n\nAfter a fair bit of this I’ve come to appreciate how having these tightly focused functional units of pipeline logic that interface with eachother in regular ways helps you narrow in on a problem quickly.\ndrake provides some tools that can help with debugging but they’re not compatible with r_make(), since you need your REPL R session and the plan evaluation session to be one and the same. You can still use them though, just make sure you do so in a fresh R session. The workflow would be:\nsource(\"_drake.R\")\ndrake_debug(target = target_that_failed, plan = the_plan)\nAnd you’ll be immediately dropped into an interactive debugging session of the target function as if you had called debugOnce on it8. I rarely do this though, most of the time the first approach gets me there.\nActive Countermeasures\nHere’s where we return to the packages.R and .env:\npackages.R ships with:\nlibrary(conflicted)\nlibrary(dotenv)\nlibrary(drake)\nand here’s .env:\n_R_CHECK_LENGTH_1_LOGIC2_=verbose\n_R_CHECK_LENGTH_1_CONDITION_=true\nNow let’s talk about what’s driving this.\nAvoiding conflicts with {conflicted}\n{conflicted} makes sure you avoid the particularly insidious class of R bugs that arise from function masking in packages. That is: when two or more packages provide functions of the same name, the package you call library() on last has its function end up in your namespace. Change the order of your library calls, change the way your code functions. Yikes.9\nMy team tend to pull many different kinds of data together in our analyses. 20 or so library calls is in packages.R is not that uncommon. So there’s been plenty of scope for this issue to arise and hours have been lost. One of the things I despise about this bug is that the error messages you get are dumbfounding. Things you’re sure should work just suddenly don’t. It’s the kind of thing that makes you get all superstitious and fall back to turning it on and off again.\nconflicted will mean a few extra lines in the packages.R file of the form:\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nAnd yes doing that for filter every time you use it for the first time does get old. But it’s a tiny price to pay for never losing hours to this issue again.\nMaking R stricter with env vars\nThe only thing the {dotenv} package does is load the environment variables specified in the .env file. We prefer this to way to using an .Renviron file in the project root, since that would mask the user’s local .Revniron in their profile. .env is also a cross language standard for doing this.\nThe payload of the .env file are two R options that we’re turning on - this is what they do:\n_R_CHECK_LENGTH_1_LOGIC2=verbose makes the usage of ‘scalar’ logic operators like && and || with vectors longer than 1 element an error and includes tracing information. Why? Compare these expressions and their output:\nlibrary(tibble)\nlibrary(dplyr)\nmtcars <- as_tibble(mtcars)\nnrow(mtcars)\n\n# [1] 32\n\nmtcars %>%\nfilter(gear >= 4 || am == 1) %>%\nnrow()\n\n# [1] 32\n\nmtcars %>%\nfilter(gear >= 4 | am == 1) %>%\nnrow()\n\n# [1] 17\n\nSys.setenv(`_R_CHECK_LENGTH_1_LOGIC2_`=\"verbose\")\nmtcars %>%\nfilter(gear > 4 && am == 1)\n\n# ----------- FAILURE REPORT --------------\n# --- failure: length > 1 in coercion to logical ---\n# --- srcref ---\n# :\n# --- package (from environment) ---\n# package:dplyr\n# --- call from context ---\n# NULL\n# --- call from argument ---\n# gear > 4 && am == 1\n# --- R stacktrace ---\n# ...\nSee this issue for some exposition on what is happening.\nSo the variations read very similarly. They probably sound the same in your head, and yet the results! You may be confident you’ll never make this mistake. But how confident are you in spotting it in someone else’s code as a deadline looms?\n_R_CHECK_LENGTH_1_CONDITION_=true changes a warning that would normally be thrown by trying to use an if statement with length > 1 vector into an error. E.g.\nif(c(TRUE, FALSE)) print(\"Doin' stuff\")\n\n# [1] \"Doin' stuff\"\n# Warning message:\n# In if (c(TRUE, FALSE)) print(\"Doin' stuff\") :\n#   the condition has length > 1 and only the first element will be used\n\nSys.setenv(`_R_CHECK_LENGTH_1_CONDITION_`=\"true\")\nif(c(TRUE, FALSE)) print(\"Doin' stuff\")\n\n## Error in if (c(TRUE, FALSE)) print(\"Doin' stuff\") :\n##  the condition has length > 1\nThe rationale here is that upgrading the warning to an error forces us to fail early and hard. We don’t want to spot something like this after we’ve let the pipeline run for an hour, cache or no cache.\nAs a general principle, failing early and hard plays really nicely with drake since the processing up to the point of failure is not wasted. This idea can be extended from warnings to coding in assertions about input targets where appropriate.\nComplementary tools for reproducibility\nJust briefly I want to discuss managing versions of R package dependencies. We have two approaches that are employed depending on the durability of the project. Spoiler: neither of them involve Docker - we haven’t had the need yet.\nFor long lived projects\nFor things we want to be able to return to in months and years from now we use a kind of lighter-weight wrapper for RStudio’s {renv} called {capsule}. The idea behind capsule is that since we execute our pipeline in an isolated environment, it is that package environment and that environment only that needs to be clearly specified. A user’s interactive development environment does not need to be described and locked down in the same way.\nIn practice this side-steps a lot of awkwardness under renv that comes with trialling packages in development you quickly decide not to use, and accessing packages that are convenient for development but not required for the pipeline to run.10\nUsing capusle we can:\ncapture a description of the package environment set up by packages.R with capsule::create()\nrun a pipeline inside an environment that matches the description with capsule::run_callr(function() drake::r_make())\nFor short lived projects\nRecently we’ve evolved a different approach for things that are perhaps more time sensitive and at the same time more disposable. Think maps to respond to queries from ranking officials etc.\nWe use a packaged called {using} to map out any critical version dependencies in the packages.R. Instead of yelling to someone “This’ll only work with the dev version of mapdeck” as you email them a link to a gist or repo, you can write:\nusing(mapdeck, min_version = \"0.3.2003\", repo = \"https://github.com/SymbolixAU/mapdeck\")\nAnd as a nicety the user is prompted to install if the dependency is not satisfied.\nA driver for this is long compilation times for development versions of various packages that make building a complete environment using capsule a bit annoying for time sensitive things that will only be thrown away. 11\n“But will it scale?”\nWe’ve had as many as four data scientists contributing simultaneously to projects in this style. My sense from hanging out with you all online is that this is actually above average. One unexpected benefit of the one-function-per-file convention is that space is created for collaborators to make contributions without tripping over eachother.\nI can count the git merge conflicts we’ve had on one hand. They all happened in the plan file as that’s now the one place you can really step on someone. Conflicts in this file are not too bad to resolve since the code is so high level - just a list of targets and function calls.\nScaling to a large code base is not something I can comment on. We’ve probably done a few thousand lines of code at peak. I’ve heard that lots of code can challenge drake as it parses all it all to detect dependencies. This feels like a surmountable problem. Don’t let it hold you back.\nOn the other end of things I don’t think there actually is a project size too small. Many of the discussed benefits still apply and overhead is low thanks to automations. I’ve used this workflow a few times to create pipelines that output a single plot image. Even on a tiny project there’s something to be said for sticking to conventions to avoid bogging down in new decisions.\nConclusion\nIf you’ve had some programming education you may recognise a key philosophy of this approach as encapsulation. That’s the breaking of machinery into pieces, hiding it away behind descriptive labels, and clear interfaces. Encapsulation done well allows us to reason about a complex data science pipeline on different abstraction levels as necessary without getting overloaded.\nWhen you combine that encapsulation with a drake’s cache and dependency sniffing smarts, you get the ability to hone in on bugs extremely quickly. You also minimise the cost of having had them, since you reuse as much prior processing as possible in a principled way that guarantees reproducibility.\nUsing functions for the structure they can provide is different to the dominant way they are pitched in our community, as tools to avoid “repeating yourself”. In this context, I would change a popular maxim slightly to say:\n\nIf there’s a piece of code you’ve had to comprehend three or more times, consider writing a function (and choosing a good descriptive name).\n\nDoing this well still involves much art. Don’t worry if you’re not ready for the rest of the fucking owl yet. Building comfort with writing functions by forcing yourself to do it more often would be a great stepping stone to a workflow like this.\nThe end\nThanks for reading. My objective here hasn’t been to sell you on my tools and my workflow. Although they are there to be used and I welcome your feedback.\nI’m offering what I have for you to reflect on, adapt, and remix in the context of your own working styles, team dynamics, and coding preferences. I think workflows that work can’t be established any other way.\nWith thanks to\nNick Tierney for a thoughtful draft review of this post.\nMy colleagues, who have been on this journey with me for the last year, and among them especially Anthony North.\nrOpensci, and all drake’s contributors but especially Will Landau who is without a doubt the most responsive R package maintainer on the internet.\n\nI think I was first exposed to these kinds of ideas when I read this Scaling Knowledge at AirBnB blog post way back in 2016. Still worth a read now: https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091↩︎\nfull plan here↩︎\nF2 or Control + Mouse1 in RStudio↩︎\nThis may be not possible when using the more fancy dynamic target mechanisms, but we rarely use these.↩︎\nI’ve bound ‘loadd target at cursor’ to a keyboard shortcut here. It’s a simple matter to do this for VSCode or Emacs, ping me if you want an example.↩︎\nOkay some sleight of hand here. You’d need to have loaded the library calls first. I have a shortcut setup to do source(\"./packages.R\")↩︎\nYes. Incredibly drake parses the code inside your RMarkdown and registers them as dependencies of the plan.↩︎\nSince your target functions are just regular R functions you can already use debugOnce on them.↩︎\nAs people seem to enjoy pointing out, this functionality was added to base R since 3.6. Conflicted is still the superior choice in my opinion. It’s documentation is more coherent, and it let’s you take a lazy, function by function approach, addressing conflicts only if they actually arise in your code.↩︎\n{mapview}, {datapasta}, {styler}, {lintr}, {languageserver}, {fnmate} are all examples.↩︎\nOne could argue that in a time sensitive situation we need the protection of the capsule approach more than ever. I have a lot of sympathy for that argument.↩︎\n",
    "preview": "posts/the-drake-post/spinach_small.jpg",
    "last_modified": "2024-03-11T22:25:08+10:00",
    "input_file": {}
  },
  {
    "path": "posts/recover/",
    "title": "Stop() - breathe - recover()",
    "description": "Over the last couple of months a debugging technique involving an R function called `recover` has transformed my approach dealing with errors in R code. This post is an introduction to a ~20min video I've made where I demonstrate my `recover` approach on two examples.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2019-07-28",
    "categories": [
      "rstats",
      "debugging"
    ],
    "contents": "\nOver the last couple of months a debugging technique involving an R function called recover has transformed my approach dealing with errors in R code. This post is an introduction to a ~20min video I’ve made where I demonstrate my recover approach on two examples.\nI haven’t been this excited to share something with #rstats for a while - so trust me - this is a technique worth knowing. Especially if you subscribe to the tidyverse’s nested everything-in-a-tibble modelling workflow. When I’m manipulating list columns of tibbles and models I’ve found I’m that much farther from the context I need to understand errors. Getting that to context has previously been a labor intensive and demotivating task. Not so with recover! Just watch go the video! Come back here after for some post script comments.\n\n\nP.S.\nAs you saw, recover makes debugging complex errors easier by placing all the information you need to debug it at your finger tips. It. Is. Awesome! But it’s also just one tool in a full debugging arsenal. I certainly don’t have it enabled always. You’ll see pretty quickly how frustrating that is. Despite R’s reputation, most of the time an error message alone is enough for me to figure out what went wrong (with some help from Dr. Google).\nAt the end of the video I said you’ll need other techniques to tackle issues in your program logic that don’t result in an error. For those I highly recommend Kara Woo’s RStudio::conf 2019 talk: Box plots: a case study in debugging and perseverance where, among other things, she demonstrates nimble usage of debugonce. That’s another favourite debugging function of mine. 1\nThe code used in my examples can be found at: https://github.com/milesmcbain/recover_demo\nRelated Techniques\nRStudio has something like recover: the ‘re-run with debug’ link that appears alongside error messages in the console. And although this lets you browse the entire stack of code and variables, as far as I can see it doesn’t let you choose a frame and execute code within its context (like I did in my second example).\nBreakpoints are something that I have moved away from over time, despite having used them heavily with other programming languages prior to using R. I am not fully sure why this is, although I think they’re just generally less useful when you have a strong REPL like R.\nA Thankyou Owed\nI think a strong debugger is often a draw card that drives people to IDEs, so having these tools available within any R session opens up the playing field for different R editing setups. It really is amazing to have a tool like recover available within any R REPL. Hats off and thankyou to the R-Core team and contributors!\n\nUsing a lot of debugonce has made me realise how important program structure and style is to having debuggable code. I don’t think you can go past Jenny Bryan’s UseR 2018 talk: Code Smells and Feels for an approachable introduction to ideas along those lines.↩︎\n",
    "preview": "posts/recover/recover_vid.png",
    "last_modified": "2024-03-11T22:23:51+10:00",
    "input_file": {},
    "preview_width": 893,
    "preview_height": 503
  },
  {
    "path": "posts/hacking-r-library-paths/",
    "title": "Hacking R's library paths",
    "description": "If you've been tripped up by R's library paths in the past, or you just enjoy a good hack, you might appreciate this little trick",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2019-06-20",
    "categories": [
      "rstats",
      "hacks"
    ],
    "contents": "\n\n\n\nIf you’ve been tripped up by R’s library paths in the past, or you just enjoy a good hack, you might appreciate this little trick I discovered today. Here’s an alternate to .libPaths() that will let you set your R library paths to precisely whatever you choose, whenever you like1:\nset_lib_paths <- function(lib_vec) {\n\n  lib_vec <- normalizePath(lib_vec, mustWork = TRUE)\n\n  shim_fun <- .libPaths\n  shim_env <- new.env(parent = environment(shim_fun))\n  shim_env$.Library <- character()\n  shim_env$.Library.site <- character()\n\n  environment(shim_fun) <- shim_env\n  shim_fun(lib_vec)\n\n}\n\n> .libPaths()\n[1] \"/home/miles/R/x86_64-pc-linux-gnu-library/3.6\"\n[2] \"/usr/local/lib/R/site-library\"                \n[3] \"/usr/lib/R/site-library\"                      \n[4] \"/usr/lib/R/library\"    \n\n> set_lib_paths(\"~/code/library\")\n> .libPaths()\n[1] \"/home/miles/code/library\"\nThis code is inspired by some I found in the source of Gabe Becker’s switchr package. It looks a little cryptic, but when you figure out what’s going on it’s really quite exciting.\nSo let’s break it down:\nAccording to the help of .libPaths() (which is a pretty ‘classic’ base R help file):\n\nIf called with argument ‘new’, the library search path is set to the\nexisting directories in ‘unique(c(new, .Library.site, .Library))’\nand this is returned. If given no argument, a character vector\nwith the currently active library trees is returned.\n\nWhich is to say .libPaths(new) will prepend your library paths, .Library and .Library.site, with whatever new contains. .Library and .Library.site are just regular R objects though. You can look up their values by typing them into your console. E.g. On my PC here:\n> .Library\n[1] \"/usr/lib/R/library\"\nSo you might think (like I did), oh cool I can just overwrite these! Alas no. They don’t appear in a regular environment e.g.:\n> environment(.Library)\nNULL\nAnd while you can assign to them, it doesn’t affect the behaviour of .libPaths(). So we can deduce that .libPaths() must have access to some other environment that contains the versions of these objects it references.\nOn this assumption, what this function does is take a copy of the .libPaths() function and inject a shim environment between the copy and its original parent environment. Into the shim environment it injects new objects named .Library and .Library.site which are set to empty character vectors.\nThese injected objects mask the true versions that reside somewhere up the environment chain that we can’t easily reach. So when the code in our copy of .libPaths() goes reaching for these objects it will find our empty copies first and its behaviour will now be to set our library paths to c(new, character(0), character(0)) …PHWARH!2\nBeyond this function, which could be incredibly useful in workshop/classroom settings, I’m quite taken with the environment shim technique. It’ll be interesting to see what fun havok it can wreak with other people’s package code. Can it facilitate a kind of monkey-patching of unexported functions? We’ll see. Let me know if you use it for something gnarly!\nHeader Image Credit:\nNew York Zoological Society, Public domain\n\nUnbelievably libPaths() doesn’t let you remove your system or site library from the search path.↩︎\nImportantly since we inserted our shim environment like a link into the existing chain, the .libPaths() code can still access any other objects it needs in higher scopes. Also the original .libPaths() works as normal↩︎\n",
    "preview": "posts/hacking-r-library-paths/Monkey-typing.jpg",
    "last_modified": "2024-03-11T22:20:06+10:00",
    "input_file": {}
  },
  {
    "path": "posts/packrat-lite/",
    "title": "A workflow for lightweight R dependency management",
    "description": "Recently I proudly shared some code that was only weeks old and had maybe 40 dependencies... what could possibly go wrong right?",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2019-04-09",
    "categories": [
      "rstats",
      "reproducibility"
    ],
    "contents": "\n\n\n\nA couple of weeks ago I proudly shared a drake pipeline with a colleague along with some basic instructions: “Install R -> clone this repo -> install these packages -> run this file”. The code was only weeks old, had maybe 40 recursive dependencies and used only CRAN versions… what could possibly go wrong right?\nWell let me tell you it was a catastrophic failure. My colleague is new to R, and I cringed and fretted for his user experience as the code basically blew up on the launchpad, spewing alien error messages the likes of which I had never seen.\nAs I dug into his sessionInfo() it turned out he had installed Microsoft R Open, which runs an R version and CRAN package versions that are quite behind the current releases. This was somewhat of a relief, but my eyes were now fully opened to what can happen when dependencies are not stated in explicit enough detail. 1\nI decided to research dependency management in R for something simple and light that could provide a foolproof end user experience i.e. ‘a button’ or a one-liner could be run once the repo had been cloned to set up an environment with the exact same package versions I had used in development. I tried: checkpoint, jetpack, and packrat and eventually settled on packrat. The rest of this post describes my packrat workflow and some of the issues with the other two packages.\nA lightweight packrat workflow\npackrat was the last thing I tried because I had already tried it a long time ago and found it a struggle - it seemed to want me to place all the source code of all my dependencies under version control(!!!). From reading around online this seems like a common experience - the packrat API is a bit confusing, much of it seems geared toward enterprise-level dependency management, rather than project-level.\nIt turns out the kind of workflow I was looking for is possible, but you have to deviate from the default options and workflow advertised in the documentation. Here I describe the workflow I used to create this example repo.\nInitialise packrat\nIn this step we initialise a project-local library which will automatically have packrat installed into it. Crucially we instruct packrat to update our .gitignore so that the library itself excluded from our repository.\npackrat::init(infer.dependencies = FALSE,\n              options = list(\n                vcs.ignore.lib = TRUE,\n                vcs.ignore.src = TRUE\n              ))\n\n# Initializing packrat project in directory:\n# - \"~/repos/packrat_demo\"\n#\n# Adding these packages to packrat:\n#            _      \n#    packrat   0.5.0\n#\n# Fetching sources for packrat (0.5.0) ... OK (CRAN current)\n# Snapshot written to '/home/miles/repos/packrat_demo/packrat/packrat.lock'\n# Installing packrat (0.5.0) ... \n#   OK (built source)\n# Initialization complete!\n# Unloading packages in user library:\n# - rmarkdown, lubridate, dplyr, readr, drake, ggplot2\n# Packrat mode on. Using library in directory:\n# - \"~/repos/packrat_demo/packrat/lib\"\n\ninstall.packages\nNext we install our packages into local library as we normally would with install.packages(). Why did we not let packrat do this for us? Installing many libraries at once can take a longish time and can occasionally throw errors we would like to know about or prompts we would like to answer. It’s also a nice check to confirm that our dependencies are really what we think they are - i.e. we don’t have stray lib:: or library(lib) calls hiding in the code somewhere.\ninstall.packages(c('drake', 'readr', 'dplyr', 'lubridate', 'rmarkdown', 'ggplot2'))\n\n# ... installation guff ...\n\nAt this point we should test our code to make sure it runs without missing dependency errors.\nSnapshot library\nIn this step we create a metadata file, packrat.lock, that records our R version and all the versions of the packages in our project-local library. This is the thing we will check into our git repo so that others can regenerate our environment on their end when they clone it.\npackrat::snapshot(ignore.stale = TRUE, \n                  snapshot.sources = FALSE,\n                  infer.dependencies = FALSE)\n\n## Adding these packages to packrat:\n##                  _         \n##     BH             1.69.0-1\n##     R6             2.4.0   \n##     RColorBrewer   1.1-2   \n##     Rcpp           1.0.1   \n##     assertthat     0.2.1   \n##     backports      1.1.3   \n##     base64enc      0.1-3   \n##     base64url      1.4     \n##     cli            1.1.0   \n##     clipr          0.5.0   \n##     colorspace     1.4-1   \n##     crayon         1.3.4   \n##     digest         0.6.18  \n##     dplyr          0.8.0.1 \n##     drake          7.1.0   \n##     evaluate       0.13    \n##     fansi          0.4.0   \n##     ggplot2        3.1.1   \n##     glue           1.3.1   \n##     gtable         0.3.0   \n##     highr          0.8     \n##     hms            0.4.2   \n##     htmltools      0.3.6   \n##     igraph         1.2.4   \n##     jsonlite       1.6     \n##     knitr          1.22    \n##     labeling       0.3     \n##     lazyeval       0.2.2   \n##     lubridate      1.7.4   \n##     magrittr       1.5     \n##     markdown       0.9     \n##     mime           0.6     \n##     munsell        0.5.0   \n##     pillar         1.3.1   \n##     pkgconfig      2.0.2   \n##     plogr          0.2.0   \n##     plyr           1.8.4   \n##     purrr          0.3.2   \n##     readr          1.3.1   \n##     reshape2       1.4.3   \n##     rlang          0.3.4   \n##     rmarkdown      1.12    \n##     scales         1.0.0   \n##     storr          1.2.1   \n##     stringi        1.4.3   \n##     stringr        1.4.0   \n##     tibble         2.1.1   \n##     tidyselect     0.2.5   \n##     tinytex        0.11    \n##     utf8           1.1.4   \n##     viridisLite    0.3.0   \n##     withr          2.1.2   \n##     xfun           0.6     \n##     yaml           2.2.0   \n\n## Snapshot written to '/home/miles/repos/packrat_demo/packrat/packrat.lock'\nIn case you are wondering this is a complete list of recursive dependencies - our dependencies’ dependencies.\nThe reason for the non-default argument choices is as follows:\n* ignore.stale stops packrat from second guessing the us. We are saying just make a metadata file from the local library as it is right now, irrespective of what it looked like in the past.\n* snapshot.sources stops packrat downloading the source files for all our dependencies. We’re trusting CRAN to keep these available.\n* infer.depenencies is as in the previous step. We are in control and being explicit about what our dependencies are using install.packages.\nCommit and push snapshot\npackrat has created/updated a number of files for us:\n* .gitignore has had entries added to ignore the packages in the project-local library.\n* ./packrat/packrat.lock records versions of R and all the packages in the local library.\n* ./packrat/packrat.opt records our options choices.\n* ./.Rprofile will initialise the packrat local library for a user starting an R session in this project. If they’ve just cloned it will be empty.\n* ./packrat/init.R is an automation script pointed to by .Rprofile\nInterestingly, of these only packrat.lock is strictly required. A user can restore the project library given only this file. The other files make it a slightly smoother process, in that restoration is a single function call instead of two - but this comes at the cost of some potentially surprising automation.\nI’m trying to keep my workflow lightweight so I only commit ./packrat/packrat.lock. I add the others to my .gitignore - and make sure that is also committed.\nRestoring from a fresh clone\nAssuming you committed packrat.lock, the local library can be restored with two function calls after opening an R session in a freshly cloned project directory:\n> packrat::restore()\n# Installing BH (1.69.0-1) ... \n#   OK (built source)\n# Installing R6 (2.4.0) ... \n#   OK (built source)\n# Installing RColorBrewer (1.1-2) ... \n#   OK (built source)\n# Installing Rcpp (1.0.1) ... \n#   OK (built source)\n# Installing assertthat (0.2.1) ... \n#   OK (built source)\n# Installing backports (1.1.3) ... \n#   OK (built source)\n# Installing base64enc (0.1-3) ... \n#   OK (built source)\n# Installing clipr (0.5.0) ... \n#   OK (built source)\n# ...\n\n> packrat::init(infer.dependencies = FALSE,\n              options = list(\n                vcs.ignore.lib = TRUE,\n                vcs.ignore.src = TRUE\n              ))\n# Initializing packrat project in directory:\n# - \"~/repos/packrat_demo\"\n# Initialization complete!\n# Packrat mode on. Using library in directory:\n# - \"~/repos/packrat_demo/packrat/lib\"            \nIf it’s not clear, packrat::restore() downloads the libraries to ./packrat and packrat::init() points our project there to look for it’s dependencies.\nI prefer to wrap these two up into a single file, setup.R, that the end user is instructed to source in the repo README. Obviously they need to have packrat installed to run that code, but I think the normal R error mechanisms that will trigger if they don’t should be clear enough to see them through.\nUpdating the library\nAs development progresses you will likely make changes to your project-local library. Remember to create a new snapshot and push the packrat.lock periodically.\nWhat about wrapping packrat?\nInitially I thought about wrapping up this workflow with a nicer API in a new package, maybe called slackrat or something, but then I remembered hearing that RStudio were working on a sequel to packrat: renv and it seems like they are proposing a very similar workflow. I am guessing this renv is a little ways off. We’ll see if I break in the meantime.\nPackrat alternatives\njetpack looked really interesting and I had high hopes, but it is a bit new and rough around the edges for me.\n* I did like that it uses a DESCRIPTION file to store package dependencies.\n* I didn’t like that it uses a different set of commands to install packages instead of install.packages.\n* It gives very little feedback while working so I couldn’t tell if it had hung or not when using its install functions.\n* It has extremely scant documentation.\ncheckpoint was the first thing I tried and it sort of worked, but the problem is that it needs to be included in the project source to do its magic - and that magic needs to happen every time the project runs. So what I found was it clashes with drake, since in that workflow what you want to be doing is calling make() frequently to take advantage of the cache, but checkpoint would spend a significant amount of time searching through all the project files for dependencies each time make() is called. It just slowed the flow too much.\nConclusion\nDespite some awkwardness I found packrat still to be the best option for lightweight dependency management for R projects. Hopefully with pak and renv in development we’ll see something similar but much less janky available soon. Good luck making your projects reproducible!\nHeader Image Credit:\nImage by Nadine Doerlé from Pixabay\n\nTo add insult to injury my colleague is a JS developer with some fairly mad skillz and I proceeded to get schooled on npm and how he had just effortlessly deployed a custom-built front end of our geospatial models to production that had over 1500 package dependencies↩︎\n",
    "preview": "posts/packrat-lite/nutria-1200457_1920.jpg",
    "last_modified": "2024-03-11T22:20:34+10:00",
    "input_file": {}
  },
  {
    "path": "posts/for-loops/",
    "title": "Horseman number for",
    "description": "Two #rstats community personalities whom I very much admire have taken my name in vain in relation to for loops. What's up with that?",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2019-02-14",
    "categories": [
      "rstats",
      "loops"
    ],
    "contents": "\n\n\n\nA curious thing happened last week. Two #rstats community personalities whom I very much admire appeared to independently take my name in vain:\n\nAbout to write a for loop in #rstats. Sorry @MilesMcBain, but it feels right.\n@rdpeng\n\n\nI’ve said FOR LOOP three times already today, and yet @MilesMcBain hasn’t jumped through my monitor to set my desk on fire….\n@coolbutuseless\n\nThe background to this is a longish story, but the summary is that some years ago now I swore off for loops completely in an effort to get better at functional programming, and learn purrr and the apply family.\nInitially it was just going to be for one project, but once I got a streak going it got kind of addictive - and I did get pretty good at doing things the purrr way. Every so often I felt so pleased with myself that I would try to swagger about on #rstats Twitter posting my number:\n\nI was just reminded I’ve gone 782 days with no loops written in #rstats. Why do we need to teach these to beginners again?\n@milesmcbain\n\nCringe.\nSo I mean sure, being a spectre people feel the need to banish is partly hilarious. But it also makes me feel some guilt. I detest code shaming. And I wonder if there are people less experienced than Mike and Roger out there who have been made to feel genuinely bad about writing for loops due to my tweets on the subject over the years.\nThe aim of this post is to attempt to clarify my position and fill out some nuance that is not possible in the Twitter format.\nCognitively loaded cupcakes\nI’m hardly an original thinker on this subject. The presentation that really flipped me from ambivalence to wanting to move away from loops was this 2016 talk by Hadley Wickham1. There’s a section about for loops as cupcake recipes that highlights how much baggage for loops force you to carry, felt especially once you have internalised the concept of iteration. He goes on to show how this heaviness or noisiness can obscure what the important points of difference are between your looping constructs.\nI pretty much buy all of this. Maybe I have a small brain, but the cognitive load issue resonated with me. There’s a literate programming angle there too. Not to mention the joy of freedom from the insanity of mindlessly typing out the same result <- vector(length(thing)) i in length()/seq_along() boiler plate code Every. Single. Time.\nIt’s a trap\nIn R for loops are particularly challenging to get right. For starters there’s no one way to determine the number of iterations that will always work. Consider this mess:\n1:number is common and seems to work fine but then you get burned by 1:0 == c(1, 0)\nlength(thing) is okay but then you get burned by length(NA) == 1 and if thing unexpectedly has dimensions is will just silently use the wrong one - like number of columns in the dataframe.\nseq_along() is pretty robust but has the same issue with NA.\nnrow() is good but for multi-dimensional only.\nBut wait there’s more! To use loops efficiently in R you need to have awareness of how your data structures will behave in memory when acted on by your loop:\nYou need to declare some storage for the loop results before the loop body otherwise you will fall into the growing a data structure R anti-pattern.\nYou do not want to use for loops to modify dataframe rows. It will be horrifically slow - this is the row-wise modification of data in a loop R anti-pattern\nSo here we see something that many people consider should be one of the first things taught in a programming curriculum is a dead set minefield in R. R developers before us have been kind enough to write us some functions to concrete over much of this danger. It makes sense to me to use them.\nThe tragedy of decompilation\nA strong theme of some of the programming reading I have been doing recently is the importance of strategies to manage the complexity of your code for finishing projects and maintaining them2. The major tool you have to do this is the ability to create abstractions. I’ve found trying to get better at this to be extremely challenging, but the creative process feels like it could be one of the most enjoyable aspects of programming for me.\nWhen you create an abstraction you build up an object or function in code that maps closely to some part of your mental model of your problem. If you do it well enough, you can express your coded solution simply, and ‘in your own words’, and have it solved. That is MAGIC. Think about the most popular R packages. By and large their popularity is due to the quality of the abstractions that they provide.\nA loop is not an abstraction. It is a low level construct that maps relatively closely to how computers work, not necessarily to how humans think. To use a loop you have to take your high-level formulation of a solution and ‘build down’ or ‘decompile’ to the level of primitive computing operations like for and if else. Sometimes this is unavoidable, but the danger in leaving your solution expressed in these terms is that the high level mental constructs you formed your solution with are not present in the code.\nThis means that when someone else, or you in the future, reads the solution, decompilation artifacts can be mistaken as important to the algorithm. An example decompilation artifact is the sequentiality constraint inherent in a loop - is this important or could the iteration happen in parallel? It’s not clear without tracing through all the looped steps carefully.\nExample: The great linear algebra farce\nI am anticipating skepticism on this last point because loops have a way of getting into our heads. So I’ve picked a related example outside computer science to try to drive this home. I am guessing there are a great many of you out there like me, who received explicit direct instruction in linear algebra topics in high school mathematics. You may have learned a textbook definition of matrix-vector multiplication like this:\n\n\n\nFigure 1: Textbook definition of matrix-vector multiplication\n\n\n\nNot fully comprehending what this is saying, you were then probably trained to perform a mechanical iterative process involving multiplying then adding lines of corresponding numbers in sequence. And this mechanistic low-level view may still be how you understand matrix-vector multiplication today. The mechanical process would have certainly come in handy in solving those exam questions. But there’s also a very good chance, that you - like me, were deprived of any appreciation of the higher level thinking that gave rise to the matrix abstraction and was decompiled into the mechanical process involving primitive arithmetic.3.\nThe higher level thinking is that the matrix represents a transformation function that stretches and rotates the vector in space. I didn’t encounter this view of things until I took introductory computer graphics at university. And even then I didn’t fully appreciate the power of matrices as geometric abstractions until I watched 3Blue1Brown’s amazing youtube series: The Essence of Linear Algebra. Once you understand this, you can calculate the multiplication by hand in slightly different and more memorable way that follows from geometric interpretation.\nMy understanding is that we owe much to MIT Professor Gilbert Strang for revolutionising the way modern linear algebra is taught - with a renewed emphasis on the geometry that matrices and vectors abstract. His lecture videos are similarly thrilling to 3Blue1Brown’s although a bit more of a time investment.\nI really do feel it is a farce that a generation of students got handed a boring mechanical process instead of an exciting and powerful abstraction. And this is exactly the kind of tragedy you too can create if you decompile your insights into mechanistic loops and leave them there for others to ponder.\nWhat is it good for?\nStill with me? You’re Awesome. Let’s talk.\nWhat I hope you’re getting is that I see many pitfalls you can avoid by not using for loops. However they’re not without their uses.\nFirstly, I’ll say that quite often Data Science feels like a street fight. It’s a battle between you and your data that with luck and grit you may just win by the skin of your teeth before deadline. And in these survival situations it’s important that you win. So if you feel you have to write a disposable loop in the moment to make that happen, and see another week - more power to you. I’ve been there. You won’t get any judgement from me.\nSecondly, to quote Jenny Bryan:\n\nOf course someone has to write loops. It doesn’t have to be you.\n\nExcept that maybe this time it is you. You’ve got some new low level manipulation of data you want to create a function for. Fair enough. It might be polite to place it where other people aren’t forced to look at it. rcpp functions are great for this.\nConclusion\nI think another popular way to have this conversation is: “You shouldn’t for loops because vectorisation” - I find this problematic because if you try to drill into what the process of vectorisation actually is, it involves calling and or writing loops - just in a lower level language than R.4\nSo I think a better conversation is to be had about the benefits of avoiding for loops at the ‘top level’ of your expressions that solve your problem. That’s where your abstractions belong. And the supporting argument here is much more about communication than anything else - although technical pitfalls are there for new R programmers.\nPS - If anyone is curious my streak without for loops lasted 1068 days. And it ended VOLUNTARILY. ;)\nHeader Image Credit:\nFour Horsemen of Apocalypse, by Viktor Vasnetsov. Painted in 1887.\nPublic Domain\n\nYou can’t see the slides for this section in the video, but I eventually saw them at rstudio::conf 2019. You can find them here in the ‘Building Tidy Tools’ archive, ‘05-fp.pdf’↩︎\nPractical Common Lisp, The Structure and Interpretation of Computer Programs↩︎\nA mechanical process that feels awfully like two nested for loops.↩︎\nAgain I have to credit Hadley Wickham for this idea. It’s based on something he tweeted at me↩︎\n",
    "preview": "posts/for-loops/1024px-Apocalypse_vasnetsov.jpg",
    "last_modified": "2024-03-11T22:19:40+10:00",
    "input_file": {}
  },
  {
    "path": "posts/rstats-anti-pattern-row-wise/",
    "title": "R anti-pattern: Row-wise modification of data in a loop",
    "description": "The Row-wise modification of data in a loop is a trap for programmers coming to R from other languages.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-12-07",
    "categories": [
      "rstats",
      "loops"
    ],
    "contents": "\n\n\n\nThe purpose of this post is to document something we’ll call the Row-wise modification of data in a loop R anti-pattern. This anti-pattern is a trap for programmers coming to R from many other programming languages. Hopefully this post will help you understand it and avoid it.\nThe pattern\nSuppose you have some operation you want to perform on every row in a column, saving the results in the original column. You’ve heard about some fancy packages to do this, but why bother when a simple loop will do the trick? Someone said that loops are no longer slow in R, right?\nLet’s assume you have this data and you want update each value of df$a. As an example we’ll just do a simple conversion:\nmtcars\nlibrary(tibble)\nlibrary(microbenchmark)\n\ndf <- data.frame(\n  a = runif(1000),\n  b = runif(1000)\n)\n\n## crack out the loop!\nfor (i in nrow(df[, 1])) {\n    df[i,1] <- df[i,1] * 180/pi\n}\nThe key feature of the anti-pattern is that a data.frame column, in this case df[, 1] is being modified many times in sequence - once for each row.\nA new challenger appears\nA colleague who has been using R for some time, sees your code over your shoulder and lets out some tut tuts. “You really should use lapply or map for that.” he says.\nReally? Shouldn’t they take about the same time? lapply and map must be just a loop under the hood! You decide to benchmark the suggestion:\nmicrobenchmark(\n  for (i in nrow(df[, 1])) {\n    df[i,1] <- df[i,1] * 180/pi\n  },\n  df$a <- lapply(df$a, function(val) val * 180/pi))\n  \n>\nUnit: milliseconds\nexpr                min       lq     mean    median\nfor (...            11.794536 12.17966 15.64883 12.682777\ndf$a <- lapply(...  1.883315  1.97313  2.59433  2.067858\n       uq      max neval\n 16.30356 50.23415   100\n  2.35720 15.98660   100\nWow. lapply was 8x faster!\nWhat is going on\nAt this point you might be getting mad at the person that told you that loops aren’t slow - but it’s not their fault. Loops are not the problem here - it is the way R manages memory. In this case R is doing some extra work it doesn’t need to do, and we can demonstrate it using tools from the pryr package.\nLet’s consider the first five iterations of the loop, and print out the address of the column being modified, using inspect(df[, 1]), each time:\nlibrary(pryr)\n\nfor (i in 1:5) {\n  df[i,1] <- df[i,1] * 180/pi\n  print(inspect(df[, 1]))\n}\n\n>\n<REALSXP 0x117c6f40>\n<REALSXP 0x117c98c0>\n<REALSXP 0x1ae1dec0>\n<REALSXP 0x1ae1fe40>\n<REALSXP 0x2446cc60>\nWhat we observe is that at the end of each iteration, the column has a new address in memory. ==Now the only way that this can happen is if the entire column is copied to the new location, every single time it is modified.==\nWith loops that modify multiple columns, there is a copying cost associated with each column, each iteration. For large datasets this can easily cause processing time blowouts.\nBy contrast, the lapply construct only modifies the column once, all in one go. So it does not pay any penalty associated with copies induced by repeated assignments to the column.\nWhy it is going on\nI said before this copying was extra work R didn’t need to do - let’s explore that quickly. R is clever in one way in that it can share identical columns between dataframes to save memory. This is covered in some detail in Advanced R.\nSo whenever a column is modified it is copied, in case that column was a column referred to by another dataframe. But this seems kind of dumb in the case where there is only one dataframe that uses that column - which is what we had in our example. It should be possible, by counting references to a column, to know when copying is required and when in-place modification is okay. Apparently this is coming in future versions of R.\nNot quite the end of the story\nAmongst the commotion and exclaiming at this revelation your supervisor approaches to see what is happening. She looks at the code briefly, blinks, and says “That’s nice but you really could do that in one line, like this”. She scribbles out this R code onto an important document on your desk:\ndf$a <- df$a * 180/pi\nAnd instantly you know she’s right. You don’t even need to run the benchmark to know it’s going to crush the alternatives. Here you learned an important lesson: in R often the fastest iteration construct is no iteration at all. Taking advantage of the internally vectorised nature of R’s base functions and operators is the way to go where possible. I had a great lesson in this watching Jenny Bryan’s webinar on row-oriented workflows which I thoroughly recommend.\nSummary\nAnywhere you encounter a dataframe (or tibble) being modified row-wise in a loop is an opportunity to improve run time significantly. I once found something like this written by a teammate and we got the run time down from around 6 hours to around 20 minutes by changing to another approach. Ofcourse thesedays most people are using dplyr or data.table to do this kind of work, so you’re only likely to encounter it in legacy code or code written by people who are new to R.\nOne final comment: this anti-pattern is fine where the run time is not a concern - i,e, the dataset is small. Never let benchmark bullies intimidate you into changing code that works, due to notional time constraints.\nWith thanks to the #rstats twitter crowd who chimed in on this twitter thread in particular @groundwalkergmb and @ThomasMailund who provided code examples on which I based the one in this post.\nHeader Image credit:\nBy Ryan9270144, 11/5/2015, CC-BY-SA 4.0\nhttps://commons.wikimedia.org/wiki/File:El_Diablo_Roller_Coaster.jpg\n\n\n\n",
    "preview": "posts/rstats-anti-pattern-row-wise/El_Diablo_Roller_Coaster.jpg",
    "last_modified": "2024-03-11T22:24:18+10:00",
    "input_file": {}
  },
  {
    "path": "posts/the-roots-of-quotation/",
    "title": "The Roots of Quotation",
    "description": "In this post I use Lisp as a prop to have the quoting conversation from the start. It's a deep dive that will hopefully arrive at a deep understanding.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-07-26",
    "categories": [
      "rstats",
      "lisp"
    ],
    "contents": "\n\n\n\nRecently I’ve been trying to learn more about Non-standard evaluation and specifically quoting in R. It’s been a pretty frustrating time. The best way I can describe it is a constant maddening feeling like I am coming in halfway through a conversation.\nOne thing I kept noticing again and again is R documentation that references quoting often references Lisp, as if deferring explicit definition of some concepts to your knowledge of Lisp. For example, ?bquote1 :\n\nAn analogue of the LISP backquote macro. bquote quotes its argument except that terms wrapped in .() are evaluated in the specified where environment.\n\nYou do know Lisp right? Yeah nah, me either. But I have been taking some baby steps in that direction with Peter Seibel’s excellent (free online) book Practical Common Lisp.\nIn this post I’m going to use Lisp as a prop to have the quoting conversation from the start. From there I’ll build up to ‘quasiquotation’ and ‘unquoting’, and finally distill the jargon into some short take-aways. It’s not an overview. It’s a deep dive that will hopefully arrive at a deep understanding. Let me know if you found the read worth it.\nIn the beginning there were seven functions\nIt seems that when people talk about the birth of Lisp, they frequently cite a paper by John McCarthy called Recursive Functions of Symbolic Expressions and Their Computation by Machine Part 1. I found the human-readable tribute version, The Roots of Lisp by Paul Graham, to be a bit more digestible.\nAs told by Graham, In McCathy’s original paper in 1960, he laid out 7 primitive operators and then proved those 7 could be combined into a recursive evaluation system for arbitrary expressions of a list-based form. That system became Lisp, but more importantly for this discussion, one of those 7 infinity stones, that have shaped modern computation ever after, was called quote.\nDiscovering this was extremely exciting to me. In quote we have something analogous to the Higgs Boson of Non-Standard and Tidy Evaluation! Genealogy, etymology, and heritage - all subsequent quoting functions and evaluation systems flow from quote. More exciting still is that quote can be described and understood in quite simple terms, owing to the simplicity and elegance of McCarthy’s evaluation system.\nA Study of Lisp’s Quote\nAs a primer, here’s a great quote about quote:\n\nQuote may seem a bit of a foreign concept, because few other languages have anything like it. It’s closely tied to one of the most distinctive features of Lisp: code and data are made out of the same data structures, and the quote operator is the way we distinguish between them.\nPaul Graham\n\nNow I’m going to present a scenario that will give you some insight into the importance of quote in making evaluation work, but before I can do that I’ll need to give you a brief primer of Lisp functions and evaluation rules.\nLisp in a paragraph\nIn Lisp everything is either an atom or a list. atoms are literals like 3.14, \"Hello\", or names of variables or functions like: myvar, list, quote. Lists are denoted with () and contain atoms and or other lists. When writing Lisp programs you write lists, each of which is interpreted as a function call unless otherwise instructed2. In a function call the first list element is a name identifying the function and the following elements become its arguments. To make that concrete, this R expression:\nlist(1, 2, list(3, 4) )\n\n#> [[1]]\n# [1] 1\n#\n# [[2]]\n# [1] 2\n#\n# [[3]]\n# [[3]][[1]]\n# [1] 3\n#\n# [[3]][[2]]\n# [1] 4\nhas this lisp equivalent:\n(list 1 2 (list 3 4))\n\n; => (1 2 (3 4))\nThe set up\nLet me now introduce you to a Lisp function called set. It has a role as an assignment function. Given that, you might try something like:\n(set myvar \"myvalue\")\nBut that’ll get you an error: The variable MYVAR is unbound. So the program tried to evaluate myvar and found it was not defined - we knew that - we were just trying to define it!\nSo okay let’s go out on a limb:\n(set \"myvar\" \"myvalue\")\nError again: The value \"myvar\" is not of type SYMBOL when binding SYMBOL\nYikes. So it won’t take a string as a variable name because that’s not a ‘symbol’, but if we give it plain text it will try to evaluate it and find it unbound. You may have guessed where this is going. We need to tell the program: Don’t evaluate myvar, it’s not code, it’s data for the set operation. We need to use quote:\n(set (quote myvar) \"myvalue\")\nThere are several shorthands available to make this less cumbersome. In Lisp (quote myvar) can also be written as 'myvar, and furthermore, these are all equivalent:\n(set (quote myvar) \"myvalue\")\n(set 'myvar \"myvalue\")\n(setq myvar \"myvalue\")\nIt may not surprise you to learn setq (a quoting set) is a standard way to perform assignment in Lisp. In fact it’s verging on impossible to do something resembling traditional assignment in Lisp without a quoting function being involved somehow.3\nGetting us on a technicality\nAn interesting discussion point arises from the previous example: quote seems to return the text we supply as a ‘symbol’ since that is the input expected by set. How can we reconcile that with descriptions of quote that say it ‘simply’ returns its argument unevaluated? Consider these R and Lisp expressions:\n## R\nx <- quote(y)\nclass(x)\n# [1] \"name\"\n\n;; Lisp\n(setq x (quote y))\n(type-of x)\n; => SYMBOL\nCome on R, the thing I passed you didn’t have a class. You definitely evaluated something, and Lisp you’re no better… can unevaluated text have a data type?\nIt turns out that these type of definitions of quote are correct only if we apply a narrow and technical definition of evaluate. The relevant background to comprehend this definition is well articulated by Peter Seibel in Chapter 4 of Practical Common Lisp. Here’s a crude summary:\nCompilers and interpreters are most often opaque to programmers. It is usual though that they are made up of subprograms. There is usually a distinct program with the responsibility of parsing the text we type and creating a structure of machine-readable of tokens or symbols. Then there is usually a distinct program responsible for mapping parts of this structure to low level code to be executed on the processor. This last program - the one in control of execution - is considered to be the ‘evaluator’ of the code.\nSo what this means is that code can be transformed and structured by sub-programs4 without technically being evaluated until it is processed by the ‘evaluator’. In the Lisp family of programming languages it is possible to instruct the evaluator to leave off, and instead of an evaluation result, the machine-readable symbolic structure that would have been ‘evaluated’ is returned. This is what quote does.\nCode as Data\nIf you have investigated much of the tidy eval documentation you may know the machine-readable symbolic structure returned by quote as an Abstract Syntax Tree. In Lisp it’s just regular nested lists. In the previous section, the R class \"name\" and Lisp type SYMBOL are atomic elements of these structures respectively. Quoting more complex code will yield different structures. Since Lisp uses plain lists for data and code, it is intuitive that syntactical structures can be manipulated using regular list mechanisms - somewhat surprisingly this holds for R as well:\n## R\ny <- quote(abs(x - 1))\nclass(y)\n# [1] \"call\"\n\ny[1]\n# abs()\n\ny[2]\n#(x - 1)()\n\ny[[1]] <- quote(sin)\ny\n# sin(x - 1)\n\n;; Lisp\n(setq y (quote (abs (- x 1))))\n(type-of y)\n; => CONS\n\n(car y)\n; => ABS\n\n(cdr y)\n; => ((- X 1))\n\n(setf (car y) (quote sin))\n\n(print y)\n; => (sin (- x 1))\nLisp notes: car can be thought of as y[[1]] and cdr can be thought of as y[-1]. setf is a general version of setq that can handle the equivalent of [<- in R.\nData to Code\nThe last piece of the puzzle is taking the data that we have manipulated and evaluating it as code once again. Then we’re metaprogramming. Continuing from above, we do this in both Lisp and R with eval, after setting a value for x.\n## R\nx <- 1\neval(y)\n# [1] 0\n\n;; Lisp\n(setq x 1)\n(eval y)\n; => 0.0\nQuote derivatives\nWith our understanding of quote firming up it is natural to see if we can use it to understand other related concepts like unquoting and quasiquotation. Applying the natural interpretation of the prefixes ‘un’ and ‘quasi’ we might guess:\nunquoting is the reverse of quoting? Machine-readable back to human readable? Can it be be ‘evaluation’? We already have a word for that.\nquasiquotation is something that is almost but not quite quotation?\nAnd we’d be wrong on two counts. But it’s not our fault - these are terms are straight up jargon. The concepts they describe are simple to explain and motivate though. A last Lisp example should do the trick:\nLet’s say we are operating in a scenario where we are trying to compose a list to eventually become function call that takes a function as an argument. We have to build two lists, quoting most of the arguments, except the last one where we want the result of the code in that position, (+ 1 1), as the argument.\nWe are actually quoting more often than we are evaluating, e.g:\n(list 'myfun 'arg1 (list 'myfun2 'arg2 (+ 1 1)))\n; => (MYFUN ARG1 (MYFUN2 ARG2 2))\nUgh, very noisy. It would be convenient syntactically if we could switch into a complimentary mode where instead of telling Lisp what we want to quote, we just tell it what needs to be evaluated. Quoting becomes the standard procedure.\nThis anti-quoting mode is what was named ‘quasiquotation’. In Lisp you switch it on using the ‘`’ operator, and you signal things needing evaluation using a ‘,’. So when ‘quasiquoting’ the above function would be:\n`(myfun arg1 (myfun2 arg2 ,(+ 1 1)))\n; => (MYFUN ARG1 (MYFUN2 ARG2 2))\nSignaling things need evaluating in the midst of quasiquotation is what has been called unquoting. Notice the effect of unquoting is that the result of the evaluation, '2', becomes data in the list that is returned.\nIn simple cases like above unquote is effectively saying ‘evaluate’. But it does make sense to have a different term, since it is possible to have nested quasiquotation and unquote is not equivalent to evaluate in that context - it’s an evaluation somewhere down the line.\nFYI you’ve now seen the Lisp ‘backquote macro’ referred to by R’s bquote that I showed the help for in the introduction.\nA Distillation\nNow let me try and distill the concepts we have covered into a few memorable one-liners:\nquote transforms code into data.\nThat data being the machine-readable symbolic structure needed to formally evaluate it.\n\nquasiquotation is a mode that assumes we want all code transformed to data.\nunquote flags code that we wish to have the result of transformed to data.\nIn the context of quasiquotation.\n\nConclusion\nThe big light bulb arising from this for me was when I gained appreciation for quote as a code -> data transformation. I think the main reason I found studying quoting in Lisp revelatory in this regard is that the mechanics are a) simple and b) predictable. Code and data are both plain lists. These properties make Lisp a useful sandbox for building confidence with metraprogramming ideas.\nIn R you can really only have a) OR b) but not both. Base R quoting and Non-Standard evaluation functions are few and seem simple on the surface but are dogged by behaviour that changes with context (e.g. substitute) or are less general/robust than we might like (e.g. bquote).\nTidy eval deserves our appreciation for doing something similar to what purrr did for functional programming: it’s rounding off the jagged edges in the API and making metaprogramming in R much more stable and predictable. The trouble, as I have discussed previously, is this has come at the cost of simplicity. There’s much work to be done to break that down and this post has just focused on one foundational aspect.\n\nExamples: bquote (base R), quasiquotation (rlang), quasiquotation (Advanced R)↩︎\nIn truth there are other types of calls, and the ones Lisp nuts really bang on about are macro calls↩︎\nThought experiment for the reader: Does this hold for R?↩︎\nCalled names like ‘lexical analyser’ and ‘parser’↩︎\n",
    "preview": "posts/the-roots-of-quotation/Quote---2-.jpg",
    "last_modified": "2024-03-11T22:25:22+10:00",
    "input_file": {}
  },
  {
    "path": "posts/r2vr3-shading-meshes-in-webvr/",
    "title": "R2VR: Shading Meshes in WebVR",
    "description": "In this post I'll contiue to riff on the Uluru mesh, discussing how to colour a WebVR mesh by shading using a raster layer, or applying a texture.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-07-05",
    "categories": [
      "rstats",
      "R2VR"
    ],
    "contents": "\n\n\n\nIn the previous post in this series I introduced A-Frame, r2vr, and demonstrated how to render a mesh in WebVR using #rstats. In this post I’ll contiue to riff on the Uluru mesh, discussing how to colour a WebVR mesh by shading using a raster layer, or applying a texture.\nAlong the way we’ll talk about smoothing, lighting, and tweaking workflow with the A-Frame inspector.\nShading\nThe last post left off at this:\n\n\n\nFigure 1: The ‘peach puddle’ example\n\n\n\nAn iconic Australian landmark looking a little off-colour. There are several ways to improve this, and the first way we will consider is shading using a raster layer. This is quite useful if you have raster layers that describe distributions of variables of interest like say, vegetation cover. Indeed this is what is happening here in this map, from the introductory post to this series:\n\n\n\nFigure 2: Elevation, vegetation, and water in WebVR\n\n\n\nIn this picture you can make out the triangular resolution of the underlying mesh, but notice that the colours transition smoothly, within an individual mesh face. One way to do this would be with a texture, but that would need to be quite a large high-resolution image. Instead, this has been achieved using a run-time procedure called ‘vertex shading’. The idea is that instead of suppling a texture, we supply a colour value for each mesh vertex, and the user’s graphics card does a smooth interpolation between them.\nSince Uluru doesn’t have much going on in terms of vegetation, I decided to use the original elevation raster as colour source as well. So the Uluru mesh will be coloured according to height. It sounds boring, but the results look quite arty if I do say so myself.\nI’ll omit the code we’ve discussed previously, but complete prerequisite code is present in the companion respository for this post. Here’s the breakdown of how we can shade the mesh, assuming it is called uluru_bbox_trimesh:\nSetup\n## install latest r2vr using devtools\n## install_github('milesmcbain/r2vr')\n\nlibrary(raster)\nlibrary(scico)\nlibrary(r2vr)\nlibrary(tidyverse)\n\n## load JSON conversion helper function\nsource(\"./helpers/trimesh_to_threejson.R\")\n\n## load colour palette index helper function.\nsource(\"./helpers/colour_from_scale.R\")\n\n## load vertex to face colour conversion\nsource(\"./helpers/vertex_to_face_colours.R\")\nExtract raster data\nIn this case we already have this data in uluru_bbox_trimesh$P[,3], so this code is just for demonstration, to show what to do if using a new raster that isn’t height.\n## Extract data from raster for each vertex (x,y) coord.\ncolouring_raster_data <-\n  raster::extract(nt_raster, uluru_bbox_trimesh$P[, 1:2])\nApply colour palette\nWe’re taking a vapourwave colour scheme from the ‘tokyo’ palette in Thomas Lin Pedersen’s scico package. I’ve written a couple of helpers for arranging the colour data. vec_pal_colours() will take:\n* a numeric vector of values\n* a colour palette function\n* a number of colours\nand return:\n* a vector of colours as integers\n* and a vector of indexes into the colour vector for each numeric value.\nHere we again run into the idea of using an index wherever possible to compress data structures that is common in 3D programming. Note the ‘trick’ of using purrr::partial to fill in the palette argument of scico(), so it can be blindly applied with just the number of colours argument.\nA second helper, vertex_to_face_colours(), is used to reshape the list of colour indexes: The 3D JSON format needed for WebVR wants a colour mapping per vertex per face, vec_pal_colours() only created a colour mapping per vertex. The helper takes the list of vertex colour indicies and the list of face vertex indices from uluru_bbox_trimesh$T and returns a 3 column matrix of face colour indicies, each column representing a vertex (1, 2, and 3), and each row representing a complete triangular face.\n## Choose a palette function to transform to colours\nn_colours <- 256\npalette_function <-\n  purrr::partial(scico, palette = \"tokyo\")\n \n## Generate colours\nvertex_colour_data <-\n  vec_pal_colours(colouring_raster_data, palette_function,\n                    n_colours, zero_index = TRUE)\n                    \nstr(vertex_colour_data)\n# List of 2\n# $ colours: num [1:256] 1641779 1773107 1969716 2035765 2101303 ...\n# $ indexes: num [1:8913] 9 9 5 8 246 9 7 19 8 11 ...\n\nface_colours <-\n  vertex_to_face_colours(vertex_colour_data$indexes,\n                         uluru_bbox_trimesh$T)\n                         \n str(face_colours)\n # num [1:17619, 1:3] 9 8 9 9 9 8 30 237 26 9 ...                      \nGenerate Mesh JSON\nTo generate the JSON we supply the list of all colours we want to use, and the face-wise vertex indicies into that list, as extra arguments to the trimesh_to_threejson() function we used in the previous post.\n## Generate a shaded model JSON\nmesh_json <-\n  trimesh_to_threejson(vertices = uluru_bbox_trimesh$P,\n                       face_vertices = uluru_bbox_trimesh$T,\n                       colours = vertex_colour_data$colours,\n                       face_vertex_colours = face_colours)\n\n## write JSON\nwrite_file(mesh_json, \"./data/uluru_mesh.json\")\nRender in WebVR\nWe create an asset to hold the JSON and a JSON model entity as per the previous peach puddle example. A couple of new entities are on display after that. We create a slightly more dramatic black backdrop entity in sky, and then add an entity that makes keyboard and mouse control slightly more convenient.\n## Render in VR\n## Bigger than our previous 'puddle':\nscale_factor <- 0.01 \n\nuluru_json <-\n  a_asset(id = \"uluru\",\n          src = \"./data/uluru_mesh.json\")\n\nuluru <-\n  a_json_model(src_asset = uluru_json,\n               scale = scale_factor * c(1, 1, 1),\n               position = c(0, 0 + height_correction * scale_factor, -15),\n               rotation = c(-90, 180, 0)\n               )\n\nsky <- a_entity(tag = \"sky\",\n                color = \"#000000\")\n\ncontrols <- a_pc_control_camera()\n\naframe_scene <-\n  a_scene(template = \"empty\",\n          title = \"Uluru Mesh\",\n          description = \"An A-Frame scene of Uluru\",\n          children = list(uluru, sky, controls))\n\naframe_scene$serve()\nbrowseURL(\"http://127.0.0.1:8080\")\n\n## don't forget to:\naframe_scene$stop()\nResulting in:\n\n\n\n\nFullscreen link: https://plucky-pencil.glitch.me/\nSmoothing\nOur mesh looks pretty ‘jaggy’ but precisely this low-poly look is kind of hip right now in WebVR land. If you don’t care about being hip, you can ask the user’s browser to compute a set of normal vectors for each vertex that their graphics card will use to interpolate a smooth 3D surface (so called ‘vertex normals’). Rendering smooth things is more computationally costly so use with caution. It is worth noting that these vectors are just estimated from the normal vectors of surrounding faces. So they can produce a result that departs from the ‘true’ object shape. Doing this in r2vr is just one extra argument to a_json_model():\n## Smooth using vertex normals\nuluru_smooth <-\n  a_json_model(src_asset = uluru_json,\n               mesh_smooth = TRUE,\n               scale = scale_factor * c(1, 1, 1),\n               position = c(0, 0 + height_correction * scale_factor, -15),\n               rotation = c(-90, 180, 0))\n\naframe_scene2 <-\n  a_scene(template = \"empty\",\n          title = \"Uluru Mesh\",\n          description = \"An A-Frame scene of Uluru\",\n          children = list(uluru_smooth, sky, controls))\n\naframe_scene2$serve()\n\n## don't forget to:\naframe_scene2$stop()\n\n\n\nFigure 3: Smoooooooth\n\n\n\nTexturing\nIf you’re still reading, I’m guessing you’re here for this. The gif I posted of this example to Twitter got some attention - Thankyou all for the encouraging words!\nI’m only going to summarise this, because in truth I’m not convniced I fully appreciate it. The code to fetch the satellite tile and calculate the all important texture coordinates was gifted to me by the amazing Michael Sumner.\nIt fetches a satellite tile from Mapbox using the slippymath package, and then does a dance of normalising our mesh vertex coordinates to range between 0 and 1, based on where they fall within the satellite image extent. In 3D graphics, texture coordinates (called UVs) always fall in the interval [0,1]. The conversion to an image pixel coordinate is made at run-time by graphics hardware.\nOnce the texture coordinates are obtained, generating the JSON for the 3D model is again a straight call to trimesh_to_threejson(), but for one minor complication: When we pass a texture file to be referred to in a model, we have to keep in mind that its path needs to be expressed relative to the model location, not our own current working directory - If you think about this it is really the only way models + textures could be portable. Below are the two tranches of code to generate a textured mesh and render it in WebVR, making use of entities we already created.\nBuild textured mesh\nWe’re taking advantage of slippmymath to conveniently fetch Mapbox satellite imagery. If you reproduce this, be wary of terms of use.\nlibrary(fs)\nlibrary(slippymath)\nlibrary(curl)\nlibrary(glue)\n\n## Fetch a tile from mapbox calculate the tiles with slippymath\nuluru_tiles <- bb_to_tg(uluru_bbox, max_tiles = 40)\n\nmapbox_query_string <-\n  paste0(\"https://api.mapbox.com/v4/mapbox.satellite/{zoom}/{x}/{y}.jpg90\",\n         \"?access_token=\",\n         Sys.getenv(\"MAPBOX_API_KEY\"))\n\nimages <-\n  pmap(uluru_tiles$tiles,\n       function(x, y, zoom){\n         outfile <- glue(\"{x}_{y}.jpg\")\n         curl_download(url = glue(mapbox_query_string),\n                       destfile = outfile) \n         outfile \n       },\n       zoom = uluru_tiles$zoom)\n\n## composite images with slippymath\nuluru_raster <- tg_composite(uluru_tiles, images)\n\n## Crop the image\nuluru_raster <- raster_crop_bbox(uluru_raster, uluru_bbox)\n\n## write to PNG (that's the only way we can texture map)\ntexfile <- \"./data/uluru_satellite.png\"\nraster_to_png(uluru_raster, texfile)\n\n## calculate the trimesh x and y in 0 - 1 space for texture coordinates\n## using r2vr.gis::range_scale\nxym <- uluru_bbox_trimesh$P[,1:2]\n\nxyim <- apply(xym, 2, range_scale)\n## This works because the image and the mesh have the same area and the area is\n## relatively small, so the won't be much opportunity for the texture to be\n## distorted by difference in projection.\n\n\n## generate JSON containing texture\n## pass just the name of the texture file so rendering process will look in the\n## same directory as JSON model\nuluru_tex_json <-\n  trimesh_to_threejson(vertices = uluru_bbox_trimesh$P,\n                       face_vertices = uluru_bbox_trimesh$T,\n                       vertex_uvs = xyim,\n                       texture_file = fs::path_file(texfile)\n                       )\n\n## write JSON file\nreadr::write_file(uluru_tex_json, \"./data/uluru_tex.json\")\nRender in WebVR\nSmoothing the mesh is good idea when applying a texture. Notice how we wrap the model and associated texture up in a single asset object.\nNote this example used Google map data, prior to them locking down their tile API\n## The JSON references the satellite png file so it needs to be \n## included in 'parts' so R can serve it.\nuluru_tex <- a_asset(src = \"./data/uluru_tex.json\",\n                     id = \"uluru_tex\",\n                     parts = \"./data/uluru_satellite.png\")\n\nuluru <-\n  a_json_model(src_asset = uluru_tex,\n               mesh_smooth = TRUE,\n               scale = scale_factor * c(1, 1, 1),\n               position = c(0, 0 + height_correction * scale_factor, -15),\n               rotation = c(-90, 180, 0))\n\naframe_scene3 <-\n  a_scene(template = \"empty\",\n          title = \"Uluru Mesh\",\n          description = \"An A-Frame scene of Uluru\",\n          children = list(uluru, sky, controls))\n\naframe_scene3$serve()\n\n## don't forget to:\naframe_scene3$stop()\nThe result:\n\n\n\nFigure 4: Map texture: Google, (c) 2018, CNES / Airbus, DigitalGlobe, Landsat / Copernicus\n\n\n\nLighting\nOne of the things you’ll quickly realise when playing around in VR is that lighting can have a huge impact on the mood of your scene. For example here’s an attempt at a sunrise/sunset vibe, created entirely by tweaking the colour, intensity, and position components of the two default lights:\n\n\n\nFigure 5: Sunrise on the rock\n\n\n\nLonger shadows etc are needed to fully carry it off, but for something I whipped up in a few minutes of playing around it’s okay. The point I wanted to make here is that this type of creative task would be an absolute chore to do by repeatedly editing code and re-serving your A-Frame scene - and the good news is you don’t have to.\nUsing the A-Frame Inspector\nA-Frame has something called the ‘A-Frame Inspector’ that allows you to add, remove, and tweak entity configuration whilst viewing the results live. It really is freaking awesome for creative tweak-sessions. Once you find something that looks great, it’s just a matter of translating that config back to your code, there’s even a helpful facility to copy entity config as HTML, which has a close mapping to r2vr syntax. Access the inspector in any scene with the keys ctrl + alt + i.\n\n\n\nFigure 6: Go go gadget lights!\n\n\n\nNote that the default lights (1 directional, 1 ambient) are injected into the scene automatically if none are configured. When you configure your own light entities, be sure to consider mixing multiple types like this for best results.\nConclusion\nWe covered a lot of ground here, but hopefully you’re starting to get a feel for the power placed at your finger tips by this tool. I’ve started little gallery of my experiments on the r2vr repo page that may inspire you to try something, and if you do I’d love to hear about it.\nI’m under no illusions that the code presented here requires a significant degree of technical knowledge to fully appreciate. The great thing about this framework is it is highly composable, and my belief is this will mitigate this issue as the tool matures. For example, it might make sense to abstract all the helper methods floating around for generating model JSON into an a_* function that takes all the pieces and returns an model entity with the compiled JSON asset attached. This would be pretty simple to do, but I’m hesitant to optimise the higher level API without more dog-fooding and more feedback.\nSpeaking of feedback - I’ll have a poster in the Wednesday session at UseR!2018 next week. Feel free to chat to me about this or anything else :)\nNext up\nNext up I’ll be discussing how to annotate meshes using WebVR elements. There’ll be some species distrubution data I’ll plot over a mesh and experiment with ways of presenting distribution model predictions made in R.\nWork described here is part of a project funded by ACEMS in partnership with CSIRO Data 61, at Queensland University of Technology. Specialist support has been provided in-kind by the Australian Antarctic Division.\n\n\n\n\n\n\n",
    "preview": "posts/r2vr3-shading-meshes-in-webvr/uluru_header.png",
    "last_modified": "2024-03-11T22:23:35+10:00",
    "input_file": {},
    "preview_width": 1834,
    "preview_height": 1058
  },
  {
    "path": "posts/crushing-the-contact-details-file/",
    "title": "Crushing the Contact Details file with #rstats",
    "description": "It is a universal law that if you volunteer to help organise something, you will at some stage be issued with a flat text file of contact details.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-06-24",
    "categories": [
      "rstats",
      "regex",
      "productivity"
    ],
    "contents": "\n\n\n\nIt is a universal law that if you volunteer to help organise something, you will at some stage be issued with a flat text file of contact details to be input into a table. This post is about how you can crush that file into a fine, fine powder, using the power of R.\nMy last encounter was two weeks ago, with 40 or so records arriving by Slack message, and needing to be input into a Google Sheet. Here is a dramatic recreation:\nLatonia Lamer l.lamer@outluck.com\nVerlie Van Der Vales vvdv@gmule.com\nBranden Bohner brenden.bohner@hurtmail.com\nAlejandro Amezcua a.amezcua@outluck.com\nAnna Armijo anna.armijo@freemail.com\nDanny De Downer deedeedee@hurtmail.com\nMathilde McShane matty.mcshae@gmule.com\nMalorie Moroney maloriem85@gmule.com\nAmina Amico a.amico@outluck.com\nRuss Ruppe russ@russtastic.com\nMario McRae  mario@thatismcrae.com\nNote: Randomly generated details.\nIt is amazing how perfectly annoying this task is. I am sure it is POSSIBLE to write an R script that reads from the Slack message into R, munges it, and writes it directly to the spreadsheet, but you’d never recover the time you invested to write it. Next time the data will be attached to an email, or Trello card, and needing to go to Office365, or GitHub, or whatever, and you’re back at square one.\nSo what to do? It’s a small enough task that manual entry is an option. Suck it up and bash it out?\n\n\n\nFigure 1: Never!\n\n\n\nHere is a fast, but completely unashamedly non-reproducible way to get this done:\n1 Copy\n\n\n\nFigure 2: whistling is heard\n\n\n\n2 Paste\n\n\n\nFigure 3: Pasting with ‘datapasta’\n\n\n\n3 Munge\nNow things get interesting. We have a text munging problem. And rather than make that eye-roll joke about we need Regular Expressions (Regex) therefore we have 2 problems (Haw haw), this is a great opportunity for some practical advice about munging text.\nThere is no text munging problem that cannot be solved by a series of extract, substitute, or split steps. Regex just help us describe what needs to be extracted, substituted, or where to split the string. Discovering the order to apply these steps is the puzzle, and once you realise that’s all there is to it, it can actually be kind of fun, just like group_by, mutate, filter, summarise et. al.\nThe challenging part of these data are the ‘special people’ with multiple last names i.e. ‘Verlie Van Der Vales’ and ‘Danny De Downer’. If not for them we could use a simple ‘split on space’ strategy. (Here’s looking at you Steph De Silva).\nSo after naming the vector, we apply these steps:\nflat_name_email <-  \n  c(\"Latonia Lamer l.lamer@outluck.com\",\n    \"Verlie Van Der Vales vvdv@gmule.com\",\n    \"Branden Bohner brenden.bohner@hurtmail.com\",\n    \"Alejandro Amezcua a.amezcua@outluck.com\",\n    \"Anna Armijo anna.armijo@freemail.com\",\n    \"Danny De Downer deedeedee@hurtmail.com\",\n    \"Mathilde McShane matty.mcshae@gmule.com\",\n    \"Malorie Moroney maloriem85@gmule.com\",\n    \"Amina Amico a.amico@outluck.com\",\n    \"Russ Ruppe russ@russtastic.com\",\n    \"Mario McRae  mario@thatismcrae.com\")\n\n## Splitting/extract/substitute with stringr\nlibrary(stringr)\n\n## Extract emails\nemails <- str_extract(flat_name_email, \n                      \"[A-Za-z0-9.]+@[A-Za-z0-9.]+\")\n\n## Substitute emails with \"\", leaving full name.\nnames <- str_replace_all(flat_name_email, \n                         \"[A-Za-z0-9.]+@[A-Za-z0-9.]+\", \"\")\n\n## Split on the first space, so we have first name \n## and last names in separate vectors. \n## Simplify returns an array - helpful!\nfirst_last <- str_split(names, \" \", n = 2, simplify = TRUE)\n\n## Look at names: trailing whitespace issue\nfirst_last\n\n##\n##      [,1]        [,2]            \n## [1,] \"Latonia\"   \"Lamer \"        \n## [2,] \"Verlie\"    \"Van Der Vales \"\n## [3,] \"Branden\"   \"Bohner \"    \n### ...\n\n## Trim trailing spaces (a substitute step \" \" for \"\")\nfirst_last[,2] <- trimws(first_last[,2])\n\n## Combine in dataframe\nattendee_data <- data.frame(first_name = first_last[,1],\n                            last_name = first_last[,2],\n                            email = emails)\n  \nattendee_data\n##    first_name     last_name                       email\n## 1     Latonia         Lamer         l.lamer@outluck.com\n## 2      Verlie Van Der Vales              vvdv@gmule.com\n## 3     Branden        Bohner brenden.bohner@hurtmail.com\n## ...\nTo understand this code completely you only need to understand one Regex that describes an email: [A-Za-z0-9.]+@[A-Za-z0-9.]+.\nIn English this says:\n* [A-Za-z0-9.] A Character that is an upper or lower case letter, number, or a dot. + one or more times.\n* @ followed by an @\n* [A-Za-z0-9.] followed by a character that is an upper or lower case letter, number, or a dot. + one or more times.\nNotice how only by extracting and substituting away the emails, were we free to apply a simple strategy of ‘splitting on the first space’ to solve the remainder of the problem.\nOther things to note:\n* It is possible to write this in similar #tidyverse style code using only tidyr::extract, and tidyr::separate. That would have made for more difficult commentary, and so is left as an exercise to the reader.\n* It is possible to solve the whole problem in one monstrous abomination of a Regex, but you wouldn’t bother.1\n4 Copy\n\n\n\nFigure 4: Matthew Lincoln’s ‘clipr’ is the unsung hero of many #rstats packages including ‘reprex’ and ‘datapasta’\n\n\n\n5 Paste\n\n\n\nFigure 5: whistling resumes\n\n\n\nConclusion\nHow nice is it to be able to harness the scything power of R without having to deal with yet another data import/export problem? Does anyone else ever wonder about a world where programs are not so precious about their data, and we could just tell a computer: “Pipe data from my last Slack message into RStudio please.” and it JUST DOES IT. I’m all for data security, but within my own system I don’t need a disparate set of fortresses accessible to each other only by cumbersome protocol. I want my apps to form a team that cooperates freely under the banner of me getting shit done.\nHeader image credit:\nBy Jot Powers, 1/2006, CC BY-SA 2.5, https://commons.wikimedia.org/w/index.php?curid=558469\n\nFor the Stack Overflow crowd: first_last_email <- str_match(flat_name_email, \"(^[A-Za-z]+)\\\\s+((?:[A-Za-z]+\\\\s+)+)([A-Za-z0-9.]+@[A-Za-z0-9.]+$)\")↩︎\n",
    "preview": "posts/crushing-the-contact-details-file/1024px-Batman_-truck-.jpg",
    "last_modified": "2024-03-11T22:19:15+10:00",
    "input_file": {}
  },
  {
    "path": "posts/r2vr2-rendering-meshes-in-webvr/",
    "title": "R2VR: Rendering Meshes in WebVR",
    "description": "How to render a 3D mesh of Uluru in WebVR using the A-Frame framework via R.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-05-29",
    "categories": [
      "rstats",
      "R2VR"
    ],
    "contents": "\n\n\n\nIn my previous post in this series I showed you how to generate a 3D mesh of Uluru using R and public data. This post will discuss how that mesh can be rendered in WebVR using the A-Frame framework via R. WebVR is appealing as a target platform due to its support on a wide range of hardware, including common smart phones.\nIntroducing A-Frame\nA-Frame is a framework from Mozilla that lets you describe a WebVR scene in HTML. Just like a regular web page, you can use the might of Javascript to manipulate the HTML elements and their properties to create a dynamic and engaging scene. The programming model uses an entity-component-system architecture 1 - if you squint it’s similar to R’s S3, in the sense that a single entity can be decorated with many components that add new behaviours.\nTo make this concrete here’s a simple “Hello World” type scene:\n\n\n\nBe sure to check the output on the ‘App’ tab. You can control the view with the mouse and the WASD keys. The blue cube entity is declared by the <a-box> tag. You can play with its components to change its behaviour or appearance, for example:\n* Changing the colour using the color component.\n* Altering the speed or axis in in the spin component.\n* Remove the shadow by deleting the shadow component.\n* Scale it up or down in size using the scale component.\nIf you’ve done any web programming you’ll notice this is just a matter of updating HTML element properties. This is extremely convenient for us, since generating and serving HTML from R is a well trodden path, with lots of helpful tooling!\nI could say a lot more about A-Frame but it’s not the subject of this post. Instead I’ll just say two things: If you’re interested in learning how to use it and have just a couple of hours, the A-Frame School is great crash course. Secondly, the limits to what you can achieve with it are WAY out there. Is anybody excited about mixed-reality multi-user data visualisation? How about a data capture environment that is one in the same as the data visualisation environment?\nRendering Uluru\nThe companion GitHub repo to this post contains the GIS data and trimmed down R code from the last post to generate the 3D mesh similar to what we ended up with last time. After running the prerequisite section, the mesh ends up stored in a variable called uluru_bbox_trimesh.\nThe next step is to convert that mesh to a 3D model JSON format using this specification. I have written a function to do that called trimesh_to_threejson()2. This function makes one important change to the mesh: it centres the vertex coordinates without changing the scale. Centering is required because the WebVR scene treats the model vertices as absolute positions in space, but by convention the scene is built up near the origin. Without centering, the projected coordinates in meters of Uluru are very farm from 0, 0, 0!\nTo render Uluru in WebVR we’re going to use an R package I have created called r2vr. This package helps you generate and serve A-Frame HTML. It’s currently GitHub only, so you can install with devtools::install_github(\"milesmcbain/r2vr\"). Finally, since our Uluru mesh is on a 1:1 scale, and VR meters look like real world meters, I suggest scaling it by a factor of 0.001 initially.\nPutting all that together looks like this:\n## install r2vr using devtools\ninstall_github('milesmcbain/r2vr')\nlibrary(r2vr)\n\n## load JSON conversion helper function\nsource(\"./helpers/trimesh_to_threejson.R\")\n\n## After prerequisite code, our mesh is now in uluru_bbox_trimesh.\n\n## write to JSON\nmesh_json <- trimesh_to_threejson(vertices = uluru_bbox_trimesh$P, face_vertices = uluru_bbox_trimesh$T)\nwrite_lines(mesh_json, \"./data/uluru_mesh.json\")\n\n## render in A-Frame\n## use a 1/1000 scale factor because Uluru is really big!\nscale_factor <- 0.001\n\nuluru <- a_asset(id = \"uluru\",\n                 src = \"./data/uluru_mesh.json\")\n\naframe_scene <-\n  a_scene(.template = \"basic\",\n          .title = \"Uluru Mesh\",\n          .description = \"An A-Frame scene of Uluru\",\n          .children = list(\n            a_json_model(src = uluru,\n                         material = list(color = '#C88A77'),\n                         scale = scale_factor*c(1,1,1),\n                         position = c(0,0,-3),\n                         rotation = c(0, 0, 0))))\naframe_scene$serve()\n\n## don't forget to:\naframe_scene$stop()\nAnd our reward is this:\n\n\n\nFigure 1: Uluru flipped and half-buried\n\n\n\nSo not quite there yet. On the bright side we just got a lesson in the A-Frame coordinate system: In A-Frame, The third spatial axis (Z) points toward/away from the viewer, while the Y-axis points “up” and “down”. I keep one of these by my monitor, you may find that helpful too:\n\n\n\nFigure 2: The A-Frame Coordinate System\n\n\n\nCorrecting Uluru\nTo get Uluru right side up we can imagine applying some kind of spatial rotation, say -90 degrees about the X-axis. That would be a good start. There’s one other problem we need to correct: Since we centred all the axes, Uluru sitting right side up will also be sitting halfway into the floor. We’ll apply a correction to the Y-axis height to fix this.\nTo correct the height of the model, we need to know what the ‘ground’ height was in our original raster. This is subjective and will depend on the terrain and how you want to render it. We’re lucky since Uluru appears to be a bit of an island in an outback sea. I’ve used the maximum height of the corners of the bounding box, extracted from the raster, as the ground height. To make the correction, we add the difference of the ground height and the mean height of the mesh to the model position. The code below will make this clear. Note that we also need to multiply the height correction by the scale factor:\n## We need to correct the height based on ground height.\n## In this case we'll find ground height from the  highest corner of the bounding box.\nground_height <-\n max(raster::extract(nt_raster, uluru_bbox_mpoly[[1]][[1]][[1]], nrow = 1))\n\nheight_correction <- -1 * (ground_height - mean(uluru_bbox_trimesh$P[,3]))\n## We're reversing the correction that would have been applied to the\n## ground height by centering.\n\n## Rotated and height corrected render:\n\nscale_factor <- 0.001\n\nuluru <- a_asset(id = \"uluru\",\n                 src = \"./data/uluru_mesh.json\")\n\naframe_scene2 <-\n  a_scene(.template = \"basic\",\n          .title = \"Uluru Mesh\",\n          .description = \"An A-Frame scene of Uluru\",\n          .children = list(\n            a_json_model(src = uluru,\n                         material = list(color = '#C88A77'),\n                         scale = scale_factor*c(1,1,1),\n                         position = c(0,0 + height_correction * scale_factor ,-3),\n                         rotation = c(-90, 180, 0))))\naframe_scene2$serve()\n\n## don't forget to:\naframe_scene2$stop()\nAnd this time we get:\n\n\n\nFigure 3: Uluru or a Frappé accident?\n\n\n\nIf you’ve run this locally, take a ‘walk’ around using WASD and your mouse, and appreciate it from a few different angles. One thing that may interest you is that on the scale of the model, each green grid square is a 1km x 1km.\nAnd if you have a VR capable device be sure to connect to your scene and experience it in VR-propper! The ngrok tool is very handy for exposing scenes you’re serving locally over the internet.\nDiscussion\nIn this post we took but a few baby steps toward data-driven virtual realities. Perhaps calculating the VR height correction factor using data from the scientific computing environment hints at why it bridging these things might be a good idea. If we couldn’t input the calculation from R, we’d have to have cut and paste a magic number with 6 or so significant figures into a html file and that just feels bad.\nr2vr is very much a work in progress, but since it is really just a HTML generator/server, it is possible to use it make all kinds of data driven scenes already. Unfortunately to get the most out of it right now, you’ll need an appreciation for A-Frame. The A-Frame School I mentioned is good resource, as is the A-Frame Documentation. I do feel bad saying that, but I plan to follow this post with many more examples!\nGetting Experimental\nIt’s quite interesting to me how using this tech brings out a playful ‘arty’ side I wouldn’t have guessed I possessed. Rigging up a good looking scene feels like cross between painting and photography. With the basic pieces I’ve shown so far, there’s some fun to be had looking at ways to appreciate the scale of the model.\nFor example, you might try rendering Uluru at full scale and placing it a known distance away. Here’s a pic I made of it at full scale from a vantage 2km away (2000 VR distance units):\n\n\n\nFigure 4: Uluru in Architectural Grey\n\n\n\nFor this type of thing try using the template = \"basic_map\", which has lighting that is a bit more sympathetic to large geographical scenes.\nA slightly more wild idea I had was to place the viewer and a scale model of the Sydney Opera House inside Uluru to try to get a sense of its volume. I’ve published a live version of this on Glitch.\nUp Next\nIn my next post in this series I’ll explain how to shade a mesh using GIS layers. Along the way we’ll talk about making it less “jaggy” using vertex normals, the A-Frame lighting system, and the A-Frame inspector.\nWork described here is part of a project funded by ACEMS in partnership with CSIRO Data 61, at Queensland University of Technology. Specialist support has been provided in-kind by the Australian Antarctic Division.\n\n\n\nHeader image credit: By Miles McBain - Rock Opera, CC-BY-SA 3.0\n\nI’d never heard of it either, but well worth the read if you’re into this thing called programming.↩︎\nIdeally we would write the mesh in glTF format which A-Frame has excellent native support for. There isn’t a glTF writer for R although I am aware of one in Python that is lacking documentation. Reticulate may be an option to explore. I’d love some help with this problem!↩︎\n",
    "preview": "posts/r2vr2-rendering-meshes-in-webvr/rock_opera.png",
    "last_modified": "2024-03-11T22:22:03+10:00",
    "input_file": {},
    "preview_width": 1921,
    "preview_height": 1083
  },
  {
    "path": "posts/2018-in-pursuit-of-productivity/",
    "title": "2018: In Pursuit of Productivity",
    "description": "2018 was probably my busiest year on record. I reflect on some of the productivity tools that got me through.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-05-23",
    "categories": [
      "productivity",
      "software development"
    ],
    "contents": "\n\n\n\n2018 was a crunchy one. I feel like I started the year behind and finally caught up yesterday when I put the finishing touches on my rstudio::conf talk - yesterday being Janurary 3, 2019.\nThis was also the year I learned what burnout looks like - my episode happened shortly after UseR. In retrospect I was all in on a mindset of “I just have to make it to UseR” - and I did that, and it was amazing - but after that everything just kind of crashed. It took me some time to get out of a pretty negative space.\nI spent a lot of this past year feeling frustrated with my progress as deadlines loomed. Just now I counted up the number of ‘line in the sand’ deadlines I met this year, and for me, it’s pretty high:\nI count:\nMachine Learning workshop @ ABS\nR Foundation summit\nUseR\nPoster @ UseR\nConsutling project go-live\nSoftware Carpentry workshop @ QUT\nGit workshop @ ACEMS Retreat\nr2vr ReefVR Demo\nr2vr + quadmesh workshop @ FOSS4GSOTM\nWith the way I work, there’s always a crunch with deadlines. Something something perfectionism. But with a young child at home now I couldn’t fall back on my old tactics of working through into the early hours. 1 I had to get REALLY serious about optimising the time I did have. Some of the things I got up to are pretty nerdy and may repulse you. I make no apologies! The rest of this post is about those tools I picked up that helped.\nPomodoro\nPomodoro is a productivity system where you divide your work day into intervals and mini breaks. Lengths vary. I do: work 25 minutes, break for 5. Once you complete 4 ‘reps’ you get a longer break for 20 minutes, or longer if it’s lunchtime.\nThis is not a thing I do religiously. But I find myself going back to this every time I feel the pressure because it reliably boosts my productivity.\nHere’s my take on why:\nMy struggle for productivity is really a struggle against interruptions and micro distractions that pull me out of my flow and trigger a procrastination. And I’ve noticed I’m especially susceptible to this when doing taxing mental work. I liken it to trying to lift a heavy weight with someone trying to make you laugh. You’ll crack and drop the weight - not because you felt additional pain, but because secretly you were glad of the excuse to.\nPomodoro works because at any given moment you have a break coming up in at most 25 minutes. So when one of these distractions pops up, be it some kind of notification, a colleague, a thought out of the blue, a compulsion to check social media, whatever, you can dismiss it easily with: ‘I’ve only got X minutes until my break, I’ll return to this then’. Funnily enough I often do not return to it.\nI use the Brain Focus Android app as my pomodoro timer. I tried a few and like this one best. I find this works well because I play music using my phone during an interval, and the app will vibrate the phone and turn down the music to play an alarm signaling the end of the interval. There’s no chance you can miss it.\nThe Lucy D’Agostino McGowan method\nI don’t even know where to start with Lucy’s post One Year to Dissertate. It’s a blueprint for taking control of your time to deliver a large project.\nIt’s had an impact on me - to the point where I’d say the things I took from it are what got me through the last leg of 2018. You should just go read it. It’s this interesting mix of simple tools arranged in a simple system - but the combination feels so sophisticated and clever.\nI wasn’t preparing a dissertation, but the things I found in the section on planning worked just as well for the various workshops I had to prepare. Immediately useful were:\nThe time-boxed sessions for eliciting project tasks which you prioritise and assign real deadlines.\nMaking weekly plans with tasks drawn onto your week.\nDedicating time to revisit both above.\nWhen I feel a deadline looming I tend to develop tunnel vision. But in late 2018 I had several important unrelated deadlines landing in quick succession. Tunnel vision would have set me up for to failure. It was Lucy’s planning templates that helped make clear to me exactly what time I really had, and how that would have to affect the scope of each project.\nAssigning real deadlines was painful as I was used to a to-do list type approach, where you have a queue you’re just pulling things from. But the task deadlines combined with some forward planning are very effective at exposing things you can’t possibly get done so you can re-plan. And having the weekly plan on your desk is such a procrastination killer! You can see exactly what you’re sacrificing at any moment.\nLearning to type\nSomewhat embarrassingly, up until mid 2018 I was a 4 - 6 finger typist. I wasn’t exactly slow, but my style was inefficient, and error prone. I also had habit of looking down at the keys, although I could type without doing this if I forced myself. That’s what comes from learning to type ‘organically’ while wasting a youth on computer games and internet forums.\nAt some point during this year I had the insight that typos were this persistent niggling source of distraction. It’s as if each one created a tiny frustration that got added to a buffer. At some point my typo frustration buffer would overflow and my brain would have to dump the thread of whatever I was working on. When that happened I would be wide open for the forces of procrastination.\nSo I decided to purchase a mechanical keyboard2 and spend 30 minutes an evening as often as possible on improving my typing. Intially I was using gtypist - a free command line tutor, and eventually I moved to typingclub, also free. I made the most dramatic progress with typingclub. It has some sweet analytics that can help you decide what to practice, so I would recommend going straight to that.\nAfter about 3 months of semi-regular practice I was doing around 60wpm with accuracy in high 90s, and the looking down habbit completely broken. Now I feel like my typing is better able to keep up with my train of thought, which has increased the speed at which I can draft and interate on text. It’s also changed the way I anticipate writing - the dread has dialed way down.\nDedicated Learning Time\nI’ve found it really hard when I’m under the pump to make time to learn new things. Even if I can see those things have the capacity to make me more productive on a project. It’s like I find learning too enjoyable to be work.\nMy response to this realisation was to dedicate the first 30 minutes of my work day to reading, working through examples, or watching videos I have been meaning to get to but have been getting knocked down in my queue by higher priority work.\nI started doing this in my lull after useR and I found the thought of getting to work to spend some time learning helped provide the motivation in the morning I was otherwise struggling for.\nSince doing this I have heard other people talking about this kind of idea. This week Jessie Roberts described to me a similar plan to dedicate 30min each day to things that are ‘important but not urgent’. I like that way of framing it. The forward planning from Lucy’s method seems like exactly this.\nThe Viminning\nNow this is where things get qutie nerdy. My new confidence with typing kicked off an interest in keyboard shortcuts which blossomed into a fully fledged obsession with trying to control as much of my computer as possible with the keyboard.\nThe frustrating thing you’ll quickly find if you head down this path is that a lot of the applications you use will have their own unique keyboard shortcut conventions. There’s just too much to memorise. So over the past year I’ve been gradually replacing or configuring my applications so they all use a single control scheme - and that control scheme is inspired by vim.\nI think I’d better do a separate post on the rationale and the details, but here’s a tour:\nI control my desktop and its windows using a tiling window manager called i3. Window arrangement uses vim inspired keys (h,j,k,l).\nI write code using Spacemacs, which among other things, places a vim control scheme over Emacs (and ESS for R).\nI control my Chromium browser using an extension called ‘Vimium C’ that implements an amazingly faithful vim control scheme for a browser.3\nI launch applications with a launcher called rofi that uses a vim navigation scheme.\nI use a terminal file browser called ranger that has vim navigation and command scheme.\nSo what’s it like to have one unified(ish) keyboard control scheme for all your work tools? It feels fast and instinctive. I feel frustrated by a regular desktop setup now - like I can’t wait to get back to my vimland. Everything is sluggish and clunky by comparison 4.\nI don’t think this path is for everyone, but I do think everyone can benefit from reexamining the painful UIs in their life that they’ve normalised. Frustration and tension make us tired, susceptible to distraction, and give up prematurely. Giving up murders good ideas.\nMy tools: Spacemacs, Chromium, Ranger, and rofi in i3 window managerHello 2019 - Sorry I’m late\nThis year my resolution for my work is for less crunchy, more smooth. I’m changing jobs at the beginning of February and I hope to bring some of the planning tools I learned from Lucy in from the start, so that I never reach the peak workloads of 2018. Consequently I am looking forward to more worry free weekends with the family!\nAlso, I don’t think I am quite done with this ‘make everything vim’ idea. :P\nYour mileage with productivity techniques mentioned here may vary, I am far, far from a guru.\nHeader image is CC0 from https://pixabay.com/en/corn-flakes-corn-breakfast-food-1915632/\n\nProspective breeders beware. Here’s the harshest realisation I’ve had since becomming a parent: No matter what kind of night you’ve had, the kid is waking up EARLY and FULL OF BEANS↩︎\nThe Magicfore 68 with blue switches to be precise. E.g. this. Mine came with some dodgey soldering that made 2 keys unreliable. I had the gear to fix that so it was not much hassle, and the Ebay seller even knocked 10USD for my labour - I really like it, so great value!↩︎\nI made a fallback mode in i3 where I can operate my mouse pointer using vim keys for websites that force the issue - yes I am aware how crazy this sounds.↩︎\nThis might partly be due to dropping down the key repeat time for key presses - this is worth trying, trust me.↩︎\n",
    "preview": "posts/2018-in-pursuit-of-productivity/corn-flakes-1915632_1280.jpg",
    "last_modified": "2024-03-11T22:14:14+10:00",
    "input_file": {}
  },
  {
    "path": "posts/solving-the-challenge-of-tidyeval/",
    "title": "Solving the Challenge of Tidyeval",
    "description": "Ok here's the vignette... scroll scroll scroll... now I just quo... no wait enquo, no wait sym? Ahhh! equos that has to be it...",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-05-23",
    "categories": [
      "rstats",
      "tidyverse",
      "tidyeval"
    ],
    "contents": "\n\n\n\n\nThis dplyr code sure is working great! Now to wrap it up in a nice function to keep things clean and modular… oh fuck! The arguments aren’t interpreted correctly… that’s right. That tidyeval business. Let’s google ‘programming with dplyr’ - Ok here’s the vignette… scroll scroll scroll… now I just quo… no wait enquo, no wait sym? Ahhh! equos that has to be it… shit shit shit! Why is this not working?! Okay okay, I’ll read it top to bottom AGAIN. ‘Quote’? ‘Closure’? ‘Quosure’? ARGH I don’t care, tell me how to MAKE THIS WORK!\n– Me, Every 3 months.\n\nIt’s well known that the R community at large finds tidyeval challenging. To get a feel for the sentiment just check out the tidyeval tag on the RStudio Community Forum. In this post I discuss why tidyeval is challenging, and what we might be able do about it.\nNot your average tidyverse package\nAn overarching theme in R’s tidyverse is reducing the user’s cognitive load by creating functional APIs that align closely to the tasks the user needs to perform. The convention is to name functions using carefully chosen natural language verbs that allow the user to intuitively discover the function they need to manipulate their data using code completion mechanisms. There is no expectation that the user should read all the documentation up front before trying to use the API - indeed it’s common to see tidyverts tweeting or blogging joyously about new tidyverse functions they ‘just discovered’. 1\nrlang - the package that powers tidyeval - is a very different animal. It does not conform to these conventions. This pretty much sums up the problem:\nlibrary(purrr)\nlibrary(magrittr)\nlibrary(rlang)\n\nrlang_namespace <- ls(\"package:rlang\")\nlength(rlang_namespace)\n# [1] 429\n\nkeep(rlang_namespace, ~nchar(.x) <= 7)\n#  [1] \":=\"      \"!!\"      \"!!!\"     \"%@%\"     \"%|%\"     \"%||%\"    \"abort\"  \n#  [8] \"are_na\"  \"as_box\"  \"as_env\"  \"as_list\" \"bytes\"   \"call_fn\" \"call2\"  \n# [15] \"chr\"     \"chr_len\" \"cnd\"     \"cpl\"     \"cpl_len\" \"dbl\"     \"dbl_len\"\n# [22] \"dots_n\"  \"enexpr\"  \"enexprs\" \"enquo\"   \"enquos\"  \"ensym\"   \"ensyms\" \n# [29] \"env\"     \"env_get\" \"env_has\" \"exiting\" \"expr\"    \"exprs\"   \"f_env\"  \n# [36] \"f_env<-\" \"f_label\" \"f_lhs\"   \"f_lhs<-\" \"f_name\"  \"f_rhs\"   \"f_rhs<-\"\n# [43] \"f_text\"  \"flatten\" \"fn_body\" \"fn_env\"  \"fn_fmls\" \"get_env\" \"inform\" \n# [50] \"inplace\" \"int\"     \"int_len\" \"invoke\"  \"is_box\"  \"is_call\" \"is_env\" \n# [57] \"is_expr\" \"is_lang\" \"is_list\" \"is_na\"   \"is_node\" \"is_null\" \"is_raw\" \n# [64] \"is_true\" \"lang\"    \"lang_fn\" \"lgl\"     \"lgl_len\" \"list2\"   \"ll\"     \n# [71] \"locally\" \"modify\"  \"na_chr\"  \"na_cpl\"  \"na_dbl\"  \"na_int\"  \"na_lgl\" \n# [78] \"names2\"  \"new_box\" \"new_cnd\" \"new_raw\" \"node\"    \"ns_env\"  \"pkg_env\"\n# [85] \"prepend\" \"qq_show\" \"quo\"     \"quos\"    \"raw_len\" \"seq2\"    \"set_env\"\n# [92] \"splice\"  \"squash\"  \"string\"  \"sym\"     \"syms\"    \"type_of\" \"unbox\"  \n# [99] \"UQ\"      \"UQE\"     \"UQS\"     \"warn\" \nThere are a whopping 429 objects/functions in the rlang namespace! Around quarter of which favour this terse, abbreviated style of function naming. Now a good chunk of the functions do use classic tidyverse verb_object style, however if you read through the Programming With dplyr vignette you’ll find all the tidyeval functions presented, bar one, come from that list above.\nI suspect the reason for this minimalism is that the user has to nest rlang functions and operators within their classic dplyr code to use them. %>% can’t be used to help keep things readable. So rlang seems to have been designed to fade into the background as much as possible.\nThis results in a user experience that is quite apart from your average tidyverse package. The user almost certainly needs to consult the documentation upfront to identify the abbreviation that needs to be invoked in this huge namespace.\nNot for your average tidyverse user\nThe problem is it’s not as simple as go to the documentation -> find the function -> get the job done. A lot of space in ‘Programming with dplyr’ and other communications on the topic is dedicated to trying to explain the concepts of metaprogramming. This raises the question of who is tidyeval really for?\nDoes your average data scientist looking to make their code more readable or accessible by building functions, really need to grok metaprogramming concepts like quoting, clousures, quosures, environments, abstract syntax trees, and overscoping? These terms refer to computer science literature, and specifically the LISP family of languages. They have historical significance, but I would argue they are not particularly great names for the concepts they embody. ‘quote’ especially, because it clashes with the process of writing a literal character vector, i.e. 'this' - something most tidyverse users do every day.\nThe choice to frame tidyeval in computer science terms presents a big problem: it means to onboard your average tidyverse-using data scientist, you have to teach them computer science. This supposes they have both the time and desire to be taught computer science/metaprogramming when all they really wanted to do was put their code in a function! I think this is the source of a lot of the bewilderment and frustration with the approach taken in Programming With dplyr: Users just don’t expect to need a computer science lecture to get this type of task done.2\nMaking tidyeval more tidyverse\nLet’s keep in mind: It is design decisions taken in dplyr and ggplot2 that have driven the need for tidyeval3. It is not at all given that a data scientist needs to take on metaprogramming concepts to write a function that encapsulates manipulating or plotting tidy data.\nSo the question I am considering is: Can we bend this powerful and feature rich metaprogramming framework provided by rlang in such a way, so that an API more empathic toward the majority of tidyverse users emerges? Is there a flavour of tidyeval that can feel like it was designed for me, like the rest of the tidyverse?\nMy hypothesis is that this should be possible to do, using rlang as a platform, without writing much code at all! I am not here to tell you I have solved it, but I have made a start on a more tidyverse tidyeval.\nI’ve written a package called friendlyeval that reduces the namespace of rlang down to 5 functions and 3 operators. The Programming With dplyr vignette has been distilled down to about a quarter of the length, and rewritten using task-focused language that tries to reference the data science context. You won’t find any mention of quosures, environments, quoting, or abstract syntax trees etc.\nThe ‘killer’ feature is that friendlyeval code can be automatically converted to rlang code using an included RStudio addin. Importantly this means:\n1. You don’t have to have production code depend on something not maintained RStudio developers.\n2. Your friends don’t have to know you took the easy route.\nHelp Wanted\nI’m really keen for collaborators on this ‘compatibility layer’ idea. Naming things is hard! And that’s pretty much the central challenge of this whole thing. I haven’t even considered ggplot2 and I feel the chance of me getting it right on my own is pretty low. Feedback is going to give this idea legs, so please throw ideas at me over in the GitHub issues. Thanks for reading!\nHeader Image credit:\nBy Robson#, CC BY 2.0, https://www.flickr.com/photos/robson/9972796593/in/photostream/\n\nJust searching ‘#tidyverse discovered’ on Twitter is a thing to behold.↩︎\nAs it happens, my bachelor’s degree was in computer science and I still find tidyeval bewildering.↩︎\nAlthough I think it might have been the problems with naming plot elements in ggvis that really started us down this path.↩︎\n",
    "preview": "posts/solving-the-challenge-of-tidyeval/9972796593_14695448e7_b.jpg",
    "last_modified": "2024-03-11T22:24:43+10:00",
    "input_file": {}
  },
  {
    "path": "posts/how-to-survive-a-laptop-fire/",
    "title": "How to Survive a Laptop Fire",
    "description": "How bad would it be for your work if your laptop burned in a fire?",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-05-20",
    "categories": [
      "software development"
    ],
    "contents": "\n\n\n\nOver the last few years I’ve evolved a cloud-centric workflow that allows me to work more or less seamlessly across multiple projects, computers, and operating systems. This post is about the principles, software and services I use to make that work.\nOften when I’m advocating evangelising about working in this way I ask people something to the effect of ‘How bad would it be for your work if your laptop burned in a fire?’1 - Alot of the responses I get tend to focus on personal data - ‘Well I do backup on Dropbox!’, ‘I use Googledrive’, etc. But I argue personal data loss is really just part of the pain you’d experience. What about your Applications? Software libraries? Just how hard would it be to recover a working environment that would allow you to carry on?2\nSurvival skills\nUse free software where possible\nApart from the obvious benefit of not costing anything, free software has the added advantage that you’ll always be able to access it. It’s pretty common to get access to proprietary stuff through one’s workplace, but I don’t enjoy the idea that I could lose access to a tool I depend on if I changed jobs or budget cut was handed down.\nAnother great benefit of free software is others can freely reuse and reproduce your work if you release it publicly.\nUse cloud tools\nThe only local software I absolutely need is Chromium or Chrome. This gives me quite a lot of freedom in terms of devices I can do work from. This is because:\n* Lastpass manages all my passwords\n* I use Outlook/Gmail web interfaces for work/personal email\n* I use Goodle Docs for all simple documents, sheets, or quick slide decks\n* I store all my photos in Google Photos\n* I stream my music from Google Music\n* I store all my data and files in Google Drive\n* I host all my projects on GitHub.\n* I blog from a web interface hosted on this site.\n* I host RStudio server on my website - really handy when giving talks!\nOfcourse I don’t only use cloud tools. I develop software and analyse data on my local machine since it costs less than hosting a powerful enough server.\nKeep Local file system disposable\nEverything on my local hard-disk is disposable. Indeed this month I am on my third clean install of Linux - but that’s a story for another post. Keeping the system disposable is closely linked with keeping it simple.\nTo keep things simple, every time I get a new system I create two folders:\n1. ~/repos that holds the cloned git repositories of all the projects I am working on.\n2. ~/code that holds random code I create that is not associated with a project. This is strictly limited to disposable things, like examples or exercises. If a code snippet becomes useful, I make it into a GitHub gist.\nI use the standard folders like ~/Downloads, ~/Images etc, but again everything in them is disposable.\nSimplicity is also ensured by only using software that can be installed using a package manager. This means zero manual configuration for most of my applications. There are a handful of helpful bash scripts hosted on GitHub that I clone to ~/bin. I have configuration for my desktop setup and my text editor hosted on GitHub as well.\nSo getting a new system ready to be productive is a case of installing some software using the package manager and cloning a few GitHub repos. In theory this could done with a single command, but I am not that hardcore - yet!\nUse GitHub for all projects\nGitHub is a place to host version controlled projects. It’s not the only choice, other popular choices are GitLab and BitBucket. The main reason to choose GitHub is that everyone else is there, so it’s where collaboration happens.\nI use ‘project’ pretty loosely. Things I keep there inlcude:\n* Data analysis documents\n* Software projects\n* Slides for talks (when using markdown)\n* Learning/Study repositories of notes and examples.\nThere’s this kind of ritual I go through now when I decide to do some new work where I create and name the GitHub repo. It’s become a habit to the point where not doing this creates mild anxiety.\nOne point I should make: I commit my work frequently in small increments. In a regular work day I’ll push to Github a couple of times. This is important for the ‘version control’ aspect, which although not strictly required for laptop fire survival, is pretty damn handy.\nKeep data in the cloud\nQuite often a project will have an associated dataset. The decision about where to store this usually comes down to size. For datasets up to tens of Mb I usually store them in the GitHub repo. I store larger data in my Google Drive (I have a large amount of drive storage that came with a Chromebook I bough a few years ago).\nThere is a /repodata folder in my Google Drive that contains folders that match the names of GitHub Projects. For GitHub projects that use this datastore, I check in a script, usually called fetch_data.R that pulls the data down to the appropriate place in the local repo. I have created some tooling around this to recursively download Google Drive folders and even generate the fetch_data script for me from a Google Drive URL link. This is cool, but could be slicker still. My plan is to eventually move this stuff out of my personal mmmisc R package and into a proper package of it’s own.\nAssociated Pros and Cons\n+ Work from anything anywhere\nOccasionally when stuck without a computer I have been able to get critical tasks done with makeshift workstations. I once used RStudio server on my phone to knit an Rmd file from hospital waiting room and felt like a total badass.\n+ Collaborate easily\nHaving everything online has been amazing for opening up collaboration avenues. I can always immediately provide someone with a link to anything I’ve produced. Sometimes down to a specific line of code that implements the subject of discussion. Since I work in public GitHub repositories as much as possible, I occasionally get interest from interesting people I would never have encountered otherwise.\n- Dependent on services staying up\nEvery once in a blue moon GitHub, or Google, or something else big goes down briefly. At least a lot of other people are hurting when this happens - so I tend to jump on slack on and ride it out in the company of friends, like it were a cyclone. The inconvenience of this has only been ever been minor.\n- Dependent on internet access\nA big glaring flaw with this setup is that while I’m immune to fire, my kryptonite is now lack of internet access. This does mean I feel the anxiety produced by crappy conference/hotel wifi particularly hard! The last time I traveled (In the US) I purchased a sim card with a bunch of data so I could work in uninterrupted in my usual way, hot-spotting to my phone. That was a success.\n- Security risk\nThe other thing that may be giving some of you hives is the security risk of having your life’s work online. I take this pretty seriously. I have Lastpass generate long random passwords and I use Two-Factor Authentication (2FA) for my Google, Lastpass, and GitHub accounts. In the case of Lastpass and GitHub, this is a physical Yubikey. My phone provides 2FA for Google so I have to keep that secure as well.\nOverall\nClearly I find on the balance, doing things this way is worthwile. It meshes nicely with other values I hold relating to openess and reproducibilty. Having a simple system also frees me from regular worry at the expense of ocassional anxiety caused by things listed as cons.\nThanks for reading! I’d love to hear about any tips or hacks you have for making working this way easier.\nHeader Image Credit:\nBy secumem - secumem, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=5878118\n\nOne important way to avoid this is to stop hardcoding your local file paths in your code. Just ask Jenny Bryan.↩︎\nI once witnessed an amazing example of this at rOpenSci Ozunconf16. As keystrokes were flying in some intensise pair programming, a fresh flat white coffee was emptied on to Jeff Hanson’s laptop. I swear without missing a beat he flipped the laptop upside down to drain, then immediately ignored it, moved to his buddy’s laptop, pulled the repo, and carried on typing. When I think about being robust to loss of computer, Jeff’s complete lack of concern is the bar to which I aspire.↩︎\n",
    "preview": "posts/how-to-survive-a-laptop-fire/Burned_laptop_secumem_11.jpg",
    "last_modified": "2024-03-11T22:20:22+10:00",
    "input_file": {}
  },
  {
    "path": "posts/r2v1-meshing-uluru-from-polygons-and-rasters/",
    "title": "R2VR: Meshing Uluru From Polygons and Rasters",
    "description": "In this post I describe using R to make a 3D mesh of Uluru from publicly available spatial data.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-27",
    "categories": [
      "rstats",
      "WebVR",
      "R2VR",
      "GIS"
    ],
    "contents": "\n\n\n\nIntroduction\nIn this post I describe using R to make a 3D mesh of Uluru from publicly available spatial data. In a future post I will detail how to place 3D meshes of this form in a WebVR scene. The types of data you can use, transformations you need to perform, and issues that require consideration are discussed.\nThis is intended to be a example you can adapt for you own purposes. All code and data in this post is available in this GitHub repository.\nThe end goal of this post is a triangulated mesh of x,y,z vertices:\n\n\n\nFigure 1: A preview of an Uluru mesh created with rgl\n\n\n\nEach triangular section forms a ‘face’ that can be shaded with interesting colours or textures, perhaps derived from data. In the world of 3D graphics meshes are built from either quadrilaterals or triangles, but since I am targeting WebVR in particular, the choice is made for me: The underlying three.js framework only uses triangles.\nData\nTypically you will have some combination of shapefiles and raster data available. Shapefiles can describe 2D outlines of features as well as their 3D topology via stacked outlines - aka contours. Raster data are often a more useful for adding the third spatial dimension, since they are a common format for Digital Elevation Models (DEMs). I’ve found these tend to provide elevation information at finer resolution.\nFor this example I’ll use:\n* Shapefiles containing contours for Uluru, provided by Geoscience Australia, from data.gov.au\n* A DEM raster image, exported from Geoscience Australia’s Elevation Information System (ELVIS).\nThe sf and raster packages provide the ability to read and manipulate these spatial data. For an introduction to sf I highly recommend the blog of Jesse Sadler.\nShapes\nThe contour shapefiles are distributed as part of a GIS dataset containing a dizzying array of features of the Uluru-Kata Tjuta National Park.\nContours can give us elevation, but in this example I am just using them to get an outline. For reasons that will soon become clear, in this case this turns out to be a slightly flawed approach, however I am confident it is worthwhile examining the method for its applicability to meshing complex arbitrary shapes.\nI start by loading the shapefiles into R using the sf package, filtering the contour data down to Uluru using a bounding box, before finally finding the outer-most contour. This gives me a 2D outline I can project into 3D.\nlibrary(sf)\nlibrary(raster)\nlibrary(tidyverse)\n\n# Contours from all over Uluru-Kata Tjuta National Park                    \nuluru_kata_shape <- \nread_sf(\"./data/GEODATA_TOPO250K_TILE_DATA/G5208/Shapefiles/Elevation/g5208_contours.shx\")\n\n# Make a bounding box to focus in on Uluru\n# mymaps.google.com is a handy tool for determining bounding boxes.\nuluru_bbox_poly <- \n  st_bbox(c(ymin = -25.39521, \n            ymax = -25.2922, \n            xmax = 131.09814, \n            xmin = 130.97454), \n            crs = st_crs(\"+proj=longlat +ellps=WGS84\")) %>%\n    st_as_sfc() \n    # An sf bbox is not geometry... this converts it :S\n\n# Filter map countours to those comprising Uluru\nuluru_contours <- \n    filter(uluru_kata_shape, \n           as.vector(\n             st_contains(uluru_bbox_poly, \n                        uluru_kata_shape$geometry, \n                        sparse = FALSE)\n           ))\n\n# Find the outer contour by finding the one with the highest longitude\nuluru_outline <- \n  uluru_contours %>%\n  mutate(max_y = map_dbl(geometry,\n                         ~max(as.matrix(.)[,1]) )) %>% \n  arrange(desc(max_y)) %>% \n  filter(row_number() == 1)\n\n  plot(uluru_contours[\"FEATTYPE\"], \n    main = \"Uluru Elevation Contours\")\n  plot(uluru_outline[\"FEATTYPE\"], \n    main = \"Outline Contour\")\n\n\n\nFigure 2: Plot otuputs of contours and outline\n\n\n\nWe’re nearly done here, but for one small issue. Looking closely at uluru_outline I found:\nr$> uluru_outline$geometry\n\nGeometry set for 1 feature\ngeometry type:  MULTILINESTRING\ndimension:      XY\nbbox:           xmin: 131.0233 ymin: -25.353 xmax: 131.0512 ymax: -25.33646\nepsg (SRID):    NA\nproj4string:    +proj=longlat +ellps=GRS80 +no_defs\nMULTILINESTRING ((131.0233 -25.34388, 131.0233 ...\nWe have a MULTILINESTRING geometry which as the name suggests is a collection of lines, rather than a shape. There’s not a sensible way to interpret a set of lines as the basis for a triangular mesh - unless those lines form a closed ring, in which case they can be recast as a polygon.\nThe raw data show the points on the lines do indeed start and end in the same position, forming a closed ring:\nr$> as.matrix(uluru_outline$geometry[[1]])\n\n           [,1]      [,2]\n  [1,] 131.0233 -25.34388\n  [2,] 131.0233 -25.34345\n  [3,] 131.0234 -25.34324\n  ...\n  [97,] 131.0242 -25.34556\n  [98,] 131.0241 -25.34524\n  [99,] 131.0233 -25.34388\nSo you’d think there would be a nice simple way to cast from MULTLINESTRING to POLYGON, but unfortunately at the time of writing this is the best I could do:\nuluru_outline_poly <- \n  uluru_outline %>%\n  st_geometry() %>%\n  pluck(1) %>%\n  as.matrix() %>%\n  list() %>%\n  list() %>%\n  st_multipolygon() %>%\n  st_sfc()\n\n# When we create a new geometry we need to set CRS. \nst_crs(uluru_outline_poly) <- st_crs(uluru_contours)\nNotice I chose MULTIPOLYGON over POLYGON. This simple convenience, because I have a triangulation function I will apply later that expects a MULTIPOLYGON argument. The result:\nr$> uluru_outline_poly\n\nGeometry set for 1 feature\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: 131.0233 ymin: -25.353 xmax: 131.0512 ym\nax: -25.33646\nepsg (SRID):    NA\nproj4string:    +proj=longlat +ellps=GRS80 +no_defs\nMULTIPOLYGON (((131.0233 -25.34388, 131.0233 -2...\nRasters\nThe method raster::raster() reliably reads raster data in a variety of formats. NetCDF, GeoTIFF, plain images, etc are all handled without fuss. After reading an elevation raster in, I decided it would be a good idea to check the Uluru shape overlaid it sensibly using a plot.\nBefore I can make the plot in the code below, I have to convert (reproject) the shape to the same coordinate reference system (CRS) as the raster. A shape is usually much less data than a raster, and can be reprojected losslessly whereas a raster relies on interpolation methods. For these reasons reprojecting the shape is preferable to the raster.\nnt_raster <- raster(\"./data/ELVIS_CLIP.tif\")\n\n# sanity check - do these overlap?\n# convert uluru_outline_poly to raster CRS\n\nuluru_outline_poly <-\n  st_transform(uluru_outline_poly, crs(nt_raster)@projargs)\n\n# plot and overlap\nplot(nt_raster, main=\"Uluru outline over elevation raster\")\nlines(as(uluru_outline_poly, 'Spatial'))\n\n\n\nFigure 3: My outline fit is far from perfect!\n\n\n\nIn this case the fit doesn’t look perfect and we’ll see the result of that below. Perhaps I’ve made some critical mistake, but my current thinking is this is part of life when combining independent spatial data sets. A position in space cannot be measured without error! Perhaps a GIS pro would undertake some kind of alignment/registration step at this point. I’d love to hear about an automated way to do this in R.\nTriangulating A Shape\nOnce you have an outline of a shape, and some elevation data, the next step is to triangulate the shape - cover it with small triangles. The vertices of the triangles will have coordinates in 2 dimensions, a third elevation dimension can then be added to create the mesh.\nThere are a variety of fast algorithms available for triangulating polygons but not all are suitable for creating meshes. Keeping in mind our end goal of shading the mesh using data - we need an algorithm that can deliver a set of triangles of a similar size and shape.\nPerhaps the most famous algorithm is Delaunay triangulation which maximises the minimum interior angle of all triangles. This is great because it avoids degenerate ‘slivers’ with tiny width. But in its simplest form Delaunay doesn’t care about creating triangles of a regular size. Most critically, it also doesn’t fully respect the polygon outline since it works over the convex hull of points. You can experiment with plain Delaunay in sf since it is available using st_triangulate().\nInstead I’ll use the Constrained Delaunay Triangulation algorithm provided in the RTriangle package. Using this approach I can specify additional constraints including:\n* Line segments that must be included as part of the border, interior detail, and interior holes.\n* The maximum size of triangles.\nI have written a convenient wrapper, sf_to_trimesh() for RTriangle that works with sf geometry containing a single MULTIPOLYGON. If you have an sf geometry containing many polygons you can merge them into a multipolygon, using st_union() or st_combine() - see this example.\nSomething to take into consideration at this point is the CRS of your shape. If it is in latitude and longitude, your flat mesh will be expressed in lat-lon as well. Usually elevation data will be expressed in meters so the difference of scale is problematic. I already converted the CRS of the uluru_outline_poly to the CRS of the raster to check for overlay fit above.\nsf_to_trimesh can be supplied with a number of triangles, n_tris. This is the number of triangles that you wish to fit inside the bounding box of the shape - not the shape itself. It exists to help you determine a reasonable density of triangles. With respect to triangle density - go for thousands! The resolution needs to be fine-grained enough for the elevation features to be discernible and shading applied later on to reflect the data source.\nuluru_trimesh <- sf_to_tri_mesh(uluru_outline_poly, 2500)\nplot(uluru_trimesh)\n\n\n\nFigure 4: A flat triangular mesh appears\n\n\n\nLooking deeper into the data returned by the triangulation we have:\nr$> str(uluru_trimesh)\n\nList of 12\n $ P : num [1:1199, 1:2] -398761 -398762 -398747 -398653 -398605 ...\n $ PB: int [1:1199, 1] 1 1 1 1 1 1 1 1 1 1 ...\n $ PA: num[1:1199, 0 ]\n $ T : int [1:2275, 1:3] 672 526 145 135 468 283 688 83 318 1153 ...\n $ S : int [1:121, 1:2] 1 2 4 4 5 6 8 9 9 10 ...\n $ SB: int [1:121, 1] 1 1 1 1 1 1 1 1 1 1 ...\n $ E : int [1:3473, 1:2] 672 653 656 526 445 64 145 285 284 135 ...\n $ EB: int [1:3473, 1] 0 0 0 0 0 0 0 0 0 0 ...\n $ VP: num [1:2275, 1:2] -397618 -397377 -398090 -398248 -398054 ...\n $ VE: int [1:3473, 1:2] 1 1 1 2 2 2 3 3 3 4 ...\n $ VN: num [1:2275, \nOur main interest is in P: a 2 column matrix of unique vertices, and T: a 3 column matrix of vertex indices in P. Each row in T represents a triangle by 3 vertex indices. This is an important concept when moving to VR, so I’ll try to make it concrete for you with some code that obtains the 3 vertices of the first triangle in T:\nr$> first_tri_vertex_indices <- uluru_trimesh$T[1,]\n\nr$> first_tri_vertex_indices\n\n[1] 672 653 656\n\nr$> first_tri_vertices <- \n  uluru_trimesh$P[first_tri_vertex_indices,]\n\nr$> first_tri_vertices\n\n          [,1]      [,2]\n[1,] -397651.6 -43319.01\n[2,] -397589.4 -43325.02\n[3,] -397623.9 -43263.00\nThis kind of idea is quite prevalent in 3D graphics. Textures and colours are described in similar ways, i.e. with indices into unique data arrays. More information on the other data can be found in the the RTriangle documentation.\nAdding Raster Elevation\nThe vertices in P can be promoted to 3 dimensions by adding a third column to the matrix that represents the elevation. To do this the elevation is extracted from the raster for each vertex:\nuluru_elev <- \n  raster::extract(nt_raster, uluru_trimesh$P[,1:2])\nuluru_trimesh$P <- \n  cbind(uluru_trimesh$P, uluru_elev)\nI used rgl to preview my handiwork by passing P and T reshaped slightly to suit rgl:\nlibrary(rgl)\nrgl.clear()\nbg3d(\"white\")\n  wire3d(\n    tmesh3d(\n     vertices = t(asHomogeneous(uluru_trimesh$P)), \n     indices = array(t(uluru_trimesh$T))\n    )\n  )\nrglwidget()\n\n\n\nFigure 5: Backside of the mesh from the introduction\n\n\n\nWhen run locally, you’ll get live 3D render of the mesh you can rotate and zoom. Unfortunately the backside of the mesh is not nearly as tidy as the front. It seems as though it doesn’t drop down to the same level. This should be a certainty, given the outline contour was originally intended to represent structure on the same elevation. The sub-perfect alignment of the data sets really shows here.\nDepending on usecase this could be a problem. I give an alternative for cases like this below.\nTriangulating A Raster\nThe workflow I’ve explored was predicated on the need to create a mesh from a specific outline shape. But in some cases there is no specific shape:\n* You have a large geography you want to place someone in, inside VR - A VR ‘tile’.\n* You had a specific shape and it didn’t work out 😉 .\nA simple approach to using elevation raster data with no specific shape is:\nCreate a spatial bounding box around the area to be meshed\nTriangulate the bounding box\nAdd elevation to the triangles.\nsource(\"./helpers/bbox_to_multipoly.R\")\nuluru_contours_bbox <- \n  st_bbox(uluru_outline_poly)\nuluru_extent <- \n  bbox_to_multipoly(uluru_contours_bbox, \n                    st_crs(uluru_outline_poly))\nul_extent_trimesh <- \n  sf_to_trimesh(uluru_extent, 2500)\nul_extent_elev <- \n  raster::extract(nt_raster, ul_extent_trimesh$P[,1:2])\nul_extent_trimesh$P <- \n  cbind(ul_extent_trimesh$P, ul_extent_elev)\n\nrgl.clear()\nbg3d(\"white\")\n  wire3d(\n    tmesh3d(vertices = t(asHomogeneous(ul_extent_trimesh$P)), \n            indices = array(t(ul_extent_trimesh$T))\n    )\n  )\nrglwidget()  \n\n\n\nFigure 6: An Alternative Mesh of Uluru\n\n\n\nIn this view the Uluru is less recognisable. Applying materials and shading could make it much more life-like.\nAlternatives (watch this space)\nSenior wizards in the R Spatial community are currently working toward a generalised form for high dimensional spatial data for use with plugable tools ala the tidy tibble and the tidyverse. There are a couple of projects of interest that can simplify the kind of work demonstrated here.\nsilicate provides the underlying data model with lots of useful converters from sf and other spatial forms.\nanglr builds on top of silicate to provide a streamlined workflow for creating 3D visualisations using rgl. It’s worth checking out the README example.\nMy initial forays into this space used these tools, but at the moment they are not stable enough to be included in a guide like this. Definitely worth keeping an eye on.\nUp Next\nIn my next post in this series I’ll take you through what it takes to push meshes of this type into VR. Hint: Not much. The mesh form returned by sf_to_trimesh() is quite close to what is needed there.\nWork described here is part of a project funded by ACEMS in partnership with CSIRO Data 61, at Queensland University of Technology. Specialist support has been provided in-kind by the Australian Antarctic Division.\n\n\n\nHeader image credit: By Maurus Blank - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=1177350\n\n\n\n",
    "preview": "posts/r2v1-meshing-uluru-from-polygons-and-rasters/1200px-Uluru1_2003-11-21.jpg",
    "last_modified": "2024-03-11T22:20:52+10:00",
    "input_file": {}
  },
  {
    "path": "posts/r2vr/",
    "title": "Introducing #R2VR",
    "description": {},
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-27",
    "categories": [
      "rstats",
      "R2VR",
      "WebVR",
      "GIS"
    ],
    "contents": "\nThis post marks the start of series in which I will describe methods I have developed to make GIS data explorable in Virtual Reality. The overarching strategy is to use R’s spatial ecosystem to generate the 3D data, and then render it in online VR environments created with A-Frame WebVR. To make sense of the posts you’ll need an intermediate understanding of R, and basic familiarity with HTML for the A-Frame bits.\nThe ideas expressed here are currently being wrapped up into a yet to be named R package. These posts are as much about getting the details straight in my own head, as they are to provide some documentation to my new collaborators.\nThe posts will break down along these lines:\nMaking Meshes from Polygons and Rasters\nRendering Meshes in WebVR\nShading Meshes using GIS layers\nAnnotating Meshes with WebVR elements\nServing Multi-User WebVR Scenes\nPlease consider these as all as RFCs. I am new to spatial, 3D graphics, and web programming. I would love feedback from my betters in these fields. In that regard I would like to thank Michael Sumner (Master Spatial Artificer) for his ideas and enthusiasm so far.\nFinally, an illustrative example. Given some raster data like this:\n\n\n\nFigure 1: A vegetation index raster\n\n\n\nWe can arrive at this:\n\n\n\nFigure 2: Elevation, vegetation, and water in WebVR\n\n\n\nAnd that’s just for starters! We should be able to have some fun with this. Thanks for following along.\nWork described here is part of a project funded by ACEMS in partnership with CSIRO Data 61, at Queensland University of Technology. Specialist support has been provided in-kind by the Australian Antarctic Division.\n\n\n\n\n\n\n",
    "preview": "posts/r2vr/acp_vegetation.png",
    "last_modified": "2024-03-11T22:21:47+10:00",
    "input_file": {},
    "preview_width": 640,
    "preview_height": 640
  },
  {
    "path": "posts/ropensci-onboarding2/",
    "title": "Where is the value in package peer review? Reflection #2 on rOpenSci Onboarding",
    "description": "How is a package peer reviewer’s time best spent? When is the best time in a software package’s life cycle to undertake peer review? A user-driven perspective.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-26",
    "categories": [
      "rstats",
      "rOpenSci",
      "open science"
    ],
    "contents": "\n\n\n\nIf you read my reflection #1 on rOpenSci Onboarding, then you know I see value in the Onboarding process. A LOT of value even. This post is about where that value lies.\nThis question has important corollaries which I will explore here based on my experience as a reviewer of bowerbird:\nHow is a package peer reviewer’s time best spent?\nWhen is the best time in a software package’s life cycle to undertake peer review?\nDoing a good job\nAs I’ve read the growing number of reflective posts by other Onboarding reviewers, I’m struck similarities in our experiences. Mara Averick, Verena Haunschmid, and Charles Gray all place emphasis on reviewing from a “user’s” perspective. For Mara and Charles, this perspective was used to gain traction after initial nervousness about their ability to do a good job.\nI shared these initial butterflies. bowerbird’s code base was quite large on the first submission. The code embedded a lot of domain specific API knowledge. It was disorientating and I felt like I was thrashing around stumbling onto issues without undertaking a systematic survey.\nProbably recalling Mara’s experience, I eventually honed in on the idea that to do a good job, all I had to do was identify things that would hold bowerbird back from being a widely-used and popular package. And the best way to identify these was from the “user’s” perspective.\nThat perspective implied the systematic process that I was seeking. So I started from the typical user entry points - README, vignettes, examples in help files. I went through them and tried to use the functions as suggested to download bulk repository data (bowerbird’s usecase). I had success! I uncovered quite a few idiosyncrasies in its API and documentation that created a bumpy ride for a new user. The authors were very good about acknowledging and fixing these.\nI did review code as well. But it was a natural part of exploring the behaviour as I followed the documented workflow thread. And If I think about the impact the more technical issues I found had on the package as a whole? It’s probably negligible. Sure you can argue the code is a bit more robust, a bit easier to maintain. But it would be hard to justify maintaining code that isn’t seeing any use!\nI wasn’t lucky enough to see Onboarding editor Maëlle Salmon’s recent hatted SatRday talk, but I had to smile when I spotted this slide in her deck:\n\n\n\nFigure 1: Yes indeed!\n\n\n\nIt seems we agree on the priorities. Maëlle gives technical considerations the lowest value.\nThe R community has repeatedly heard Hadley Wickham argue that reducing cognitive load is more important than cpu load in most analysis scenarios. Following that school of thinking, in a talk called “If You Build it They Won’t Come”, Hadley seems to be pushing package developers to focus on making their packages marketable. The user is very much at the forefront of his development ideology.\nIt is interesting that Onboarding reviewers like Mara, Charles, and myself all initially assumed a more technical code-driven approach was where the value in peer review came from. Should we be surprised such value arose from the user-driven approach? A user-driven review school seems a logical counterpart to the user-driven development school that has gained popularity in the R community.\nTo a software developer, detailed user feedback is an extremely rare and valuable commodity. It’s quite amazing to receive it from multiple sources for free. The realisation that this is where the bulk of the value lies has important implications for the scaling of package peer review processes: Veteran UseRs are in much more plentiful supply than veteran package developers.\nTiming it right\nArmed with my user-driven heuristic, it didn’t take me long pick up on some of editor Noam Ross initial queries and determine bowerbird needed to be split into two packages. The documentation and API struggled to paint separate coherent workflows for two very different classes of user.\nFollowing the split I think the authors began to feel some concern at the scale of the issues emerging in review. They worried aloud they were wasting our time and sought to engage reviewers and editors in a conversation about whether bowerbird had been submitted too early in its life cycle. Opinions were split on whether early or late stage review is better. I was very much in favour of early and did not see an issue with bowerbird being submitted as it was.\nI would argue that under the user-driven school, early has to be better. There is more opportunity to use feedback to inform the package API and workflow. Or stated another way: the value obtained from the peer review is maximised. The maximisation comes at the cost of editor/reviewer time, but so long as expectations and timelines are set appropriately I don’t see this as an obstacle.\nLeaving review until late stage allows for the possibility that work done in the name of open source and open science might be wasted to some degree. I see that as tragedy worth avoiding at significant cost.\nConclusion\nA reviewer’s time is best spent on the things most impactful on a software package’s success. This is an uncontroversial statement. However, the user-driven school of package development & review suggests the things that most impact package success are those that affect the end user - documentation, API, and workflow. It is therefore in improving these things that the bulk of the value in package peer review lies.\nFalling out of this discussion are two important take aways from the user-driven school of package peer review:\nAs a useR, you are qualified as a potential package reviewer if you have some appreciation for what makes your favourite packages useful.\nAs a package author, engage early with a package peer review process if you see value in it, to get the most out of it. This is BEFORE things are ‘perfect’.\nEpilogue\nbowerbird recently cleared the review process and obtained Peer Reviewed status. In my biased opinion, the main vignette is one of the best I have read, and a fine example product of the user-driven school I have discussed here.\nCongratulations to the authors Ben Raymond and Michael Sumner!\nHeader Image: A Satin Bowerbird, credit: benjamint444, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=1588854\n\n\n",
    "preview": "posts/ropensci-onboarding2/1200px-Satin_bowerbird.jpg",
    "last_modified": "2024-03-11T20:38:26+10:00",
    "input_file": {}
  },
  {
    "path": "posts/a-fully-dockerised-ghost-blog/",
    "title": "A Fully Dockerised HTTPS Ghost Blog",
    "description": "The installation documentation for Ghost is comprehensive, but beyond my skill as a system administrator to complete. Docker to the rescue!",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-08",
    "categories": [
      "ghost",
      "docker"
    ],
    "contents": "\n\nNote: This blog is no longer built with Ghost, I’ve gone static with Distill. These instructions should still serve you well.\n\nGhost is the blogging platform that powers this blog. You can pay for a hosted instance on https://ghost.org/, but as open source software, you can also self-host. The installation documentation for Ghost is comprehensive, but beyond my skill as a system administrator to complete. Luckily Docker allows hackers like me (and you!) to piggy back off the system admin skills of those far more knowledgeable.\nThis is the post I wish I had found when I spent a day hunting for the magic Docker image that would get me up and running with a self-hosted Ghost instance.\nPrerequisites\nThis is the stuff you need to have and be somewhat familiar with to self host Ghost the way I will describe.\nA domain for your blog. I use DNSimple to purchase and manage mine.\nA Virtual Private Cloud (VPC) provider. I recommend an instance with at least 2GB RAM, 1 VCPU, with a public static IP address. I use Vultr.\nComfort with administering a VPC instance over an ssh connection: Create folders, edit a file, run a couple of commands.\nSome conceptual familiarity with Docker. We’ll use very simple docker-compose commands. So you’ll need docker-compose and docker.io installed on your VPC instance.\nDocker Composition\nUsing the docker-compose framework we build a network/cluster of container hosts that together form a complete solution.\nThe composition has these parts: 1. Ghost, our blog platform. 2. Optionally RStudio Server. My blog relates to R, this comes in really handy for me. You can remove it if you want to. 3. An nginx reverse proxy server that automatically configures itself to manage ssl handshakes, upgrade http to https, and route connections to hosts we specify in the composition. 4. An SSL certificate service that automatically obtains ssl certificates for https from Let’s Encrypt for hosts we specify in the composition. These are automatically renewed.\ndocker-compose.yml\nThis is the configuration file that defines the composition above. I’m going to give you the whole thing here and then commentate the various sections in the rest of this post.\n\nversion: '2'\n\nservices:\n\n  ghost:\n    image: ghost:1.21.3-alpine\n    restart: always\n    environment:\n      NODE_ENV: production\n      url: https://milesmcbain.xyz\n      mail__transport: SMTP\n      mail__options__service: Mailgun\n      mail__options__auth__user: <My Mailgun User>\n      mail__options__auth__pass: <My Mailgun Password>\n      VIRTUAL_HOST: milesmcbain.xyz\n      LETSENCRYPT_HOST: milesmcbain.xyz\n      LETSENCRYPT_EMAIL: <My Email>\n    volumes:\n      - ~/data/ghost:/var/lib/ghost/content\n\n  rstudio:\n    image: rocker/tidyverse\n    restart: always\n    ports:\n      - \"8787:8787\"\n    environment:\n      USER: <My RStudio User>\n      PASSWORD: <My RStudio Password>\n      ROOT: \"TRUE\"\n      VIRTUAL_HOST: <My RStudio Domain>\n      LETSENCRYPT_HOST: <My RStudio Domain>\n      LETSENCRYPT_EMAIL: <My Email>\n    volumes:\n      - ~/repos:/home/milesmcbain/repos\n\n  nginx-proxy:\n    image: jwilder/nginx-proxy\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - \"/etc/nginx/vhost.d\"\n      - \"/usr/share/nginx/html\"\n      - \"/var/run/docker.sock:/tmp/docker.sock:ro\"\n      - \"/etc/nginx/certs\"\n\n  letsencrypt-nginx-proxy-companion:\n    image: jrcs/letsencrypt-nginx-proxy-companion\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n    volumes_from:\n      - \"nginx-proxy\nghost:\nThe image: version tag can be updated to the latest version on the docker repository.\nNODE_ENV: ‘production’ is strongly recommended by Ghost developers for a live blog.\nGhost configuration options listed in the Installation and Setup Guide are configured as environment: variables. Instead of JSON style nesting ,{}, nested options are delimited using __.\nThe combination of VIRTUAL_HOST, and LETSENCRYPT_HOST instructs the nginx-proxy and letsencrypt-nginx-proxy-companion to redirect http traffic for http://milesmcbain.xyz to https://milesmcbain.xyz and then on to the Ghost instance. This is important because configuring a https url: ensures Ghost will only accept https connections. I recommend this because administering a Ghost blog involves logging into to a web interface.\nSSL certificates required for https are automatically obtained and renewed from Let’s Encrypt using the LETSENCRYPT_HOST and LETSENCRYPT_EMAIL as parameters. That email address will be emailed if the certificate for the host is due to expire. I have not seen one of these yet, since the automation is working.\nmail__ configuration is optional. Setup with Mailgun is fairly painless, It’s described in the Ghost Installation and Setup Guide. There are two main mail usecases: Emailing subscribers about new posts (if you use a subscribe button) and allowing blog contributors to recover from forgotten passwords. If you’re a solo blogger like me and not using email subscriptions you probably don’t need it.\nvolumes: is important. It defines folders on your VPC host that are mapped to the instance. You absolutely need a folder on your VPC host to keep your blog content. Otherwise it will not persist when the Docker container is stopped and restarted. E.g. for VPC provider maintenance or a Ghost version upgrade. ~/data/ghost:/var/lib/ghost/content maps the folder /miles/data/ghost on my VPC file system to /var/lib/ghost/content in the Ghost container. miles is my user I created to administer the blog on the VPC. The data/ghost is in my home directory.\nrstudio:\nThe config in ports: maps port 8787 on my VPC to port 8787 in the rocker/tidyverse Docker container where RStudio is listening. Remember to open port 8787 on your VPC using your VPC management console.\nUSER, PASSWORD and ROOT relate to the an RStudio user that can log into the server. ROOT adds the user to sudoers - this is useful if you want to install additional R packages and their Linux dependencies.\nI have set up this container with it’s own subdomain that I configured with my DNS provider. Let’s say it is rstudio.milesmcbain.xyz - so http traffic to that will be forwarded to https and then to the RStudio login page, so that login can be done securely.\nIn volumes: an area of my VPC file system is mapped to the rocker/tidyverse container so my RStudio project repositories will persist.\nnginx-proxy: and letsencrypt-nginx-proxy-companion:\nThe configuration here is the boilerplate for a minimum viable setup. The mapped volumes: are especially critical and should not be tinkered with. Both of these containers support additional configuration options that are discussed on their Github repo README pages linked above.\nPossibly of interest is SSL certificate key size and auto-renewal interval configured in letsencrypt-nginx-proxy-companion. The defaults are sensible though.\nStarting your Ghost blog\nSo if you’re still with me, you must be getting pretty keen to fire up your new blog! To do so:\nIn an ssh session to your VPC host: Copy or fork and clone the docker-compose.yml file into a folder in your VPC host user’s home directory, e.g. /<your user>/compose/docker-compose.yml\nGo to the folder and make necessary edits to the file (e.g. using vim or nano), replacing my domains, <My Email>, <My RStudio User> etc. with your personal config.\nSave and exit docker-compose.yml and run the command docker-compose up, from the same folder as docker-compose.yml. You may need to prefix with sudo.\nSit back and enjoy the show in your terminal. For a little while Docker will be downloading image data, after which it will start the composition and you will get a very detailed log showing what each container in the composition is doing:\n\n\n\nFigure 1: A test I ran on my laptop\n\n\n\nAfter the log has settled down, verify your blog is working by navigating to https://<your domain>. You should see a stock Ghost theme with example posts and text.\nBack in your terminal session, Hit ctrl + c to interrupt the Docker containers, this will trigger Docker shutdown. Follow with the command: docker-compose down to remove temporary data.\nRun: docker-compose up -d, to start your blog again, this time as a background process. You will get some simple messages about containers starting, and not the detailed log. Now you can close the terminal, exit ssh etc, and your blog will remain live.\nNavigate to https://<Your Domain>/ghost to set up your admin user and log in to the blogging platform.\nHave fun tweaking your blog, downloading themes, and writing posts!\nNotes\nIn step 4, keep an eye out for issues in the log, particularly with ssl certificate registration. You need to have a domain that can be resolved via DNS to be issued with a certificate.\nAdditional Reading\nThings not already linked that I found useful along the way to figuring this out:\nDigital Ocean Documentation on setting up a VPC server. Useful for any VPC provider. In particular: How to Set Up SSH Keys\nDeploy Ghost blog using Docker and Caddy\nsimple-lets-encrypt-docker-compose-sample\nIf you have any suggested improvements, need a hand, or just want to show off your blog to me, feel free to @ me on Twitter or leave a comment below.\n\n\n",
    "preview": "posts/a-fully-dockerised-ghost-blog/g1.png",
    "last_modified": "2024-03-11T20:38:26+10:00",
    "input_file": {},
    "preview_width": 953,
    "preview_height": 560
  },
  {
    "path": "posts/ropensci-onboarding1/",
    "title": "Waterslides vs Ivory Towers: Reflection #1 on rOpenSci Onboarding",
    "description": "Onboarding is a waterslide. rOpenSci have created a rewarding and fun process that generates high quality R packages.",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-03-06",
    "categories": [
      "rstats",
      "open science",
      "rOpenSci"
    ],
    "contents": "\n\n\n\nLast week editor Maëlle Salmon closed issue #139 on ropensci/onboarding and thus marked the end of 7 months of preening the Australian Antarctic Division’s bowerbird. I take a sense of pride as a reviewer of this package. It was unquestionably improved by the onboarding process, and it was just a really great feeling to be a part of a process that: A. worked so well, and B. was a worthwhile use of volunteered time.\nWhen I say ‘7 months’ it makes the process sound grueling. It was not. I estimate I spent around 20 hours over that time. Far from feeling worn down, all I’m thinking right now is how much I want do it again!\nWhich brings me to the first of a couple of reflections I plan to write on the Onboarding process.\nOnboarding is a Waterslide\nAs a reviewer, the Onboarding process feels like this: 1. Initial excitement and trepidation when the editor invites you to review -> standing at the top 3. A moment of blind panic when you first lay eyes on all that code -> taking the plunge 4. A receding sense of helplessness replaced by determination and enjoyment as your contribution grows -> surfing the tube 5. Joy as the final ticks are given -> splashdown 6. Happiness, relief, satisfaction as the package goes live -> coming out in the wash 7. A desire to do it again -> running to beat the queue.\nMaybe that analogy is a bit cute. But don’t let that make you miss the point: rOpenSci have created a rewarding and fun process that generates high quality R packages. It’s just like the unconference. It’s addictive. It’s a positive feedback loop that improves both the packages and the people involved.\nThe issue pulse speaks for itself. They have a steady stream of packages and reviewers knocking their door down. If some people tire of the fun, others arrive keen to take their place. The system is mostly run on public volunteer labour, so they are inclusive, and have scalability. Future availability of editors is the main bottleneck, but there are capable candidates either waiting in the wings or being nurtured in the rOpenSci community right now.\nI think some people have the impression that onboarding is about this: \nIt is. But it’s called Onboarding for a reason. Onboarded packages get added to a bona fide R package repository (like CRAN or Bioconductor) maintained by rOpenSci. For me this is the really exciting thing - it’s totally punk.\n\n\n\nFigure 1: rOpenSci founders celebrate another package sucessfully onboarded\n\n\n\nOverlooking the Ivory Tower\n\n\n\nFigure 2: An unknown R developer awaits news of their CRAN submission\n\n\n\nThis rOpenSci repository is almost the perfect antithesis of what R has in CRAN. Where the process for getting added to rOpenSci is completely transparent, CRAN is notoriously opaque, inconsistent, and aloof. I could present you with evidence that CRAN’s centralised Ivory Tower model is cracking under the current load, but I’ll save that for a dedicated post. The other contrast I’ll make is in package quality.\nPick 5 random packages from CRAN’s task views. How many have at least one vignette? How many have a URL for a repository? How many have more than the bare minimum autogen manual mechanically describing functions? I reckon you’ll get one or two best - and these are the hand picked cream of the crop. It’s not that CRAN has slipped - it’s that the bar has been lifted. RStudio, rOpenSci, and other new wave empathetic developers have seen to that.\nThe rOpenSci model can never do the volume CRAN can with automation, but it is crushing it in terms of quality. All onboarded packages have a vignette, a repo, and made sense to at least three people from diverse backgrounds. Anyone can look up the public Onboarding review to get additional insight into why a package works the way it does. An rOpenSci package is not just ‘guaranteed to work’ - it’s guaranteed to be workable. And that is something pretty darn valuable indeed.\nKeen to be a part of it? Go here to regsiter as an rOpenSci reviewer.\nHeader image credit: By Koala:Bear - https://www.flickr.com/photos/ohfuckkit/199410084/, CC BY-SA 2.0, https://commons.wikimedia.org/w/index.php?curid=11078509\n\n\n",
    "preview": "posts/ropensci-onboarding1/640px-Wet-n-Wild_Water_World_Australia_Mach_5.jpg",
    "last_modified": "2024-03-11T20:38:26+10:00",
    "input_file": {}
  },
  {
    "path": "posts/alt-r-with-vscode/",
    "title": "Going Alt-RStats with VSCode",
    "description": "I never want to see another CSV that isn't rainbow!",
    "author": [
      {
        "name": "Miles McBain",
        "url": "https://milesmcbain.xyz"
      }
    ],
    "date": "2018-02-22",
    "categories": [],
    "contents": "\nMid 2017 I got hooked on VSCode. The extension ecosystem is nothing short of a candy shop for editor nerds. I’ve gone from using VSCode for WebVR development - Can you say Chrome debugger?! - to recently wondering if I could build an #rstats data science stack from all these juicy extensions.\nIt turns out I was wondering at the perfect time. Yuki Ueda is improving the VSCode R extension weekly, and Randy Lai has just taken the whole thing to the next level with an implementation of an R language server - More about that in a minute. Both of these are under heavy active development.\nA RRogues Gallery\nHere I’ll walk you through the pieces of my current setup. Most of these are VSCode extensions, some are other bits of software.\nR Extension for VSCode\nThere’s a good set of features here already. The main niceties I enjoy are highlighters for Roxygen and .Rmd. You also have help with managing your R console. Linting is included, if that’s your cup of tea.\nR LSP Client\nThis is a key piece of the setup, but doesn’t do a lot on it’s own. It’s the VSCode client for the R language server, see next item.\nR Language Server\nThis is where the fun starts. It’s actually an R package! A server written in R that implements the language server protocol (LSP) for R. The LSP is designed to be editor agnostic, so this can work with VSCode alternatives like Atom, Sublime etc.\nAt time of writing this thing is still in early dev, but it already has some amazing tricks. Some of this stuff is beating out RStudio in UX.\nHow about hover help?\n\n\n\nFigure 1: Scrollable help file on hover\n\n\n\nCompletions and parameter hints?\n\n\n\nFigure 2: Autocompletion and function parameter hints\n\n\n\nStyle hints?\n\n\n\nFigure 3: lintr style hints\n\n\n\nrtichoke (Formerly Rice)\nrtichoke is an alternative R console cut from the same cloth as iPython. In fact it is written in Python, but if you can get past that you’ll find it has some really sweet features. Syntax highlighting with a variety of colour options, multi-line editing, vim/emacs keyboard shortcuts, and an oh so addictive enter-bash mode as seen here.\nMy favourite thing is the fast and robust completion mechanism. It just feels tighter than RStudio. It doesn’t glitch out. Try it and see what I mean. This goes some way towards compensating for the VSCode editor’s completions being unaware of your R session - something I thought I could not live without until I found rtichoke. \nBe sure to check out the options listed in the README. I’ve found the following useful to add to my .Rprofile:\n\noptions(\n    rtichoke.auto_indentation = FALSE, \n    # Makes alignment looks natural when sent to rtichoke by VSCode\n    rtichoke.complete_while_typing = FALSE\n    # Stops a brief stutter compiling a large parameter completion list for S3 methods like `plot()`\n)\nRainbow Brackets\nI remember first seeing this feature back in MS Excel in ’97, when I thought I was ever so clever hacking my calculus assignment. I now suck at calc, so the joke’s on me. But the joy of this not entirely nostalgia driven. I feel like I am genuinely less anxious reading heavily nested code in rainbow flavour.\n\n\n\nFigure 4: Lookin’ sharp!\n\n\n\nRainbow CSV\nAt the risk of overdoing it… I never want to see another CSV that isn’t rainbow! Note the column identifier on hover!\n\n\n\nMaterial Icon Theme\nR logos, Travis icons, colour codes etc for your file tree. They look smart and improve scanability.\nVim Extension\nMuch more than your average Vim emulator. It integrates with Neovim and even emulates a few popular plugins. I find it close to the real deal without the typical Vim config overhead. Makes RStudio’s Vim emulation look lame by comparison.\nMaking up the Difference\nI have a repo of misc functions, some of which fill in things I miss from RStudio. For example: rend() is my answer to the ‘Knit’ button. I may spin these out into a package of their own in the future.\nConclusion\nI am optimistic about VSCode’s future #rstats prospects given how quickly the language server has come along. There’s also some keenness building to try rolling my own VSCode extension or contributing those above. One thing I miss from RStudio are tidyverse-aware completions, e.g. completing the column names of a piped dataset in dplyr functions. Seems like a good opportunity to make a contribution.\nThus completes my post on my VSCode setup. If you’re loving VSCode as well, feel free to @ me on Twitter with your favourite extensions!\n\n\n",
    "preview": "posts/alt-r-with-vscode/hover_help2.gif",
    "last_modified": "2024-03-11T20:38:26+10:00",
    "input_file": {}
  }
]
